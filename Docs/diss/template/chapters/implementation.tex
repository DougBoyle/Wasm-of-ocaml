\chapter{Implementation}

\section{Repository Overview}
\dirtree{%
.1 . 
.2 src.
.3 ocaml/\DTcomment{OCaml compiler's frontend for parsing/typing}.
.3 middle-end/\DTcomment{Translation to IR}. 
.3 optimisations/\DTcomment{IR optimisations}. 
.3 codegen/\DTcomment{Translation from IR to WebAssembly flowgraph}. 
.3 graph/\DTcomment{WebAssembly flowgraph representation and optimisations}. 
.3 runtime. 
.4 memory.js. 
.4 runtime.wast. 
.2 samples\DTcomment{Test programs for compiler input}. 
.3 results.txt\DTcomment{Expected output of compiled WebAssembly}. 
.3 test.js\DTcomment{Instantiate complied tests and compare outputs}. 
.2 benchmarks.
.3 OCaml/.
.3 C/. 
.3 Grain/.  
.2 unit-tests/\DTcomment{Tests for isolated elements of translation/optimisation}.
}

\section{Linearised IR}
% Reference for ANF?
From OCaml's Typedtree data structure, the first pass of the compiler translates programs to be closer to administrative normal form (ANF), which linearises the syntax tree so that operands are always constants or identifiers, with the exception of control flow constructs such as if statements. Expressions are broken down into immediate, compound and linast (linearised AST) terms to achieve this. The general structure is given below, with some cases omitted.

\begin{verbatim}
linast = compound 
  | Sequence (compound, linast)
  | Let(Identifier, compound, linast) 
  | LetRec((Identifier * compound) list, linast)

compound = imm | unary_op imm | binary_op (imm, imm) 
  | Block (tag, imm list)
  | Field (imm, index) | SetField (imm, index, imm)
  | If (imm, linast, linast) | Switch (imm, (tag * linast) list)
  | Try (index, linast, linast) | Fail index
  | Function (Identifier list, linast) | App (imm, imm list)

imm = Constant | Identifier
tag = index = int
\end{verbatim}

%For example:
%
%\begin{verbatim}
%f(g(x), h(x+y))
%\end{verbatim}
%would be translated to
%\begin{verbatim}
%let v0 = g(x) in
%  let t = x + y in 
%    let v1 = h(t) in
%      f(v0, v1)
%\end{verbatim}

Temporary values are now explicit and the linear structure makes optimisations easier to implement in this form. Code which may execute zero or many times cannot be completely linearised, for example the branches of an if statement, as only one branch will actually be evaluated. 

Given an expression in the Typedtree to translate to an immediate or compound term, the expression is recursively decomposed, returning a list of let bindings for any temporary values required in its computation and an immediate or compound term for its final value. In this way, an initial call to translate an expression to a linast term builds up a list of setup operations and a final compound term, then combines these into a tree by chaining the setup operations together, terminated by the final compound term.


%Linearising is implemented using three mutually recursive functions which compile immediates (constants/identifiers), compound terms such as \verb|x + y|, and top level terms such as \verb|let x = e in e'| or \verb|e; e'|. The first two of these return a pair of the actual term needed, and a list of any setup operations extracted as part of linearising the expression being compiled. Compiling a top level term then converts this result and list of setup operations into a tree which performs all of the setup then evaluates the result.
% Give actual examples of code translation/algorithm cases?
% OCaml primitives handling?

% TODO: Describe 3-level structure of immediate, compounds and linast terms

\section{Pattern Matching}
The linearised intermediate representation also compiles patterns in OCaml's \verb|Typedtree| datatype to the code required to match them and bind variables within patterns. This includes combining the cases of a match expression or function into a single expression, making use of switch statements on integers where possible. 

I chose to implement the backtracking pattern matching compiler described by Le Fessant and Maranget\nocite{ocamlpatternmatch}, which is also implemented in the OCaml compiler for its intermediate representation. A backtracking compiler minimises the size of the code generated, whereas a decision tree compiler may produce larger code but ensures that each pattern is examined at most once, and is the approach taken by the Grain compiler. When optimised, each approach typically has similar performance \nocite{decisiontrees} however backtracking compilers appear to be the more common approach, and guarantee linear code size.


The idea is to have a vector of the values being matched, and a matrix representing the different cases and the action when each matches. An example of this is:

\begin{minipage}{0.45\textwidth}
\begin{verbatim}
let f l1 l2 = match l1, l2 with
  | [], _ -> e1
  | _, [] -> e2
  | x::xs, y::ys -> e3
\end{verbatim}
\end{minipage}
\begin{minipage}{0.55\textwidth}
 values $\begin{pmatrix}
\verb|l1| \\
\verb|l2|
\end{pmatrix}$, cases
$\begin{pmatrix}
\verb|[]| & \_ & \to & \verb|e1| \\
\_ & \verb|[]| & \to & \verb|e2| \\
\verb|x::xs| & \verb|y::ys| & \to & \verb|e3| \\
\end{pmatrix}$
\end{minipage}

The algorithm given in the paper is made more complex by the range of syntax OCaml supports. %, and to a lesser extent by the fact the IR is linearised. 
First, OCaml patterns can have guard statements which need to be tested if a pattern matches. Some of the compilation steps prefix additional variable bindings onto the actions, and the guard statement needs to be evaluated after these, but before any of the original action is evaluated. Therefore, each action $\to a$ is replaced with $\to a \ g \ b$ where $g$ is the optional guard expression and $b$ is the other bindings introduced.

The base cases of this algorithm occur when we reach a matrix with either no rows or no columns of patterns. If there are no rows, we have run out of possibilities so pattern matching fails and backtracks to another branch. If there are no columns, we have successfully matched the whole pattern. We then output code testing the guard for each action from top to bottom of the matrix, executing the corresponding action if the guard succeeds. Either some row is not guarded, in which case it runs unconditionally if that is reached, or every row is guarded in which case we similarly backtrack if all guards are false.

There are four broad cases for when the matrix is non-empty:
\begin{itemize}
\item \textbf{Variable rule}: If the first column has just variable patterns, the corresponding binding of the first value in the values vector is added to each row's bindings, then the first column of the matrix is discarded, as is the now matched value of the value vector.
\item \textbf{Constructor rule}: If the first column has just constructor patterns, several specialised matrices are produced, each removing incompatible rows and deconstructing the fields of the chosen constructor. The result of compiling each of these then forms the branches of a switch statement on the constructor tag. A default case captures any constructor with no corresponding rows, and fails so that matching backtracks. \\
$
\begin{pmatrix}
c(q_1, \dots, q_k) & p^1_2 & \dots & p^1_n & \to & \dots \\
c'(\dots) & p^2_2 & \dots & p^2_n & \to & \dots \\
\end{pmatrix}
\xrightarrow{\text{specialise } c}
\begin{pmatrix}
 q_1 & \dots & q_k & p^1_2 & \dots & p^1_n  & \to & \dots 
\end{pmatrix}
$
% OR variables are an issue even at this stage, mention difference to paper's approach here about extended scope
\item \textbf{OR rule}: If there is just one row and it starts with an OR pattern \verb"(p1|...|pk)", a $k \times 1$ matrix is produced expanding out the OR pattern. %, with each row having an empty action. 
The output is the result of compiling this matrix, sequenced with the result of compiling the rest of the original matrix. Simply duplicating the original row once for each case of the OR pattern would have duplicated all of the patterns in later columns. That could lead to exponential growth in code size, which backtracking compilation aims to avoid.
\item \textbf{Mixture rule}: If no other case is applicable, the matrix is split into an upper and lower matrix where one of the other rules can be applied to the upper matrix. The matrices are compiled separately, with failure in the upper matrix resulting in a \verb|Fail i| instruction. This is handled by the \verb|try ... with (i) ...| construct which backtracks to try the lower matrix instead.

\end{itemize}

This is how the paper describes the algorithm. In practice, OCaml has several different types that all fall under the ``constructor" rule and have to be handled separately:
\begin{itemize}
\item \textbf{Constants}: Constants can be viewed as a datatype with infinitely many 0-arity constructors. For integers, these compile to a switch statement on the integer value, always have a default case that backtracks. For floating point constants, a switch statement cannot be used so these instead compile to a sequence of if-then-else tests.
% and this switch statement will always have a default case that backtracks, since it can never be exhaustive. The decision to implement this switch statement as a branch table or nested if-then-else statements is made at the WebAssembly generation stage based on how sparse the set of values is. \\
%For floating point values, we must always use a chain of if-then-else tests so these are compiled as such at the IR level.
% TODO: Talk about branch tables or not?

\item \textbf{Tuples}: Tuples are just constructors with a single variant of a particular size. Therefore, the matrix is specialised by extracting the nested patterns and no rows need to be discarded. There is also no need for a switch statement, since there is only one case.

\item \textbf{Arrays}: In OCaml, array patterns look like \verb"[|p1, p2, ..., pk|]", matching a specific length. The length is the tag of the object, so a switch statement on the tag is generated, extracting the corresponding number of fields in each case. As there are infinitely many possibilities, a default case is always included.

\item \textbf{Records}: These are again treated as a constructor with one variant, and each pattern specifies some subset of fields to match. To allow all rows of the matrix to be processed,  the union of the fields mentioned in each row is extracted from the record, inserting the wildcard pattern \verb|_| where a row does not match a field, allowing the value extracted for that field to be ignored. In the example below, any unmentioned field $z$ is ignored entirely.

$
\begin{pmatrix}
\{x=q_1\} & p^1_2 & \dots & p^1_n & \to & \dots \\
\{y=q_2\} & p^2_2 & \dots & p^2_n & \to &\dots \\
\{x=q_3; y=q_4\} & p^3_2 & \dots & p^3_n & \to &\dots
\end{pmatrix}
\to
\begin{pmatrix}
q_1 & \_ & p^1_2 & \dots & p^1_n  & \to & \dots \\
\_ & q_2 & p^2_2 & \dots & p^2_n  & \to & \dots \\
 q_3 & q_4 & p^3_2 & \dots & p^3_n  & \to & \dots
\end{pmatrix}
$
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Description of algorithm in paper
%The basic algorithm represents the patterns to compile by a vector of values to be matched and a matrix, where each row is a corresponding vector of patterns to match against (where rows are tried in top to bottom order) plus some code, called an action, to execute if the values match that row. The handling of guard statements is not described. Without guards, when the algorithm reaches a $m \times 0$ matrix (so each row has an empty vector of patterns left to match), the pattern has now been matched so the action of the top row is returned. To handle guards, each row of the matrix is augmented to also have an optional compiled guard expression. When an $m \times 0$ matrix is reached, this compiles to a series of if-then-else tests of the guards of each row until a row without a guard is reached. If all rows are guarded, then we eventually reach a $0 \times 0$ matrix which corresponds to pattern matching having failed, so we escape to the enclosing statement testing any other patterns or trap if there is nothing else to test. \\
%As guard expressions can include variables bound by the pattern, bindings introduced by pattern matching also need to be kept separate from the action to perform when a match succeeds. This allows the guard expression to be inserted between these bindings and the body of the action, which must only execute if the guard succeeds. \\
%For describing examples, $p$ represents a pattern, $a$ is an action, $g$ is an (optional) guard expression, and $b$ are binds introduced by pattern matching. $v$ is the first value being matched against.

%Escaping out of failed matches is achieved by the intermediate representation having static exceptions in the form of a \verb|fail i| and \verb|try ... with i ...| expression. Therefore, the algorithm just remembers the closest enclosing handler at any point and escapes to that when pattern matching fails. Although regular exceptions are challenging to implement in WebAssembly, these static exceptions compile to simple blocks with branch instructions to escape out of depending on if the expression succeeds or fails.

% Mention that bindings and the actual action need to be kept separate so that the guard can be checked between them?
%Another complication is the additional cases that need to be checked for. The described algorithm considers three types of patterns which are variables $\_$ or $x$ (where the latter generates a binding of the value to $x$ when it is matched), constructors $c(p_1, \dots, p_k)$ and OR patterns $(p_1 | p_2)$. Compilation can then be broken down into a handful of cases based on the patterns in the first column of the matrix. If every pattern is a variable pattern, then the first value and first column of the matrix can be discarded after binding the value to any non-wildcard variables for each row. 

%$
%\begin{pmatrix}
%x & p^1_2 & \dots & p^1_n & \to & a^1 & g^1 & b^1 \\
%y & p^2_2 & \dots & p^2_n & \to & a^2 & g^2 & b^2 \\
%\_ & p^3_2 & \dots & p^3_n & \to & a^3 & g^3 & b^3
%\end{pmatrix}
%\to
%\begin{pmatrix}
% p^1_2 & \dots & p^1_n  & \to & a^1 & g^1 & (b^1, x=v) \\
% p^2_2 & \dots & p^2_n  & \to & a^2 & g^2 & (b^2, y=v) \\
% p^3_2 & \dots & p^3_n  & \to & a^3 & g^3 & b^3
%\end{pmatrix}
%$
%
%If instead every pattern is a constructor pattern, then the matrix is compiled into a switch statement on the variant tag of the first value. A case is generated for each constructor present in the matrix, where the body of the switch case is the result of compiling the matrix with all rows with incompatible constructors discarded, and any sub-patterns of the constructor pattern extracted out and appended to the front of each row. 
%
%$
%\begin{pmatrix}
%c(q_1, \dots, q_k) & p^1_2 & \dots & p^1_n & \to & \dots \\
%c'(\dots) & p^2_2 & \dots & p^2_n & \to & \dots \\
%\end{pmatrix}
%\xrightarrow{\text{specialise } c}
%\begin{pmatrix}
% q_1 & \dots & q_k & p^1_2 & \dots & p^1_n  & \to & \dots 
%\end{pmatrix}
%$

%Although this is described as one rule, there are many types of OCaml patterns which are all treated as constructors and need to be handled slightly differently. Tuples are treated as a constructor type with a single variant, so a switch statements is not required, just extracting each of the values contained from the tuple and corresponding patterns. Records are similarly treated as a constructor type with only one variant. As each pattern may match different fields of the record, the fields extracted are the union of the fields mentioned by each row. Where a field is extracted but a row does not match on that field, a wildcard pattern is introduced in the row to effectively ignore that extracted value.  Fields in a record which no pattern mentions are ignored entirely.

%$
%\begin{pmatrix}
%\{f_1=q_1\} & p^1_2 & \dots & p^1_n & \to & \dots \\
%\{f_2=q_2\} & p^2_2 & \dots & p^2_n & \to &\dots \\
%\{f_1=q_3; f_2=q_4\} & p^3_2 & \dots & p^3_n & \to &\dots
%\end{pmatrix}
%\to
%\begin{pmatrix}
%q_1 & \_ & p^1_2 & \dots & p^1_n  & \to & \dots \\
%\_ & q_2 & p^2_2 & \dots & p^2_n  & \to & \dots \\
% q_3 & q_4 & p^3_2 & \dots & p^3_n  & \to & \dots
%\end{pmatrix}
%$

%Patterns for arrays in OCaml look like \verb"[|p1, p2, ..., pk|]" and match an array of a specific length. Arrays are tagged with their length so an array of a certain type can be viewed as a constructor with infinitely many variants, each variant having arity equal to the length of array it matches. This therefore becomes a switch statement on the length of the array, and must always introduce a default case since there are infinitely many variants so the match cannot be exhaustive. Integer constants are similarly handled as a constructor with infinitely many variants, except that these variants all have arity 0 as there are no sub-patterns to match. Floating point constants can be pattern matched in OCaml however these must be compiled to nested if-then-else statements, since my IR only allows switching on integers, either an integer constant or the tag of some constructor. This means switch statements can always be compiled as branch tables in WebAssembly later. \\
% Do I want to mention that there can be other rows after the OR pattern
%If neither of the previous rules apply then either the matrix  starts with an OR pattern in its first row, which can be expanded into multiple rows, or the matrix is split into an upper or lower part where each can be checked recursively, with a failure in the upper matrix being handled by trying the lower matrix (referred to as the mixture rule).\\

Finally, OCaml has alias patterns such as \verb|((x::xs) as lst)| which must be supported. These are handled by preprocessing the first column of the matrix at each step, replacing aliases in each row with a binding of the first value to the corresponding aliased variable. Redundant OR patterns are also removed, replacing \verb"(_|p)" with just \verb|_|.
%The last feature of the OCaml language that needs supporting is aliases, for example \verb"match v with ((x::xs) as lst) -> ...". These are handled by always preprocessing the first column of the matrix to replace aliases in each row with a binding of the first value to the aliased variable. Preprocessing also simplifies any or patterns in the first column, replacing \verb"(_|p)" with just \verb|_| since the second part of the or pattern would never be considered.


$
\begin{pmatrix}
(p^1_1\ \mathrm{as}\ q) & p^1_2 & \dots & p^1_n & \to & a^1 & g^1 & b^1 \\
(\_ | p^2_1) & p^2_2 & \dots & p^2_n & \to & a^2 & g^2 & b^2 \\
p^3_1 & p^3_2 & \dots & p^3_n & \to & a^3 & g^3 & b^3
\end{pmatrix}
\to
\begin{pmatrix}
 p^1_1 & p^1_2 & \dots & p^1_n  & \to & a^1 & g^1 & (b^1, q=v) \\
\_ & p^2_2 & \dots & p^2_n  & \to & a^2 & g^2 & b^2 \\
p^3_1 & p^3_2 & \dots & p^3_n  & \to & a^3 & g^3 & b^3
\end{pmatrix}
$

% MENTION ISSUE WITH OR PATTERNS?
%There is a complication with OR patterns which I handle differently to the paper. 


\subsection{Optimisations to pattern matching}
The above process is made more efficient by including auxiliary information, which is described here rather than waiting till the optimisations section.
% Mention OR pattern change or not?


First, the mixture rule can swap rows where they are incompatible. Patterns must be matched in top-to-bottom order, but this does not matter for two rows which do not overlap in the set of values they may match. Therefore, the mixture rule lifts up rows where safe to do so, trying to increase the size of the first matrix that can be handled by a single use of the constructor or variable rule. This decreases the number of splits from the mixture rule that are needed. \\
As an example, these programs are equivalently but the constructor rule can only be applied to the first row on the left, but to the first two rows in the right:

\begin{minipage}{0.45\textwidth}
\begin{verbatim}
match l1, l2 with
  | [], _ -> 1
  | _, [] -> 2
  | x::xs, y::ys -> 3
\end{verbatim}
\end{minipage}\qquad
\begin{minipage}{0.45\textwidth}
\begin{verbatim}
match l1, l2 with
  | [], _ -> 1
  | x::xs, y::ys -> 3
  | _, [] -> 2
\end{verbatim}
\end{minipage}

Another improvement is made by passing a boolean for exhaustiveness to the procedure. The OCaml front-end already has utility functions for estimating both when patterns are incompatible and when a set of patterns is exhaustive. Exhaustiveness is unchanged by recursive calls, except for the mixture rule where the top matrix is never considered exhaustive. When a matrix is known to be exhaustive, switch statements can leave out cases for missing constructors, as they must have been matched by an earlier stage of pattern matching. 



Next, several other data structures are added to the procedure. They are all related so I describe each of them before explaining their benefit. \\
The procedure is extended with a context, a pair of matrices of just patterns with no actions or guards. The first is called the prefix, remembering already matched structure, and the second is the fringe, the matrix of patterns used above to indicate patterns left to be matched. Together, these describe the possible values that may reach the point in the pattern matching code currently being produced. Initially there is just one row, where the prefix is empty and the fringe is \verb|_|, as nothing is known about the value being matched. The specialisation operations on matrices extend to contexts in the obvious way, for example with constructors ($\bullet$ separates the prefix and fringe):

$
\begin{pmatrix}
 p^1_1 \dots  p^1_k & \bullet & c(q_1, \dots, q_k) & \dots & q^1_n \\
 p^2_1 \dots  p^2_k &\bullet & c'(\dots)  & \dots & q^2_n  \\
\end{pmatrix}
\xrightarrow{\text{specialise } c}
\begin{pmatrix}
p^1_1 \dots  p^1_k  & c(\_, \dots, \_) & \bullet &  q_1 & \dots & q_k & \dots &  q^1_n 
\end{pmatrix}
$

The procedure is also passed a list of reachable handlers. When the mixture rule splits a matrix into upper and lower parts $P$ and $Q$ and introduces handler $i$, then $P$ is complied with $(i, Q)$ added to its handler list, although only the patterns of $Q$ need to be remembered in the handler list, not the actions or guards. Again, operations on matrices are naturally extended to each matrix in the list. 

Correspondingly, in addition to the code to match a matrix, the procedure returns a jump summary, a mapping from indices of enclosing handlers to the contexts in which those handlers will be jumped to. To compute this, where the algorithm inserts \verb|fail i| to backtrack, it also adds the current context to the jump summary for index \verb|i|, indicating that values of the form described by that context will trigger backtracking. For each index, the contexts of each branch are unioned at switch statements, amounting to concatenating the rows from each context. As jump summaries are returned out of nested calls, patterns are popped off the prefix of each context back to the fringe, to be consistent with the values matched up to that point in the process but now making the fringe more precise. \\
If compiling the upper matrix in the mixture rule returns jump summary $\rho$, then when compiling the lower matrix $Q$ for handler $i$, the context $\rho(i)$ is used from the jump summary.\\


The reachable handler list remembers which patterns each handler will try to match if exited to, and the contexts gotten from the jump summary provide some information on the structure of values which will trigger that handler. \\
The first improvement is that, rather than always backtracking to the nearest handler, handlers guaranteed to fail can be skipped over. Since the matrices matched by reachable handlers are specialised as more specific cases are matched recursively, at a point where backtracking occurs some of those matrices may be empty. This indicates that the patterns matched by that handler are incompatible with the known structure of the value causing backtracking, so the procedure instead inserts a \verb|fail i| to the first handler \verb|i| with a non-empty matrix. Note that, where the mixture rule is needed multiple times to split up a matrix, it now makes sense to nest the handlers so that they are all available to jump to, maximising the opportunities to skip handlers.  

Second, the lower matrix for a handler may not cover all constructor cases if some are covered by the upper matrix matched before backtracking. Naively, this would result in additional backtracking cases for the missing constructors, which we know will never occur. This is the purpose of taking the context from the jump summary. If some constructor is incompatible with the first column of the fringe for each row of the context, the value being matched will never be that constructor when the handler executes, so cases can be safely discarded to reduce code size. In some cases, switch statements may be reduced to a single case by this approach, in which case the test can be removed entirely.

% Haven't mentioned OR patterns optimisation


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% MENTION USE OF EXHAUSTIVENESS INFORMATION EARLY ON! EXPLAINS WHY WE DON'T NEED A TEST ON THE LAST ROW!

%First, there are cases where it can be useful to swap the order of rows in the pattern matrix. When no rule can be directly applied, we split the matrix into two parts and recursively match each of them, backtracking out of the first case if necessary. A simple program matching two lists can be equivalently written as (the action in each case is unimportant):



%These are equivalent as the last two rows are incompatible. Although patterns must be matched in order, any pattern matching row 3 could never match row 2 so they are safe to swap. The second case is more efficient as we can handle rows 1 and 3 together with the constructor rule, and only need to split the matrix once rather than twice. Of course, the first case could also avoid splitting the matrix twice if rows 2 and 3 were matched on the second pattern before the first, in which case applying the constructor rule would be possible. The problem of selecting the optimal column is thought to be NP-Complete and heuristics for this are more commonly used for decision tree based pattern matching, and the paper does not suggest how row and column based optimisations could be combined. % Cite the end of the OCaml paper for this (e.g. NP-Complete statement)
% Do I actually need to mention this?

% TODO: Not why we use contexts!!!
% Actually quite complex - interaction between jump summaries, reachable handlers, and how extraction from context is used to choose handler on each exit of failed constructor rule (i.e. simpler exhaustiveness because some case can't actually occur)

%Next, in addition to the matrix storing patterns an actions, we maintain and extended to structure called a context, which does not include actions but keeps an additional list of patterns on each row, called the prefix, for the structure already matched by pattern matching. The issue with using just the matrix described previously is that we effectively forget about what has already been seen once part of a match succeeds. Therefore, 

% Very long explanation, would go better in preparation???
%If matching some pattern fails and we need to backtrack, we backtrack to the nearest enclosing handler. In some cases, we already know that this handler will fail too due to patterns matched before we had to backtrack. If so, then we can speed up the process by considering all enclosing handlers and jumping to the nearest one that may actually succeed. For this reason each handler has an integer identifier \verb|i|.

% Not my job to duplicate explanations from the paper?
%As an example, consider the program below (written out of order to show how the improved mixture rule previously described would process it): % Cite this as being the example given in the paper
%\begin{verbatim}
%type t = Nil | One of int | Cons of int * t
%match l1, l2 with
%  | Nil, _ -> 1
%  | Cons(x, xs), Cons(y, ys) -> 5
%  | _, Nil -> 2
%  | One x, _ -> 3
%  | _, One y -> 4
%\end{verbatim} % TODO: Can cut all this down a little?
%
%In pseudocode, this is what it will compile to:  % TODO: Cite that this comes from the paper!
%\begin{verbatim}
%try
%  try
%    try
%      switch (l1):
%        case Nil: 1
%        case One: fail 1
%        case Cons:
%          switch (l2):
%            case Cons: 5
%            case One|Nil: fail 1
%    with (1)
%      switch (l2):
%        case Nil: 2
%        case Cons|One: fail 2
%  with (2)
%    switch (l1):
%      case One: 3
%      case Cons|Nil: fail 3
%with (3) 4
%\end{verbatim}

%The OCaml compiler already warns about non-exhaustive patterns, so we can use exhaustiveness information to determine that any tests in the last handler would be redundant, as pattern matching cannot fail from there if the original patterns were exhaustive.

%The mixture rule groups the first two rows to be matched by the constructor rule, and divides the remaining rows into separate 1-row matrices tried in turn. The goal is to jump past one handler to an enclosing one, so \verb|try ... with i ...| blocks are formed in bottom-up order so that matching of the first two rows is enclosed by 3 handlers. \\ % TODO: Add a diagram/pseudocode for this?
%For a pair of values such as \verb|Cons(a, b), One(c)|, we first attempt to match against just the first two rows. We successfully match the \verb|Cons(x, xs)| constructor but fail on the second row. Naively, we would now try the first handler, which tests the second value against \verb|Nil| again and fails, then goes to the second handler which tests the first value against \verb|One x| and so also fails. Given the tests that had already been performed when we first failed, we knew the value had the form \verb|Cons(_, _), One(_)| and that this is incompatible with the matrices of both of the first two handlers, so we could have immediately jumped to testing the last row.

%To track this information, we additionally maintain a list of reachable handlers, mapping their indices \verb|i| to the remaining matrix that handler tries to match. Similar to how rows of matrices are manipulated in the original algorithm, we can specialise the rows of these handler matrices as pattern matching progresses, removing incompatible rows. If one  of the cases when matching a constructor pattern then requires us to backtrack, we can now go to the first handler with a non-empty matrix in the reachable handlers, since all previous handlers will immediately fail. \\

% Include optimised version of code?
% TODO: Contexts and jump summaries

%Having done this, it reveals another form of inefficiency. We can only reach a particular handler from exits at specific points in the code and, having successfully matched patterns up to those points, the set of possible values when that handler is reached is restricted. In the example above, we now jump straight to 
%the last handler from line 10 when the second list is a \verb|One _| value, splitting the old \verb"case One|Nil" line into two different exits. This means that the first handler is only entered from one of two locations, when \verb|l1 = One v| or when \verb|l1 = Cons (v, vs)| and \verb|l2 = Nil|. In the latter case, we will return 2, but in the first case we exit to the second handler. This handler then checks if \verb|l1 = One v|, but we just established this is the only situation in which the second handler executes. Therefore, the other cases of the switch statement can be discarded and the whole second handler reduces to just the value 3.

%To identify these case, we maintain an extended version of a pattern matrix called a context, which does not include actions but keeps an additional list of patterns on each row, called the prefix, for the structure already matched by pattern matching. As with the reachable handler list, the operations of pattern matrices are extended to apply to this, with additional operations to push or pop pattern between the prefix and the fringe of patterns yet to be matched. Now, as well as returning code to match the patterns in a matrix, the procedure recursively computes a jump summary, mapping each handler index to the union of the contexts at all points that exit to that handler. Recursive calls push patterns onto the prefix of the current context, as they match sub-patterns in a more specialised context, and returns pop patterns off the prefixes of context in the jump summary as we return to higher up points in the pattern matching process. 

%Now, when we compile a handler, we take its context to be the context mapped to in the jump summary returned from the try block. When we construct a switch statement for the constructor rule, we can exclude any constructor patterns which are incompatible with all rows of the context, indicating that we will never see that constructor for values reaching this point in the pattern matching code. \\

% OR PATTERNS - DIFFERS FROM PAPER!
% Is this actually something I want to draw attention to???
% Should probably change this in my code to do it the other way, as it only every applies to OR patterns doesn't it? Can easily determine which variables need to be passed out.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%                            OR PATTERN ISSUES                             %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%My implementation differs from the one described in the paper by not also specifying variables passed through to handlers in addition to a handler index. When an OR pattern \verb"(p1|...|pk) q1 q2 ..." is split into several rows, unless it is the only row of the matrix, we introduce an exit to a handler which matches the remaining patterns \verb|q1 q2 ...|. This is done to avoid duplicating the patterns \verb|q1 q2 ...| in the original matrix, which could result in duplicate code across switch cases. The issue here is that the patterns \verb"p1|...|pk" could each bind some variable \verb|x|, which is then in scope when we enter the handler. The paper handles this by listing all such variables at the exit and at the start of the handler block. \\
%Because the parser in the compiler front-end assigns a unique identifier to each variable, there is never a scoping issue where the \verb|x| in the handler could refer to the same \verb|x| declared elsewhere. Even with variables listed on exits and handers, we still need an extra data structure to remember identifiers outside of the subtree they are defined when performing analyses or translations, for example so tat we know which lower level local variable the identifier was mapped to when we come to translate the handler. Therefore, I chose to leave out these lists of variables since they serve little purpose other than to make such scoping complications explicit, so exits and handlers are only annotated with the index of the handler they correspond to. \\
%This scoping complication disappears at the WebAssembly level, since static exceptions compile to code looking more like a standard \verb|break| statement, jumping to the next block (the handler) at an exit, or jumping past the next block if pattern matching ultimately succeeds.
%This is not necessary as parsing uniquely identifies reuses of the same variable name in different scopes, so a table can track all the variables seen so far without having to deal with renaming issues, at the relatively small overhead of needing to remember more variables simultaneously.
%%% ALSO PREVENTS OPTIMISING, BUT VERY RARE ANYWAY?
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%In summary, the compilation scheme now has three additional arguments, a boolean indicating if we know the current matrix of patterns to be exhaustive, a context giving additional information on the patterns already matched, and a list of the reachable trap handlers with the matrix each handler matches. It also returns both the code to match a given matrix, and a summary of the set of values which can cause the code to exit to each of the enclosing handlers. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Compiling Identifiers}
After any optimisations on the IR are done, the linearised tree is lowered to WebAssembly in two stages. The first of these pulls function definitions out to the top level of the program, making closures and free variables explicit. It also converts the IR tree to lists of instructions (with nesting still present for \verb|if|, \verb|for| and \verb|while| statements as WebAssembly supports nested blocks) and replaces string identifiers.


Identifiers are replaced with a datatype indicating the kind of variable being accessed, with free variables being stored in the closure of a function:\\
\verb"binding = Global of int | Arg of int | Local of int | Closure of int"\\
%whether that variable is accessed as a global variable, a function argument, a local variable of the function, or a free variable in the function's closure, with an integer index into the corresponding set of variables. \\
Each let binding in a function's body generates a new local variable for that identifier. This results in a large number of local variables that is reduced after peephole optimisations on the generated WebAssembly are performed. The WebAssembly representation is stored with a flowgraph between instructions, a graph indicating which instructions may execute immediately before or after each instruction in a function. Live variable analysis is performed on this, identifying the points in a function where each local variable's current value could be used later. These points indicate regions where a variable cannot be overwritten. From this, a clash graph is constructed with edges between every pair of variables that are ever simultaneously live so cannot be allocated to the same local variable. A graph colouring heuristic then maps these local variables to a hopefully much smaller set of local variables and unused local variables are removed from the function.
%Initially, assignment of local variables was done following the scoping of the program, so each nested let binding would use one more local variable and a \verb|for| loop would use two, one for the loop variable and another to remember the upper limit of the loop the first time it is evaluated. \\
%This was replaced with generating a new local variable for each identifier in the function, and keeping these distinct even when generating WebAssembly. Then, after performing peephole optimisations on the WebAssembly, I perform live variable analysis, which determines the points in the program where each local variable's value could be used later so cannot be overwritten. From this, I construct a clash graph with edges between every pair of variables that are ever simultaneously live so cannot be allocated to the same local variable. A graph colouring heuristic then maps these local variables to a hopefully much smaller set of local variables and unused local variables are removed from the function. % TODO: Better to describe register allocation and LVA somewhere else? Maybe even in preparation chapter?

This register allocation process is necessary for lower level target languages where there is only a finite set of registers available. Although a WebAssembly function can be defined with arbitrarily many local variables, it is likely that a platform's WebAssembly implementation will use less memory by using fewer locals. Also, when garbage collection is described later, this directly affects the height of the shadow stack used to track local variables. Therefore, it is a useful optimisation over allocating local variables according to scoping, which will generally be much less efficient.

%After any optimisations are performed, the linearised tree is then compiled to a lower intermediate form. The tree is replaced with a list of instructions, and identifiers are resolved into integer indexes for argument, local, global or closure bound variables. 
% Give more detail about blocks in WebAssembly
%Conversion to a list of instructions is straightforward since WebAssembly contains nested blocks. \verb|Block| constructs in WebAssembly contain a list of instructions and introduce a label at the end of the block, which can be jumped to from within the block. \verb|If| blocks are similar but contain two lists of instructions, and \verb|Loop| blocks have the label at the start of the block, allowing code to be repeated. 
%Rather than map to these straight away, programs are still described using \verb|if| and \verb|while| constructs, which are easily mapped to WebAssembly at the code generation stage. % Put examples when we get there, not here

%This stage also lifts any function declarations to the top level of the program, since closures and environments are now explicit, so the result is a list of functions. The complexity here is in determining the free variables of a function and the number of local stack variables needed by the function body, both of which are done by simple recursive algorithms. The number of locals needed is the maximum needed at any point in the function body. This increases by one after each let binding is evaluated, and for loops also require two to remember the current and end values for the loop (for loops in OCaml have a very constrained \verb"for var = i [up|down]to j do ... done" syntax). 

% Currying removed at this point. Wasn't being used anyway so why do I allow it at all? Does it actually provide any benefit to me?

\section{Code Generation}

Finally, the program is translated to WebAssembly code, with most operations mapping directly to fixed blocks of code that are concatenated together. Variable accesses need to be mapped to \verb|LocalGet| and \verb|GlobalGet| operations, and equivalent \verb|Set| instructions. \verb|Global i| maps directly to \verb|GlobalGet i|, and \verb|Arg i| maps to \verb|LocalGet i|, as function arguments are the first local variables in WebAssembly. Two additional ``swap" local variables are introduced in each function, needed to compile some operations to WebAssembly. For a function with \verb|n| arguments, we therefore compile \verb|Swap i| as \verb|LocalGet (n + i)| and \verb|Local i| as \verb|LocalGet (n + 2 + i)|. Lastly,  a free variable \verb|Closure i| compiles to a \verb|LocalGet 0| to get the closure pointer, then a \verb|Load| with an offset to that variable.

%%%%%%%%%%%%%%%%%%%%%

%WebAssembly generation is then straightforward, as each of the constructs and primitive operators in my lower IR are all simple to implement. The variable bindings in the lower IR were separated into argument, local, global and closure bindings. Argument and global bindings translate directly to Local and Global operations respectively. The locals in a function are laid out as function arguments, then swap space locals used to implement some of the operations in WebAssembly, then locals allocated above for let bindings and loops. Therefore, local bindings are offset by the number of swap spaces and function arguments. The first argument to each function is its closure, so closure bindings first get local 0 then do a load from that address, with an offset selecting the correct variable from the closure. 

I also wrote a runtime in WebAssembly providing functions for memory allocation, polymorphic comparison, boxing of floats in memory, and list append. These, along with the memory controlled by the memory allocator, are declared as imports into the compiled WebAssembly module. The module then exports each top level global variable as its name from the OCaml program, and each of the user defined functions. \\
As explained below, integers and pointers are tagged to distinguish them, so I wrote a short JavaScript wrapper which interprets values accordingly. This means that JavaScript code can interact with the WebAssembly values without needing to understand the runtime representation, and can call functions using returned closure pointers.

% These are declared as imports to the WebAssembly module and each function index in my IR must be offset by the number of runtime functions imported. Additionally, the memory for the module is also imported from the runtime since it is managed by the allocation function from the runtime imports. Lastly, the global variables for each user variable declared at the top level of the OCaml program are exported, along with all functions from the module. Every function is exported so that values representing closures can be returned from WebAssembly and called. As integers are encoded by shifting them left by one, and so that pointers to closures can be used to call functions in WebAssembly from JavaScript, I added a JavaScript wrapper which uses knowledge of how values are represented in memory to be able to pass values back into compiled functions, or return integer values back to a JavaScript caller.

Even before implementing a garbage collector, data still needed to be tagged to identify its type so that OCaml's \verb|compare| function, which is defined on all types, would work correctly. As every operation is on either 32-bit integers or 64-bit floats, pointers are always aligned on 4-byte boundaries so the last 2 bits are unused. I therefore tag integers as 10 or 00 by shifting them left one, closure pointers as 11, and other blocks from tuples or constructors as 01. The memory layout of these values is given below:

  \begin{tabular}{c|c|c|c|c|c|m{2cm}|} \cline{2-7}
Constructor block &   variant tag  & arity &   \multicolumn{3}{c}{\hspace{2cm}{elements} } &  \\ \cline{2-7} \multicolumn{1}{c}{}  \\ \cline{2-7}
Tuple/Record &   0  & arity &   \multicolumn{3}{c}{\hspace{2cm}{elements}} &  \\ \cline{2-7} \multicolumn{1}{c}{}  \\ \cline{2-4}
Boxed float &  -1 & 0 & 64-bit float value      \\ \cline{2-4} \multicolumn{1}{c}{}  \\ \cline{2-7}
Closure & function index  & arity &   \multicolumn{3}{c}{\hspace{2cm}{free variables}} &  \\ \cline{2-7} 
\end{tabular}

Since arguments and return values in WebAssembly are strongly typed and polymorphic OCaml functions need supporting, all values are 32-bit integers which are decoded as pointers to data or functions as just described. In particular, floating point values need to be ``boxed'' in memory and represented as pointers to data blocks storing their value. The variant tag on data blocks goes from 0 to the number of variants of that type, so -1 is used as a special value to distinguish blocks storing a float value.

The arity field in closures indicates the number of free variables. \verb|compare| is undefined on function values, but the arity is still needed for garbage collection so that pointers stored as free variables in closures can be searched. This field is unnecessary for floating point values, which are distinguished by having a tag of -1, but is included and set to 0 so that garbage collection can search each of these blocks for pointers in the same way, by first looking at the arity field then checking that many subsequent fields. A float contains no pointers, hence the field is set to 0 so that the following 64-bits are not misinterpreted.

%Although not strictly necessary, an arity field is included for both floating point values and closures. In the case of floating point values, it is always 0, and for closures it is the number of free variables. For floating point values, this ensures the 64-bit float value is aligned, which may improve memory access performance, and a floating point value can be treated as any other data block by the garbage collector, as the block will be inspected and the arity field indicates that we do not need to inspect the block further for other pointers. \\
%In the case of closures, we need to know the number of free variables for garbage collection, so that pointers stored in the block can be followed in the marking algorithm. This again means all pointers can be treated uniformly by the garbage collector, checking first the arity field and then any potential pointer fields within the block, as indicated below:



%As OCaml limits the number of different constructors supported% Reference required
%, the variant tag is guaranteed to be a non-negative integer. Therefore, as floats need to be``boxed'' and stored in memory as a data block, -1 was used to identify floats, which are stored similarly to . This is necessary since the arguments and return values of WebAssembly functions are strongly typed. Therefore, all functions take a 32-bit integer which is decoded as an integer or pointer to data or a float as required by the body of the function.

\section{Optimisations}
\subsection{Tail calls}
Tail call optimisation is important in functional languages, since loops in a functional language are written as recursive functions, so a naive implementation for a large number of iterations can easily exhaust the available stack space. This is avoided by wrapping the body of the function in an imperative while loop and replacing the tail call with code that saves the arguments the function would have been called with and updates a variable to say the loop should not exit yet. This is extended to mutually tail recursive functions by additionally storing which function should be called next when a tail call is reached.

% Don't need so much detail?
There is first an analysis pass to construct a call graph of just the tail calls in recursive functions. Nodes in the graph correspond to recursive functions, with each node having edges to the functions it tail calls. This is done conservatively, so if a function makes a tail call with a function variable that cannot be identified as a particular function, no edge is added.  Tail call optimisations only apply to fully applied curried functions, so we also discard any over or under-applied call sites.


%The optimisation is done in two passes, the first of which just identifies which functions to tail call optimise. As the linearised tree for the program is traversed, we keep track of whether a function call at the current point in the tree would be a tail call (rather than being the body of a let binding, sequence of loop).
%This allows us to mark every potential call site optimisation in the program and to construct a call graph consisting of just tail calls between procedures.

As detailed below, tail call optimising a recursive function increases its size, and the optimisation is more expensive for mutually tail recursive functions as each function must be split in two. Therefore, this call graph is iteratively pruned to identify functions worth optimising. Functions with no tail calls are removed from the graph, as are functions which only make tail calls to themselves so should be optimised in isolation. Once this process converges, we have the set of functions to make mutually tail recursive. Tail calls between these functions will then be rewritten to avoid making additional function calls.

%
%% Body could be a tail call, not that we currently are a tail call
%\begin{itemize} % TODO: Find better way to describe this
%\item The body of a recursive binding to a variable \verb|f| is traversed as being a tail call position for the function \verb|f|.
%\item The body of a non-recursive binding or first part of a sequence is not a tail call position.
%\item The condition and body of a while loop and the body of a for loop or anonymous function are not tail call positions.
%\item If the current position is viewed as a tail call position for some function \verb|f|, then so is the body of each branch of an if statement or switch statement.
%\item If the current position is viewed as a tail call position for some function \verb|f|, then so is the remaining expression after a recursive or non-recursive binding or first part of a sequence.
%\end{itemize}
%These rules allow keeping track of whether an application is in a tail call position or not as the tree is traversed. Each variable declared by a recursive binding to a function is also added to an initially empty table \verb|tbl| and mapped to an empty set of variables and the function's arity. When an application of \verb|g| is found in a tail call position for some function \verb|f|, we check two things. First, \verb|g| must be known at compile time to be one of the recursive functions declared in the program and therefore present in \verb|tbl|. 
%% TODO: Is this obvious
%This is because we can only rewrite tail calls to functions optimised to make tail calls, the reason for which will be apparent when the changes made to functions are described below.
%Second, the number of arguments applied to \verb|g| must match the arity recorded for \verb|g| in the table. This allows curried functions to be tail call optimised. If the function is under-applied, then we do not have all of the arguments ready to evaluate it so it can not be tail called yet. If the function is over-applied, then the result of calling \verb|g| cannot be returned immediately so this is again not a tail call. if both conditions are satisfied, we add \verb|g| to the set of variables \verb|f| is mapped to in \verb|tbl|.
%
%At the end of this process, we have a table which stores the functions that are tail called within each recursive function. From this we determine which functions to tail call optimise on their own, and which to optimise as mutually tail recursive functions (which has a greater impact on code size since the function must be split into two functions). 

\begin{verbatim}
while graph changing:
  for each f in graph:
    if f makes no tail calls:
      remove f and all edges to it
    else if f only tail calls itself:
      remove f and all edges to it
      optimise f on its own
\end{verbatim}
%There is little to gain from rewriting a function which makes no tail calls, so these are not optimised and hence no other function can optimise tail calls to them. Due to the added code bloat of optimising a function for mutually recursive tail calls rather than just tail calls to itself, a function which could only optimise calls to itself is optimised separately and tail calls from other functions to it are also removed. This leaves us with all other functions benefiting from being made mutually tail recursive.\\

When deciding on this method for choosing which functions to tail-call optimise on their own or as mutually recursive functions, I studied how the Grain complier implements this analysis. In doing so, I identified a bug where it would wrongly optimise certain tail calls, and submitted an issue on GitHub detailing this which has since been corrected. % Cite github issue?

% TODO: Better to just give pseudocode and leave out the details?
For a function \verb|f| taking argument \verb|x| which is to be made mutually recursive on its own, we create new variables \verb|result|, \verb|continue| and \verb|x'|. The argument of \verb|f| is changed to be \verb|x'| and the body of \verb|f| is replaced with an assignment \verb|x = x'| and a while loop on the \verb|continue| variable. This loop sets \verb|continue| to false and \verb|result| to the result of evaluating the original body of \verb|f|. A tail call \verb|f(y)| is replaced with \verb|x = y; continue = true|. This will therefore execute the body of \verb|f| again with the new argument each time a tail call occurs. When the loop eventually exists, \verb|result| is the value the function returns. When \verb|f| is a curried function with multiple arguments, a new variable is created for each argument.
% Mention that linearised tree may need to be rewritten

\begin{minipage}{0.3\textwidth}
\begin{verbatim}
let rec f x = 
  ...
  f(y)
\end{verbatim}
\end{minipage}
\hfill $\Longrightarrow$ \hfill
\begin{minipage}{0.6\textwidth}
\begin{verbatim}
let rec f x' = 
  continue = true; result = 0; x = x';
  while (continue){
    continue = false;
    result = {
      ...
      x = y; continue = true
    }
  }
  result
\end{verbatim}
\end{minipage}

Above we were able to include the body of the function in the new function. For mutually tail recursive functions, the while loop must execute one of a number of mutually recursive functions so this is not possible. Instead, the body of \verb|f| is moved to a new function \verb|_f| taking a unit argument and the new function \verb|f| just executes a while loop, calling a function selected by a variable \verb|next|, set at each tail call. \verb|next|, \verb|continue| and \verb|result| are all shared global variables in this case, so they can be set by different functions. A shared set of global variables is also used to pass arguments at tail calls, and their values are bound to the original function variables when \verb|_f| executes.
% Give pseudocode for this too?

% Describe graphs


\subsection{Copying expressions, variables and constants}
As OCaml is a mostly functional language, local variables in the IR are only assigned to once (excluding variables introduced specially for tail call optimisation, which are marked as mutable and ignored), making these optimisations relatively straightforward. 

% TODO: Describe IR structure in more detail e.g. imm/compond/linast and labelled of nodes as Local|Export|Mutable
First, the program is scanned and each \verb|let x = Constant c| or \verb|let x = Ident y| binding is added to a table. % (unless \verb|x| is one of the variables introduced by tail call optimisations and is therefore mutable). 
These are ``immediate" values in the linearised IR and indicate bindings that do no real work. Whenever a variable \verb|x| present in the table is used, it is replaced with its known value. 

This is also extended to values passed through immutable memory, such as tuples, constructor fields or some record fields. During translation to the IR, these expressions in OCaml's \verb|Typedtree| are translated to the \verb|Block| expression in the IR, and tagged with an \verb|ImmutableBlock l| annotation, where \verb|l| is a boolean list indicating which of the block's fields are immutable. \\
When the IR tree is analysed, the variables or constants stored in the immutable fields of a block are added to that node as another annotation. A table mapping variables to annotations records this when the block is bound to a variable, so this propagates to any uses of the variable too. This information is also approximated at branches such as \verb|if| and \verb|switch| statements, by taking the intersection of the known fields when each branch returns a block. For example, \verb|let p = if cond then (x, y) else (x, z)| would annotate the \verb|if| node and hence also \verb|p| with that fact that its first field will be \verb|x|. \\
When an immutable field of a block is accessed as \verb|Field(x, i)|, if this annotation is present on \verb|x| and contains its element at index \verb|i|, the node is replaced with the known constant/variable to propagate this value through the program.

% Could have done some sort of flow based analysis here but didn't, do I want to?
%That alone is not enough to detect fixed values being propagated through memory. There are four sources of memory blocks in the OCaml Typedtree (ignoring floats, which are treated as constants until later stages, and closures, which are hidden in this representation): constructors, tuples, arrays and records. Of these, constructor and tuple fields are always immutable and record fields are unless specified as mutable. Therefore, during translation to the IR, although all of these are mapped to a single \verb|Block| structure, constructors, tuples and records are annotated with a list of booleans for which of their fields are immutable. We also annotate the nodes for these blocks with the constants or variables stored to each of those immutable fields. 
% Describe this process in more detail? Relates to constant propagation too.
% TODO: Could just describe by data-flow equations?
% Doing this across function calls would require CFA, which gets complex when not talking about function values
%While traversing the IR tree, variable bindings copy these annotations to a table mapping identifiers to annotations, which are copied back onto nodes in the tree where these variables are used. Then, when a \verb|Field(x, i)| node is encountered, if such an annotation is present on \verb|x| then we check if element \verb|i| of the list is immutable. If so, the field access is replaced with the variable or constant known to be stored there. \\
%Where possible, this information is also preserved across branches by taking the intersection of lists present on each branch if they exist. For example, consider the program \verb|let p = if cond then (x, y) else (x, z) in p[0]|. We would annotate the nodes forming each of the pairs, then the node corresponding to the \verb|if| statement would be annotated with the list \verb|[Some x, None]| and this would be used to identify \verb|p[0]| as \verb|x|.
% Annotations not carried across function arguments/results, would get a lot more complicated and need to make some approximations to ensure termination

As well constant and identifier assignments, common subexpressions are optimised too. When a binding is found of an expression which does not modify program state and has the same value whenever it is evaluated, that binding is stored in a table of common expressions. Identifying such expressions is done by the effects analysis described below. Then, whenever such expressions are encountered in the IR tree and are present in the table, they are replaced by the identifier known to hold that value. So that variables are only reused when in scope, each entry is removed from the table after the subtree below its binding is searched.

These passes tend to result in unused variables due to expressions being replaced with their known values. Therefore, after all of these passes have been performed,  dead assignments are identified as variable assignments that are never used. Of these, ones that have no side effects can then be removed. \\
These optimisations also increase the number of cases found for tail call optimisation. Where a function \verb|f| is assigned to another variable \verb|g| and that is used instead, tail call optimisation will not identify which function is being called by \verb|g|. These optimisations replace some such cases with using \verb|f| directly, in which case the function can now be identified and the call will be considered for tail call optimisation.

% Doing things in copy propagation/CSE doesn't make much difference overall

% TODO:
% CSE, evaluating simple expressions, optimising branches and dead assignments
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%

% TODO: Make side effect analysis iterative to solve functions?
\subsection{Effects analysis}
OCaml is an impure language, so analysis of side effects is necessary to determine when expressions can be safely replaced or removed as above. This is done using the following annotations on nodes in the IR tree:

\verb"annotation ::= ... | Pure | Immutable | Latent of annotation list"

\verb|Pure| identifies expressions that do not modify program state, making them safe for dead assignment elimination to remove. Of the pure expressions, \verb|Immutable| identifies expressions that will return the same value each time they are evaluated, so common subexpression elimination can remove repeated occurrences of such expressions.

For the purpose of removing dead assignments, creating a block in memory is considered pure as it could be immediately garbage collected if unused, and immutable if every field of the block is immutable. Accessing a field of a block is pure, and is immutable if that field of the block is immutable. 

For branches in \verb|if| and \verb|switch| statements, %the condition/switch argument is always an integer variable or constant, so
 the annotations on the overall statement are the intersection of the annotations on each branch, therefore safely underestimating these properties. That is, the expression is only pure if all branches are pure.

%Due to the IR being linearised, the argument to an if or switch statement is always an immutable integer variable, so these expressions are immutable/pure if each possible branch is, which is easy to check. \\
% Describe in terms of annotations?
Functions are the most interesting case since the function definition itself is pure and immutable, but the body of the function may cause latent side effects when the function is called. After analysing the body of the function, the annotations of the body are copied into a \verb|Latent| annotation and the function is annotated with this as well as \verb|Pure| and \verb|Immutable|. This wrapping of annotations is repeated for each curried argument. \\
The reverse occurs at function application, unwrapping the \verb|Latent| annotation as the analysis of each argument application. If at any point in unwrapping these, the function is not also marked as \verb|Pure| or \verb|Immutable|, the missing annotation is removed from the final annotations of the application node. This accounts for any side effects in partially evaluating the function, such as for \verb|let f x = (z := 5; (fun y -> x + y))|. Here, \verb|f 1 2| is valid as \verb|f 1| returns a function. Although the returned function is pure and will be annotated with \verb|Latent {Pure, Immutable}|, the first application is impure so the application node is not marked as being a pure expression.

%The last case of interest is functions. Defining a function causes no immediate side effects so is both pure and immutable, but we still need to capture the latent effects incurred if that function executes. This is done by first analysing the function body, then attaching a third property to the function node in the tree to indicate the latent behaviour of the function body. This latent effect property can also be nested to describe the behaviour of curried function applications. \\
 %At a function application, if this annotation is present then it is unwrapped once for each argument to get the purity/immutability of the result of that application, assuming that the pure and immutable properties are present each time the latent effects are unwrapped. If there is a level of latent effects not containing the pure/immutable property, that means we have a function which performs some side effect then returns another function, so the results is not considered pure/immutable accordingly. Also, if the latent effects property is not present, then the function assigned to the variable being applied could not be determined and nothing is assumed about its behaviour. % This rules out inferring properties of recursive functions, which in general is a good idea to preserve termination behaviour. Also, most dead/common expressions are the result of compiler optimisations, and it is rare for these to be recursive function calls
% Handling of purity properties passed through memory is similar to long description below, only worth mentioning in one place. 
% TODO: We don't actually care about the immutability of linast terms, they aren't CSE optimised are they? Instead just rely on each compound being replaced?



\subsection{Uncurrying}
% IS THIS WORTH MENTIONING? ALREADY HAVE ENOUGH DIFFERENT THINGS TO TALK ABOUT WITHOUT DISCUSSING FAILURES

%Functions which take many curried arguments are implemented as several functions, where each just constructs the closure to the next until the last evaluates the function. I looked at implementing this a different way. All but the last function just copy the closure then additionally write the argument of that call, one of the curried arguments, into the closure. Therefore, I included an integer of the number of arguments remaining, and made the original closure allocation large enough to store space for them. Then, each application just stores the argument in the closure and decrements the number of arguments remaining. When this value reaches 0, the last application is done as an actual function call. \\
%The hope was that, by no longer producing a function for each argument, the output file size would be reduced. Also, a closure may be reused in several places so often needs copying anyway, but not when we have several applications to a curried function, in which case we know the intermediate partially applied functions are not being stored anywhere. Therefore, we avoid function applications which effectively just copy the closure every time so may also save on execution time and memory usage. \\
%Unfortunately, the overhead of checking and modifying the number of arguments remaining on each function application, and the fact we still need to copy closures often enough, mean that this change did not improve any of the metrics and in some cases could increase both the filesize and memory usage by about 20\%, despite now having fewer functions defined.
% GO FIND THIS DATA!

A curried function of many arguments is implemented in WebAssembly as several functions, where all but the last function just constructs the closure to the next function, copying across free variables in the closure and the curried argument. \\
This is made more efficient for functions that are not exported and are always called with all of their curried arguments.  In this case, we can replace the set of curried functions with one function that takes all of its arguments as a tuple, and update the calling sites accordingly. Rather than actually creating a tuple in memory and passing that to the function (as in a function like \verb|f (x, y) = x + y|), the WebAssembly function takes multiple arguments directly, saving on memory usage. \\
Such functions and calls to them are tagged with an annotation in the IR tree, so that translation to WebAssembly can distinguish them from function definitions/calls using curried arguments. \\
This optimisation requires that every call to the function can be identified precisely, since some other call that is not annotated would be compiled to a function call passing one argument at a time, which would be incorrect. For this reason, the optimisation is only done when every occurrence of the function variable is an application, so it is never bound to another variable or passed between functions. The compiler then relies on copy propagation described previously to transform cases such as  \verb|let g = f in g(1,2)| to direct uses of \verb|f| and removing \verb|g|, allowing \verb|f| to be optimised. \\
This could also be done where a function always has more than one but not all of its arguments applied, but the benefit would not be as significant.


\subsection{Inlining}
Only non-recursive functions are inlined, enabling values to be propagated inside the function body and some expressions to be evaluated statically. This is done using some simple heuristics based on those described by Bonachea for the Titanium compiler\nocite{titanium}. First, functions with bodies containing at most 5 nodes in the IR tree are always inlined, as there is little cost in doing so and this handles very simple functions, such as accessing a field of a value or loading a mutable value. Also, if a function is not exported from the program and its only use is in a single application, this is also always inlined as the function definition can then be removed, so there is no net increase in code size.\\
 Lastly, larger functions are considered for inlining if doing so does not exceed the budget for code size. The original size of the program is stored in terms of the number of nodes in the IR tree, and larger functions may only be inlined if the resulting program is less than 50\% larger than its initial size. This limit was found to be enough that useful inlining occurred but the output code size did not change significantly once optimisations on the inlined code were also performed.

% TODO: Better justify these heuristics
% Also use smarter heuristics like number of function calls the function being inlined makes, or if it is being called from within a loop or not (more calls, bigger impact)


\subsection{Simplifying expressions and peephole optimisations}
Lastly, several simple optimisations are made at the IR and WebAssembly level. Integer or floating point operations in the IR tree involving constants are evaluated at compile time. Similarly, a \verb|switch| or \verb|if| statement switching on a constant is replaced with the branch known to be taken. Both of these allow function applications to be simplified after they are inlined, reducing code size and execution time. 

At the WebAssembly level, live variable analysis is used to identify unused assignments to variables that can be removed, in addition to being used for register assignment described previously. Peephole optimisations also simplify several common cases resulting from compiling each operation independently. For example, \verb|LocalSet i; LocalGet i| is replaced with \verb|LocalTee i|, and \verb|i32.const n; drop| is removed (\verb|i32.const 0; drop| is commonly seen where a unit valued expression is the first part of a sequence of expressions such as \verb|e1; e2|). These changes make clearer which values are actually used, which can allow further assignments to be removed by identifying them as assigning to dead variables.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Garbage Collection}
One of the last extensions to my project was to implement a garbage collector as part of the runtime system. Due to its complexity compared to the other runtime functions and going through several improvements, this was written in JavaScript and imported into the WebAssembly runtime rather than writing it in WebAssembly. I initially implemented a reference counting garbage collector, but changed to a tracing implementation to be able to collect cyclic data structures.

% TODO: Talk about Wasm proposal for reference types/garbage collection?

\subsection{Shadow stack}
A tracing collector marks all objects reachable from pointers on the call stack and global variables, then collects all allocated unmarked objects. However, the stack in WebAssembly cannot be directly inspected, so only the current function's local variables are accessible. The solution to this is to implement a shadow stack, reserving a section of WebAssembly's linear memory for storing copies of local variables from the call stack. The only bookkeeping required is a pointer to the top of the shadow stack, which moves up and down as functions are called and return.

Although only pointer values need to be stored on the shadow stack, identifying pointers requires propagating type information through all stages of the compiler and would still be imprecise for polymorphic functions. Instead, all local variable assignments in functions first copy the new value to the corresponding slot on the shadow stack, and checking which values are pointers is left till when garbage collection runs. This additional store on every local variable update is one of the main overheads of implementing garbage collection.


% TODO: Move to preparation chapter!
%\subsection{WebAssembly Stack Structure}
%Function activation records are part of WebAssembly's implicit stack, which distinguishes stack entries as values, labels of control flow blocks, and activation records.% https://webassembly.github.io/spec/core/exec/runtime.html 
%While most operations can only interact with the top of the stack and WebAssembly's type checking prevents a function from being able to access values on the stack below the last activation record or label, function arguments and local variables are accessed with the \verb|LocalGet|, \verb|LocalSet| and \verb|LocalTee| (equivalent to a set followed by a get) instructions. 
%
%I chose to implement a mark and sweep algorithm since reference counting alone is insufficient to collect unused cyclic structures. Although these features are very convenient for producing WebAssembly code, they pose an issue for garbage collection as the garbage collector has no way to directly access the local variables of all function calls currently active, which along with the global variables determine the root set of reachable objects for a mark and seep algorithm. The solution to this is to implement a shadow stack in the linear memory, reserving a region at the start of memory to store copies of stack values as they are updated in functions. Besides a pointer to the top of the shadow stack, no other bookkeeping is needed since, besides the garbage collector, values are only every written there not read. The start function of a program therefore allocates space for the global variables, then each function just increases the stack pointer on entry and decreases it on exit. 
%
%Although only reference values need to be written to the shadow stack, determining this usually requires testing the last bit of a value (since only pointer values have to last bit set) so instead all local variable assignments are written to the stack and this check is left for when garbage collection runs. These memory stores therefore introduces an overhead compared to compiling programs to not use garbage collection. % TODO: Optimise this for obvious cases as peephole optimisation e.g. i32.const n ; local.set i -- clearly don't need to update shadow stack


\subsection{Simple memory allocator}
The garbage collector can be split into two components, one which provides \verb|malloc| and \verb|free| functions for manual memory allocation, and the other which implements the mark and sweep algorithm on top of this to decide which blocks to free. 

I initially implemented the first part following the algorithm described in K\&R\nocite{k_and_r}. A cyclic linked list called the free list is maintained, chaining together all of the free blocks in memory, and the runtime keeps a pointer to some block in the list. Each block has an 8 byte header containing the size of the block and a pointer to the start of the next block. % Dont bother mentioning alignment?
As all values in compiled programs are 32-bit integers or 64-bit floats, blocks are always aligned to at least 4 bytes so the least significant bits of each field are unused, leaving space for an allocated and marked flag. 

\verb|malloc| scans the free list until the first large enough block is found. Either the whole block is allocated if it is a perfect fit, else the size of the block is reduced and just the tail end of it is allocated. If there are no large enough blocks, the garbage collector is called. If that frees a large enough block then the list is scanned again, otherwise the WebAssembly memory is grown and a block allocated from the new space.
%When a block is requested, the algorithm is first-fit so the list is scanned for the first block large enough for the allocation. If less than one header size larger than the size requested, the block is removed from the list, marked as allocated, and returned. Otherwise, the size of the block is reduced and just the tail end of the block is returned. 


\verb|free| also scans the free list until the last unallocated block before the block being freed is found. The freed block is then inserted here in the list, keeping blocks in the free list in memory order. If it is adjacent to the block before or after it in the free list, the blocks are merged into one large block to reduce fragmentation.

The mark and sweep algorithm then determines which blocks to free. When invoked, the shadow stack is scanned and each block pointed to is marked and put onto a stack. This stack is maintained using the pointer fields in the headers of blocks, which are otherwise meaningless for allocated blocks not in the free list. A depth first search is then done, examining the fields of each object for pointers. When a pointer to an unmarked block is found, that block is marked and added to the stack. \\
The sweep part of the algorithm then walks over all blocks in memory using the size field in each block, clearing marked flags and freeing allocated blocks that were not marked and are therefore unreachable.

% TODO: Massively cut down details, don't need such low level information, especially as it is described in K&R
%\subsection{Basic Structure}
%The implementation can be divided into two parts, the \verb|malloc| and \verb|free| functions used for explicit memory management as in a language such as C, and the mark and sweep algorithm with determines when it is safe to call \verb|free|.
%
%The initial implementation of \verb|malloc| and \verb|free| was taken from K\&R, although inefficiencies in this were resolved later. \cite{k_and_r} % https://kremlin.cc/k&r.pdf - The C Programming Language, Kernighan and Ritchie
%The runtime maintains a free list where each cell is an unallocated block of memory, with its size and a pointer to the next block in an 8 byte header. All memory allocations are kept aligned to the header size so not all bits of the size and pointer fields are required, leaving space for an allocated and marked flag. The list is circular and initially contains just one large block for the whole of memory. \\
%When a block is allocated, the free list is scanned until the first large enough block is found then either that whole block is allocated if it is a perfect fit, else the block is shrunk and just the tail end of it is allocated. If no large enough block can be found, the runtime attempts to grow the WebAssembly memory and treats that as a new free block.
%
%When a block is freed, we again scan down the free list until the last unallocated block before the block being freed is found (if the block being freed is at the start of memory, this unallocated block will be the free block at the highest address). The block being freed is then inserted into the list and merged with the block on either side of it if those are also free. This helps to reduce fragmentation and ensures that the free list is always kept in memory order.\\
%
%
%% Idea about using a list for the allocated blocks and then 'copy collecting' to a separate list in mark phase and not even needing a marked bit, just freeing the old list at the end?? Would even work with binning but requires checking sizes
%The marking algorithm makes use of the pointer to the next free block in the header, which is still present on allocated blocks (since the size field is needed on allocated blocks) but otherwise unused as the block is no longer on the free list. Starting by marking all references from the shadow stack, it uses these pointers to maintain a stack of unexplored objects and does a depth first search of the reachable objects, setting the marked bit on all of them. \\
%The sweep algorithm then walks over all blocks in memory using the size field of each block. Whenever a block is marked it clears the flag, and if a block is allocated but not marked then it is freed.


\subsection{Optimised malloc and free}
The implementation above requires \verb|malloc| and \verb|free| to both walk along the free list, either to find a large enough block or to find where to insert a freed block. \verb|free| can be made to always run in constant time, as can \verb|malloc| for the majority of calls.

First, an additional 8 byte trailer is included in each free block, used as shown below. A flag indicating that a block is allocated is now kept in the unused bits of the size field in both the header and trailer. The free list is now doubly linked but is no longer made circular. 


  \begin{tabular}{c|c|c|c|c|c|} \cline{2-6}
Free block: &  previous trailer ptr & size & $\dots$ & size & next header ptr \\ \cline{2-6} \multicolumn{1}{c}{}  \\ \cline{2-6}
Allocated block: &  marked stack ptr  & size & $\dots$ & size & unused \\ \cline{2-6}
\end{tabular}

%This is enough to be able to free a block without scanning the free list. The pointer to the block being freed is equivalently a pointer to the end of the adjacent block before it, and the size field of the block gives the position of the block immediately after it. As there is an allocated bit and size field in both the header and trailer of all blocks, the procedure checks if either of these adjacent blocks is free. If the block above is free, it is removed from its position in the free list and merged with the block being freed by updating the size fields. If the block below is free, the block being freed is merged with it, again updating the size fields and moving the trailer to the new end of the block. Otherwise, the freed block still needs to be included in the free list, which is done by inserting it onto the front of the list. 

This is enough to be able to free a block without scanning the free list. From just the freed block's address, the procedure gets its size and checks the allocated flags of the blocks either side of it, which is possible since the block before has its size and allocated flag in its trailer. This determines if either block should be merged with the one being freed. Also, since the list is doubly linked, each adjacent free block has a pointer to its successor and predecessor block so can be removed from the middle of the free list without traversing the list. If neither block is merged, the newly freed block is inserted at the front of the free list, which means that the free list no longer keeps blocks in address order.

The cost of \verb|malloc| comes from scanning the free list for a large enough block. The solution to this is binning blocks into separate free lists based on their size, so scanning for a free block can skip searching any lists of blocks that will all be too small. All but the last bin fit exact sizes starting from the smallest possible allocation, then the last bin holds all larger blocks. The compiler supports limited array operations, so the size of most allocations is determined by user defined datatypes and the number of free variables in functions. Often both are relatively small hence a large proportion of freed blocks are placed in the fixed size bins.  
There is now an array of free list pointers, and \verb|malloc| scans each list in turn searching for a block, starting from the earliest bin that could contain a large enough block. The rest of the algorithm is mostly unchanged, except that \verb|free| must insert freed blocks into the correct free list, and shrinking blocks when allocating or merging blocks when freeing means that free blocks sometimes move between lists.

%%%%%%%%%%%%%%%%%%%%%%%%%%

%After implementing and testing the previously described approach, there were two inefficiencies to address. For both allocating and freeing a block of memory, the runtime first walks down the free list until it finds a block of the right size for allocating or the adjacent free block for freeing. With some additional information attached to each block, both of these traversals can be avoided to get \verb|malloc| and \verb|free| to run in constant time. 
%
%Rather than just a header, an extra 8 bytes is allocated as a trailer for each block and the free list is changed to be a doubly linked list. The trailer contains the size and the pointer to the next block is moved to the trailer, with the header now instead holding a pointer back to the previous trailer. This means that, when the free list is in memory order, the forwards and backwards pointers just skip over regions of allocated memory. The size is also present in the trailers of allocated blocks, although the pointers in allocated blocks are still just used to keep a stack for marking, so the pointer in the trailer is meaningless. As in the original implementation, the last bit of the pointers in the header and trailer indicate if a block is allocated, and the last bit of the header size field is used to indicate a block is marked during garbage collection.


% Give reference for binning? And suggestion of sizes to use?
%The expensive part of \verb|malloc| is needing to scan down the free list until a large enough block is found. 
%This can be avoided by splitting the free list into several lists, each for a specific range of sizes, and then having an array of pointers to the start of each list. 
%% TODO: Be specific about sizes chosen and justification e.g. http://gee.cs.oswego.edu/dl/html/malloc.html
%Freed blocks are therefore put into a bin based on their size. The first several bins fit blocks of exact sizes, starting from the minimum possible block size. There are then a few bins which fit ranges that double in size with each bin, then one last bin for any larger sizes. 
%As my compiler supports very limited array operations, the size of blocks allocated by a program is primarily determined by the complexity of datatypes defined and the number of free variables in functions. In many cases, these are relatively small so the fixed size bins make up most of the allocations. \\
%With these bins, allocation scans from the earliest bin of large enough size until it finds a non-empty bin, and allocates the first large enough block in that list. In cases where the block found is significantly larger than required, an extra check is introduced that the remaining block has not shrunk to a size where it needs to move bin. \\
%The only change to the \verb|free| function for this optimisation is the determine which bin a block needs to be placed in based on its size after possibly merging with free blocks adjacent to it.
% TODO: Include the 'list of lists' diagram!

A characteristic observed from tracing the behaviour of memory intensive test programs is that, until garbage collection fails to free a suitable block and memory has to be grown, the garbage collector typically has diminishing returns each invocation as the set of long-lived objects grows. This results in there being less memory available over time and the garbage collector being called increasingly frequently. Memory grows in multiples of WebAssembly pages, which are 64KiB. Across several test programs, once the memory being freed drops below about 1KiB, the garbage collector start being called very frequently and almost always results in memory growing eventually. Therefore, another optimisation was to grow memory whenever this threshold is crossed, even if a block large enough for the requested allocation is found. This doesn't affect the amount of memory used, as memory almost always grows anyway after this point, but all of the objects freed by several frequent calls to the garbage collector are now collected in a single pass when this large block of memory is next used up, reducing the overhead of garbage collection.










