\chapter{Implementation}

\section{Repository Overview}
\dirtree{%
.1 . 
.2 src.
.3 ocaml/\DTcomment{OCaml compiler's frontend for parsing/typing}.
.3 middle-end/\DTcomment{IR translation}. 
.3 runtime. 
.4 memory.js. 
.4 runtime.wast. 
.3 optimisations/\DTcomment{IR analysis passes and optimisations}. 
.3 graph/\DTcomment{Wasm flowgraph datatype and optimisations}. 
.3 codegen/\DTcomment{Translation from IR to Wasm flowgraph}. 
%.3 jeancomeson\DTcomment{Guillaume}.
.2 samples\DTcomment{OCaml regression test programs}. 
.3 results.txt\DTcomment{Expected output of test files}. 
.3 test.js\DTcomment{Script to instantiate each test and compare output}. 
.2 benchmarks.
.3 C/. 
.3 Grain/.  
.2 unit-tests/\DTcomment{Tests for isolated elements of translation/optimisation}.
}

\section{Administrative Normal Form IR}
% Reference for ANF?
From OCaml's Typedtree data structure, the first pass of the compiler translates programs to administrative normal form (ANF), which linearises the syntax tree so that the operands to operations are always constants or idnetifiers, with the exception of control flow construsts such as if/while statements. Expressions are broken down into immediate values, compound and linast (linearised AST) terms. The general structure is given below, with some details omitted.

\begin{verbatim}
linast = compound 
  | Sequence (compound, linast)
  | Let(Identifier, compound, linast) 
  | LetRec((Identifier * compound) list, linast)

compound = imm | unary_op imm | binary_op (imm, imm) 
  | Block (tag, imm list) | GetTag imm
  | Field (imm, index) | SetField (imm, index, imm)
  | If (imm, linast, linast) | Switch (imm, (int * linast) list)
  | Try (index, linast, linast) | Fail index
  | Function (Identifier list, linast) | App (imm, imm list)

imm = Constant | Identifier
tag = index = int
\end{verbatim}

%For example:
%
%\begin{verbatim}
%f(g(x), h(x+y))
%\end{verbatim}
%would be translated to
%\begin{verbatim}
%let v0 = g(x) in
%  let t = x + y in 
%    let v1 = h(t) in
%      f(v0, v1)
%\end{verbatim}

Temporary values are now explicit and the linear structure makes optimisations such as common subexpression elimination and dead assignment elimination easier to implement in this form. Code which may execute zero or many times cannot be completely linearised, for example the branches of an if statement, as only one branch will actually be evaluated. 

Given an expression in the Typedtree to translate to an immediate or compound term, we recursively decompose the expression, returning a list of let bindings for any temporary values required in its computation and an immediate or compound term for its final value. In this way, an initial call to translate an expression to a linast term identifies a list of setup operations and a final compound term, then combines these into a tree by chaining the setup operations together, terminated by the final compound term.


%Linearising is implemented using three mutually recursive functions which compile immediates (constants/identifiers), compound terms such as \verb|x + y|, and top level terms such as \verb|let x = e in e'| or \verb|e; e'|. The first two of these return a pair of the actual term needed, and a list of any setup operations extracted as part of linearising the expression being compiled. Compiling a top level term then converts this result and list of setup operations into a tree which performs all of the setup then evaluates the result.
% Give actual examples of code translation/algorithm cases?
% OCaml primitives handling?

% TODO: Describe 3-level structure of immediate, compounds and linast terms

\section{Pattern Matching}
The linearised intermediate representation also compiles patterns in OCaml's \verb|Typedtree| datatype to the code required to match them and bind variables within patterns. This includes combining the cases of a match expression or function into a single expression, making use of switch statements on integers where possible. 

I chose to implement the backtracking pattern matching compiler described by Le Fessant and Maranget \cite{ocamlpatternmatch}, which is also implemented in the OCaml compiler for its intermediate representation. A backtracking compiler minimises the size of the code generated, whereas a decision tree compiler may produce larger code but ensures that each pattern is examined at most once, and is the approach taken by the Grain compiler. When optimised, each approach typically has similar performance \cite{decisiontrees} however backtracking compilers appear to be the more common approach, and guarantee linear code size.


The idea is to have a vector of the values being matched, and a matrix representing the different cases and the action when each matches. An example of this is:

\begin{minipage}{0.45\textwidth}
\begin{verbatim}
let f l1 l2 = match l1, l2 with
  | [], _ -> e1
  | _, [] -> e2
  | x::xs, y::ys -> e3
\end{verbatim}
\end{minipage}
\begin{minipage}{0.55\textwidth}
 values $\begin{pmatrix}
\verb|l1| \\
\verb|l2|
\end{pmatrix}$, cases
$\begin{pmatrix}
\verb|[]| & \_ & \to & \verb|e1| \\
\_ & \verb|[]| & \to & \verb|e2| \\
\verb|x::xs| & \verb|y::ys| & \to & \verb|e3| \\
\end{pmatrix}$
\end{minipage}

The algorithm given in the paper is made more complex by the range of syntax OCaml supports, and to a lesser extent by the fact the IR is linearised. First, OCaml patterns can have guard statements which need to be tested if a pattern matches. Some of the compilation steps prefix additional variable bindings onto the actions, and the guard statement needs to be evaluated after these, but before any of the original action is evaluated. Therefore, each action $\to a$ is replaced with $\to a \ g \ b$ where $g$ is the optional guard expression and $b$ is the other bindings introduced.

The base cases of this algorithm occur when we reach a matrix with either no rows or no columns of patterns. If there are no rows, we have run out of possibilities so pattern matching fails and backtracks to another branch. If there are no columns, we have successfully matched the whole pattern. We then output code testing the guard for each action from top to bottom of the matrix, executing the corresponding action if the guard succeeds. Either some row is not guarded, in which case it runs unconditionally if that is reached, or every row is guarded in which case we similarly backtrack if all guards are false.

There are four broad cases for when the matrix is non-empty:
\begin{itemize}
\item \textbf{Variable rule}: If the first column has just variable patterns, the corresponding binding of the first value in the values vector is added to each row's bindings, then the first column of the matrix is discarded, as is the now matched value of the value vector.
\item \textbf{Constructor rule}: If the first column has just constructor patterns, several specialised matrices are produced, each removing incompatible rows and deconstructing the fields of the chosen constructor. The result of compiling each of these then forms the branches of a switch statement on the constructor tag. A default case captures any constructor with no corresponding rows, and fails so that matching backtracks. \\
$
\begin{pmatrix}
c(q_1, \dots, q_k) & p^1_2 & \dots & p^1_n & \to & \dots \\
c'(\dots) & p^2_2 & \dots & p^2_n & \to & \dots \\
\end{pmatrix}
\xrightarrow{\text{specialise } c}
\begin{pmatrix}
 q_1 & \dots & q_k & p^1_2 & \dots & p^1_n  & \to & \dots 
\end{pmatrix}
$
% OR variables are an issue even at this stage, mention difference to paper's approach here about extended scope
\item \textbf{OR rule}: If there is just one row and it starts with an OR pattern \verb"(p1|...|pk)", a $k \times 1$ matrix is produced expanding out the OR pattern. %, with each row having an empty action. 
The output is the result of compiling this matrix, sequenced with the result of compiling the rest of the original matrix. Simply duplicating the original row once for each case of the OR pattern would have duplicated all of the patterns in later columns. That could lead to exponential growth in code size, which backtracking compilation aims to avoid.
\item \textbf{Mixture rule}: If no other case is applicable, the matrix is split into an upper and lower matrix where one of the other rules can be applied to the upper matrix. The matrices are compiled separately, with failure in the upper matrix resulting in a \verb|Fail i| instruction. This is handled by the \verb|try ... with (i) ...| construct which backtracks to try the lower matrix instead.

\end{itemize}

This is how the paper describes the algorithm. In practice, OCaml has several different types that all fall under the ``Constructor" rule and had to be handled separately:
\begin{itemize}
\item \textbf{Constants}: Constants can be viewed as a datatype with infinitely many 0-arity constructors. For integers, these compile to a switch statement on the integer value, and this switch statement will always have a default case that backtracks, since it can never be exhaustive. The decision to implement this switch statement as a branch table or nested if-then-else statements is made at the WebAssembly generation stage based on how sparse the set of values is. \\
For floating point values, we must always use a chain of if-then-else tests so these are compiled as such at the IR level.

\item \textbf{Tuples}: Tuples are just constructors with a single variant of a particular size. Therefore, the matrix is specialised by just extracting the nested patterns, as every row has the same constructor so none are discarded. There is also no need for a switch statement, since there is only one case.

\item \textbf{Arrays}: In OCaml, array patterns look like \verb"[|p1, p2, ..., pk|]", matching a specific length. Again there are infinitely many constructors, each of different arity, so the code switches on the length of the array, stored as its tag, and extract the corresponding number of fields in each case. As there are infinitely many possibilities, a default case is always included.

\item \textbf{Records}:
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Description of algorithm in paper
%The basic algorithm represents the patterns to compile by a vector of values to be matched and a matrix, where each row is a corresponding vector of patterns to match against (where rows are tried in top to bottom order) plus some code, called an action, to execute if the values match that row. The handling of guard statements is not described. Without guards, when the algorithm reaches a $m \times 0$ matrix (so each row has an empty vector of patterns left to match), the pattern has now been matched so the action of the top row is returned. To handle guards, each row of the matrix is augmented to also have an optional compiled guard expression. When an $m \times 0$ matrix is reached, this compiles to a series of if-then-else tests of the guards of each row until a row without a guard is reached. If all rows are guarded, then we eventually reach a $0 \times 0$ matrix which corresponds to pattern matching having failed, so we escape to the enclosing statement testing any other patterns or trap if there is nothing else to test. \\
%As guard expressions can include variables bound by the pattern, bindings introduced by pattern matching also need to be kept separate from the action to perform when a match succeeds. This allows the guard expression to be inserted between these bindings and the body of the action, which must only execute if the guard succeeds. \\
%For describing examples, $p$ represents a pattern, $a$ is an action, $g$ is an (optional) guard expression, and $b$ are binds introduced by pattern matching. $v$ is the first value being matched against.

%Escaping out of failed matches is achieved by the intermediate representation having static exceptions in the form of a \verb|fail i| and \verb|try ... with i ...| expression. Therefore, the algorithm just remembers the closest enclosing handler at any point and escapes to that when pattern matching fails. Although regular exceptions are challenging to implement in WebAssembly, these static exceptions compile to simple blocks with branch instructions to escape out of depending on if the expression succeeds or fails.

% Mention that bindings and the actual action need to be kept separate so that the guard can be checked between them?
%Another complication is the additional cases that need to be checked for. The described algorithm considers three types of patterns which are variables $\_$ or $x$ (where the latter generates a binding of the value to $x$ when it is matched), constructors $c(p_1, \dots, p_k)$ and OR patterns $(p_1 | p_2)$. Compilation can then be broken down into a handful of cases based on the patterns in the first column of the matrix. If every pattern is a variable pattern, then the first value and first column of the matrix can be discarded after binding the value to any non-wildcard variables for each row. 

%$
%\begin{pmatrix}
%x & p^1_2 & \dots & p^1_n & \to & a^1 & g^1 & b^1 \\
%y & p^2_2 & \dots & p^2_n & \to & a^2 & g^2 & b^2 \\
%\_ & p^3_2 & \dots & p^3_n & \to & a^3 & g^3 & b^3
%\end{pmatrix}
%\to
%\begin{pmatrix}
% p^1_2 & \dots & p^1_n  & \to & a^1 & g^1 & (b^1, x=v) \\
% p^2_2 & \dots & p^2_n  & \to & a^2 & g^2 & (b^2, y=v) \\
% p^3_2 & \dots & p^3_n  & \to & a^3 & g^3 & b^3
%\end{pmatrix}
%$
%
%If instead every pattern is a constructor pattern, then the matrix is compiled into a switch statement on the variant tag of the first value. A case is generated for each constructor present in the matrix, where the body of the switch case is the result of compiling the matrix with all rows with incompatible constructors discarded, and any sub-patterns of the constructor pattern extracted out and appended to the front of each row. 
%
%$
%\begin{pmatrix}
%c(q_1, \dots, q_k) & p^1_2 & \dots & p^1_n & \to & \dots \\
%c'(\dots) & p^2_2 & \dots & p^2_n & \to & \dots \\
%\end{pmatrix}
%\xrightarrow{\text{specialise } c}
%\begin{pmatrix}
% q_1 & \dots & q_k & p^1_2 & \dots & p^1_n  & \to & \dots 
%\end{pmatrix}
%$

Although this is described as one rule, there are many types of OCaml patterns which are all treated as constructors and need to be handled slightly differently. Tuples are treated as a constructor type with a single variant, so a switch statements is not required, just extracting each of the values contained from the tuple and corresponding patterns. Records are similarly treated as a constructor type with only one variant. As each pattern may match different fields of the record, the fields extracted are the union of the fields mentioned by each row. Where a field is extracted but a row does not match on that field, a wildcard pattern is introduced in the row to effectively ignore that extracted value.  Fields in a record which no pattern mentions are ignored entirely.

$
\begin{pmatrix}
\{f_1=q_1\} & p^1_2 & \dots & p^1_n & \to & \dots \\
\{f_2=q_2\} & p^2_2 & \dots & p^2_n & \to &\dots \\
\{f_1=q_3; f_2=q_4\} & p^3_2 & \dots & p^3_n & \to &\dots
\end{pmatrix}
\to
\begin{pmatrix}
q_1 & \_ & p^1_2 & \dots & p^1_n  & \to & \dots \\
\_ & q_2 & p^2_2 & \dots & p^2_n  & \to & \dots \\
 q_3 & q_4 & p^3_2 & \dots & p^3_n  & \to & \dots
\end{pmatrix}
$

Patterns for arrays in OCaml look like \verb"[|p1, p2, ..., pk|]" and match an array of a specific length. Arrays are tagged with their length so an array of a certain type can be viewed as a constructor with infinitely many variants, each variant having arity equal to the length of array it matches. This therefore becomes a switch statement on the length of the array, and must always introduce a default case since there are infinitely many variants so the match cannot be exhaustive. Integer constants are similarly handled as a constructor with infinitely many variants, except that these variants all have arity 0 as there are no sub-patterns to match. Floating point constants can be pattern matched in OCaml however these must be compiled to nested if-then-else statements, since my IR only allows switching on integers, either an integer constant or the tag of some constructor. This means switch statements can always be compiled as branch tables in WebAssembly later. \\
% Do I want to mention that there can be other rows after the OR pattern
If neither of the previous rules apply then either the matrix  starts with an OR pattern in its first row, which can be expanded into multiple rows, or the matrix is split into an upper or lower part where each can be checked recursively, with a failure in the upper matrix being handled by trying the lower matrix (referred to as the mixture rule).\\
The last feature of the OCaml language that needs supporting is aliases, for example \verb"match v with ((x::xs) as lst) -> ...". These are handled by always preprocessing the first column of the matrix to replace aliases in each row with a binding of the first value to the aliased variable. Preprocessing also simplifies any or patterns in the first column, replacing \verb"(_|p)" with just \verb|_| since the second part of the or pattern would never be considered.


$
\begin{pmatrix}
(p^1_1\ \mathrm{as}\ q) & p^1_2 & \dots & p^1_n & \to & a^1 & g^1 & b^1 \\
(\_ | p^2_1) & p^2_2 & \dots & p^2_n & \to & a^2 & g^2 & b^2 \\
p^3_1 & p^3_2 & \dots & p^3_n & \to & a^3 & g^3 & b^3
\end{pmatrix}
\to
\begin{pmatrix}
 p^1_1 & p^1_2 & \dots & p^1_n  & \to & a^1 & g^1 & (b^1, q=v) \\
\_ & p^2_2 & \dots & p^2_n  & \to & a^2 & g^2 & b^2 \\
p^3_1 & p^3_2 & \dots & p^3_n  & \to & a^3 & g^3 & b^3
\end{pmatrix}
$


\subsection{Optimisations to pattern matching}
This process was then improved by adding several auxiliary data structures, which are described here rather than waiting till the optimisations section.

% MENTION USE OF EXHAUSTIVENESS INFORMATION EARLY ON! EXPLAINS WHY WE DON'T NEED A TEST ON THE LAST ROW!

First, there are cases where it can be useful to swap the order of rows in the pattern matrix. When no rule can be directly applied, we split the matrix into two parts and recursively match each of them, backtracking out of the first case if necessary. A simple program matching two lists can be equivalently written as (the action in each case is unimportant):

\begin{minipage}{0.45\textwidth}
\begin{verbatim}
match l1, l2 with
  | [], _ -> 1
  | _, [] -> 2
  | x::xs, y::ys -> 3
\end{verbatim}
\end{minipage}\qquad
\begin{minipage}{0.45\textwidth}
\begin{verbatim}
match l1, l2 with
  | [], _ -> 1
  | x::xs, y::ys -> 3
  | _, [] -> 2
\end{verbatim}
\end{minipage}

These are equivalent as the last two rows are incompatible. Although patterns must be matched in order, any pattern matching row 3 could never match row 2 so they are safe to swap. The second case is more efficient as we can handle rows 1 and 3 together with the constructor rule, and only need to split the matrix once rather than twice. Of course, the first case could also avoid splitting the matrix twice if rows 2 and 3 were matched on the second pattern before the first, in which case applying the constructor rule would be possible. The problem of selecting the optimal column is thought to be NP-Complete and heuristics for this are more commonly used for decision tree based pattern matching, and the paper does not suggest how row and column based optimisations could be combined. % Cite the end of the OCaml paper for this (e.g. NP-Complete statement)
% Do I actually need to mention this?

% TODO: Not why we use contexts!!!
% Actually quite complex - interaction between jump summaries, reachable handlers, and how extraction from context is used to choose handler on each exit of failed constructor rule (i.e. simpler exhaustiveness because some case can't actually occur)

%Next, in addition to the matrix storing patterns an actions, we maintain and extended to structure called a context, which does not include actions but keeps an additional list of patterns on each row, called the prefix, for the structure already matched by pattern matching. The issue with using just the matrix described previously is that we effectively forget about what has already been seen once part of a match succeeds. Therefore, 

% Very long explanation, would go better in preparation???
If matching some pattern fails and we need to backtrack, we backtrack to the nearest enclosing handler. In some cases, we already know that this handler will fail too due to patterns matched before we had to backtrack. If so, then we can speed up the process by considering all enclosing handlers and jumping to the nearest one that may actually succeed. For this reason each handler has an integer identifier \verb|i|.

As an example, consider the program below (written out of order to show how the improved mixture rule previously described would process it): % Cite this as being the example given in the paper
\begin{verbatim}
type t = Nil | One of int | Cons of int * t
match l1, l2 with
  | Nil, _ -> 1
  | Cons(x, xs), Cons(y, ys) -> 5
  | _, Nil -> 2
  | One x, _ -> 3
  | _, One y -> 4
\end{verbatim} % TODO: Can cut all this down a little?

In pseudocode, this is what it will compile to:  % TODO: Cite that this comes from the paper!
\begin{verbatim}
try
  try
    try
      switch (l1):
        case Nil: 1
        case One: fail 1
        case Cons:
          switch (l2):
            case Cons: 5
            case One|Nil: fail 1
    with (1)
      switch (l2):
        case Nil: 2
        case Cons|One: fail 2
  with (2)
    switch (l1):
      case One: 3
      case Cons|Nil: fail 3
with (3) 4
\end{verbatim}

The OCaml compiler already warns about non-exhaustive patterns, so we can use exhaustiveness information to determine that any tests in the last handler would be redundant, as pattern matching cannot fail from there if the original patterns were exhaustive.

The mixture rule groups the first two rows to be matched by the constructor rule, and divides the remaining rows into separate 1-row matrices tried in turn. The goal is to jump past one handler to an enclosing one, so \verb|try ... with i ...| blocks are formed in bottom-up order so that matching of the first two rows is enclosed by 3 handlers. \\ % TODO: Add a diagram/pseudocode for this?
For a pair of values such as \verb|Cons(a, b), One(c)|, we first attempt to match against just the first two rows. We successfully match the \verb|Cons(x, xs)| constructor but fail on the second row. Naively, we would now try the first handler, which tests the second value against \verb|Nil| again and fails, then goes to the second handler which tests the first value against \verb|One x| and so also fails. Given the tests that had already been performed when we first failed, we knew the value had the form \verb|Cons(_, _), One(_)| and that this is incompatible with the matrices of both of the first two handlers, so we could have immediately jumped to testing the last row.

To track this information, we additionally maintain a list of reachable handlers, mapping their indices \verb|i| to the remaining matrix that handler tries to match. Similar to how rows of matrices are manipulated in the original algorithm, we can specialise the rows of these handler matrices as pattern matching progresses, removing incompatible rows. If one  of the cases when matching a constructor pattern then requires us to backtrack, we can now go to the first handler with a non-empty matrix in the reachable handlers, since all previous handlers will immediately fail. \\

% Include optimised version of code?
% TODO: Contexts and jump summaries

Having done this, it reveals another form of inefficiency. We can only reach a particular handler from exits at specific points in the code and, having successfully matched patterns up to those points, the set of possible values when that handler is reached is restricted. In the example above, we now jump straight to 
the last handler from line 10 when the second list is a \verb|One _| value, splitting the old \verb"case One|Nil" line into two different exits. This means that the first handler is only entered from one of two locations, when \verb|l1 = One v| or when \verb|l1 = Cons (v, vs)| and \verb|l2 = Nil|. In the latter case, we will return 2, but in the first case we exit to the second handler. This handler then checks if \verb|l1 = One v|, but we just established this is the only situation in which the second handler executes. Therefore, the other cases of the switch statement can be discarded and the whole second handler reduces to just the value 3.

To identify these case, we maintain an extended version of a pattern matrix called a context, which does not include actions but keeps an additional list of patterns on each row, called the prefix, for the structure already matched by pattern matching. As with the reachable handler list, the operations of pattern matrices are extended to apply to this, with additional operations to push or pop pattern between the prefix and the fringe of patterns yet to be matched. Now, as well as returning code to match the patterns in a matrix, the procedure recursively computes a jump summary, mapping each handler index to the union of the contexts at all points that exit to that handler. Recursive calls push patterns onto the prefix of the current context, as they match sub-patterns in a more specialised context, and returns pop patterns off the prefixes of context in the jump summary as we return to higher up points in the pattern matching process. 

Now, when we compile a handler, we take its context to be the context mapped to in the jump summary returned from the try block. When we construct a switch statement for the constructor rule, we can exclude any constructor patterns which are incompatible with all rows of the context, indicating that we will never see that constructor for values reaching this point in the pattern matching code. \\

% OR PATTERNS - DIFFERS FROM PAPER!
% Is this actually something I want to draw attention to???
% Should probably change this in my code to do it the other way, as it only every applies to OR patterns doesn't it? Can easily determine which variables need to be passed out.
My implementation differs from the one described in the paper by not also specifying variables passed through to handlers in addition to a handler index. When an OR pattern \verb"(p1|...|pk) q1 q2 ..." is split into several rows, unless it is the only row of the matrix, we introduce an exit to a handler which matches the remaining patterns \verb|q1 q2 ...|. This is done to avoid duplicating the patterns \verb|q1 q2 ...| in the original matrix, which could result in duplicate code across switch cases. The issue here is that the patterns \verb"p1|...|pk" could each bind some variable \verb|x|, which is then in scope when we enter the handler. The paper handles this by listing all such variables at the exit and at the start of the handler block. \\
Because the parser in the compiler front-end assigns a unique identifier to each variable, there is never a scoping issue where the \verb|x| in the handler could refer to the same \verb|x| declared elsewhere. Even with variables listed on exits and handers, we still need an extra data structure to remember identifiers outside of the subtree they are defined when performing analyses or translations, for example so tat we know which lower level local variable the identifier was mapped to when we come to translate the handler. Therefore, I chose to leave out these lists of variables since they serve little purpose other than to make such scoping complications explicit, so exits and handlers are only annotated with the index of the handler they correspond to. \\
This scoping complication disappears at the WebAssembly level, since static exceptions compile to code looking more like a standard \verb|break| statement, jumping to the next block (the handler) at an exit, or jumping past the next block if pattern matching ultimately succeeds.
%This is not necessary as parsing uniquely identifies reuses of the same variable name in different scopes, so a table can track all the variables seen so far without having to deal with renaming issues, at the relatively small overhead of needing to remember more variables simultaneously.
%%% ALSO PREVENTS OPTIMISING, BUT VERY RARE ANYWAY?



In summary, the compilation scheme now has three additional arguments, a boolean indicating if we know the current matrix of patterns to be exhaustive, a context giving additional information on the patterns already matched, and a list of the reachable trap handlers with the matrix each handler matches. It also returns both the code to match a given matrix, and a summary of the set of values which can cause the code to exit to each of the enclosing handlers. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Compiling Identifiers}
After any optimisations on the IR are done, the linearised tree is lowered to WebAssembly in two stages. The first of these pulls function definitions out to the top level of the program, making closures and free variables explicit, converts the tree to lists of instructions (with nesting still present for \verb|If|, \verb|For| and \verb|While| statements as WebAssembly supports nested blocks) and replaces identifiers with integer indexes.

Identifiers are replaced with a datatype indicating whether that variable is accessed as a global variable, a function argument, a local variable of the function or a variable in the function's closure, with an integer index into the corresponding set of variables. Initially, assignment of local variables was done following the scoping of the program, so each nested let binding would use one more local variable and a \verb|for| loop would use two, one for the loop variable and another to remember the upper limit of the loop the first time it is evaluated. \\
This was replaced with generating a new local variable for each identifier in the function, and keeping these distinct even when generating WebAssembly. Then, after performing peephole optimisations on the WebAssembly, I perform live variable analysis, which determines the points in the program where each local variable's value could be used later so cannot be overwritten. From this, I construct a clash graph with edges between every pair of variables that are ever simultaneously live so cannot be allocated to the same local variable. A graph colouring heuristic then maps these local variables to a hopefully much smaller set of local variables and unused local variables are removed from the function. % TODO: Better to describe register allocation and LVA somewhere else? Maybe even in preparation chapter?

This register allocation process is necessary for lower level target languages like x86 where there is only a finite set of registers available. Although a WebAssembly function can be defined with arbitrarily many local variables, it is likely that a platform's WebAssembly implementation will use less memory by using fewer locals, and when garbage collection is later described this directly affects the height of the shadow stack used to track local variables, making this a useful optimisations.

%After any optimisations are performed, the linearised tree is then compiled to a lower intermediate form. The tree is replaced with a list of instructions, and identifiers are resolved into integer indexes for argument, local, global or closure bound variables. 
% Give more detail about blocks in WebAssembly
%Conversion to a list of instructions is straightforward since WebAssembly contains nested blocks. \verb|Block| constructs in WebAssembly contain a list of instructions and introduce a label at the end of the block, which can be jumped to from within the block. \verb|If| blocks are similar but contain two lists of instructions, and \verb|Loop| blocks have the label at the start of the block, allowing code to be repeated. 
%Rather than map to these straight away, programs are still described using \verb|if| and \verb|while| constructs, which are easily mapped to WebAssembly at the code generation stage. % Put examples when we get there, not here

%This stage also lifts any function declarations to the top level of the program, since closures and environments are now explicit, so the result is a list of functions. The complexity here is in determining the free variables of a function and the number of local stack variables needed by the function body, both of which are done by simple recursive algorithms. The number of locals needed is the maximum needed at any point in the function body. This increases by one after each let binding is evaluated, and for loops also require two to remember the current and end values for the loop (for loops in OCaml have a very constrained \verb"for var = i [up|down]to j do ... done" syntax). 

% Currying removed at this point. Wasn't being used anyway so why do I allow it at all? Does it actually provide any benefit to me?

\section{Code Generation}
WebAssembly generation is then straightforward, as each of the constructs and primitive operators in my lower IR are all simple to implement. The variable bindings in the lower IR were separated into argument, local, global and closure bindings. Argument and global bindings translate directly to Local and Global operations respectively. The locals in a function are laid out as function arguments, then swap space locals used to implement some of the operations in WebAssembly, then locals allocated above for let bindings and loops. Therefore, local bindings are offset by the number of swap spaces and function arguments. The first argument to each function is its closure, so closure bindings first get local 0 then do a load from that address, with an offset selecting the correct variable from the closure. 

I also wrote a runtime in WebAssembly providing functions for memory allocation, polymorphic comparison, boxing of floats in memory, and list append and some of the integer primitives. These are declared as imports to the WebAssembly module and each function index in my IR must be offset by the number of runtime functions imported. Additionally, the memory for the module is also imported from the runtime since it is managed by the allocation function from the runtime imports. Lastly, the global variables for each user variable declared at the top level of the OCaml program are exported, along with all functions from the module. Every function is exported so that values representing closures can be returned from WebAssembly and called. As integers are encoded by shifting them left by one, and so that pointers to closures can be used to call functions in WebAssembly from JavaScript, I added a JavaScript wrapper which uses knowledge of how values are represented in memory to be able to pass values back into compiled functions, or return integer values back to a JavaScript caller.

Even before implementing a garbage collector, data still needed to be tagged to identify its type so that OCaml's \verb|compare| function, which is defined on all types, would work correctly. As every operation is on either 32-bit integers or 64-bit floats, pointers are always aligned on 4-byte boundaries so the last 2 bits are not needed. I therefore tag integers as 10 or 00 by shifting them left one, closure pointers as 11, and other blocks from tuples or constructors as 01. In the last case, data blocks are represented as a variant tag, their arity, then the elements they contain.  \\
Since arguments and return values in WebAssembly are strongly typed and we need to support polymorphic functions, all values are 32-bit integers which are decoded as pointers to data or functions as just described. In particular, floating point values need to be ``boxed'' in memory and represented as a data block. The variant tag on data blocks goes from 0 to the number of variants of that type, so -1 is used as a special value to distinguish blocks storing a float value.

Although not strictly necessary, an arity field is included for both floating point values and closures. In the case of floating point values, it is always 0, and for closures it is the number of free variables. For floating point values, this ensures the 64-bit float value is aligned, which may improve memory access performance, and a floating point value can be treated as any other data block by the garbage collector, as the block will be inspected and the arity field indicates that we do not need to inspect the block further for other pointers. \\
In the case of closures, we need to know the number of free variables for garbage collection, so that pointers stored in the block can be followed in the marking algorithm. This again means all pointers can be treated uniformly by the garbage collector, checking first the arity field and then any potential pointer fields within the block, as indicated below:

  \begin{tabular}{c|c|c|c|c|c|m{2cm}|} \cline{2-7}
Constructor block &   variant tag  & arity &   \multicolumn{3}{c}{\hspace{2cm}{elements} } &  \\ \cline{2-7} \multicolumn{1}{c}{}  \\ \cline{2-7}
Tuple/Record &   0  & arity &   \multicolumn{3}{c}{\hspace{2cm}{elements}} &  \\ \cline{2-7} \multicolumn{1}{c}{}  \\ \cline{2-4}
Boxed float &  -1 & 0 & 64-bit float value      \\ \cline{2-4} \multicolumn{1}{c}{}  \\ \cline{2-7}
Closure & function index  & arity &   \multicolumn{3}{c}{\hspace{2cm}{free variables}} &  \\ \cline{2-7} 
\end{tabular}

%As OCaml limits the number of different constructors supported% Reference required
%, the variant tag is guaranteed to be a non-negative integer. Therefore, as floats need to be``boxed'' and stored in memory as a data block, -1 was used to identify floats, which are stored similarly to . This is necessary since the arguments and return values of WebAssembly functions are strongly typed. Therefore, all functions take a 32-bit integer which is decoded as an integer or pointer to data or a float as required by the body of the function.

\section{Optimisations}
\subsection{Tail calls}
Tail call optimisation is an important optimisation for functional languages, since loops in a pure functional language are written as recursive functions. Therefore, implemented naively a long iteration can easily exhaust the available stack space. This is avoided by wrapping the body of the function in a while loop and replacing the tail call with code that saves the arguments the function would have been called with and updates a variable to say the loop should not exit yet. This is extended to mutually tail recursive functions by additionally storing which function should be called next when a tail call is reached.

% Don't need so much detail?
The optimisation is done in two passes, the first of which just identifies which functions to tail call optimise. As the linearised tree for the program is traversed, we keep track of whether a function call at the current point in the tree would be a tail call (rather than being the body of a let binding, sequence of loop).
This allows us to mark every potential call site optimisation in the program and to construct a call graph consisting of just tail calls between procedures. Tail call optimisations only apply to fully applied curried functions, so we also discard any over or under-applied call sites.

As detailed below, tail call optimising a recursive function increases its size, and the optimisation is more expensive for mutually tail recursive functions as each function must be split in two. Therefore, this call graph is now iteratively pruned to identify functions worth optimising. Functions with no tail calls are removed from the graph, as are functions which only make tail calls to themselves so should be optimised in isolation. Once the graph becomes stable, we have the set of functions to make mutually tail recursive. Tail calls between these functions will then all be rewritten to avoid making additional function calls.

%
%% Body could be a tail call, not that we currently are a tail call
%\begin{itemize} % TODO: Find better way to describe this
%\item The body of a recursive binding to a variable \verb|f| is traversed as being a tail call position for the function \verb|f|.
%\item The body of a non-recursive binding or first part of a sequence is not a tail call position.
%\item The condition and body of a while loop and the body of a for loop or anonymous function are not tail call positions.
%\item If the current position is viewed as a tail call position for some function \verb|f|, then so is the body of each branch of an if statement or switch statement.
%\item If the current position is viewed as a tail call position for some function \verb|f|, then so is the remaining expression after a recursive or non-recursive binding or first part of a sequence.
%\end{itemize}
%These rules allow keeping track of whether an application is in a tail call position or not as the tree is traversed. Each variable declared by a recursive binding to a function is also added to an initially empty table \verb|tbl| and mapped to an empty set of variables and the function's arity. When an application of \verb|g| is found in a tail call position for some function \verb|f|, we check two things. First, \verb|g| must be known at compile time to be one of the recursive functions declared in the program and therefore present in \verb|tbl|. 
%% TODO: Is this obvious
%This is because we can only rewrite tail calls to functions optimised to make tail calls, the reason for which will be apparent when the changes made to functions are described below.
%Second, the number of arguments applied to \verb|g| must match the arity recorded for \verb|g| in the table. This allows curried functions to be tail call optimised. If the function is under-applied, then we do not have all of the arguments ready to evaluate it so it can not be tail called yet. If the function is over-applied, then the result of calling \verb|g| cannot be returned immediately so this is again not a tail call. if both conditions are satisfied, we add \verb|g| to the set of variables \verb|f| is mapped to in \verb|tbl|.
%
%At the end of this process, we have a table which stores the functions that are tail called within each recursive function. From this we determine which functions to tail call optimise on their own, and which to optimise as mutually tail recursive functions (which has a greater impact on code size since the function must be split into two functions). 

\begin{verbatim}
while graph changing:
  for each f in graph:
    if f makes no tail calls:
      remove f and all edges to it
    else if f only tail calls itself:
      remove f and all edges to it
      optimise f on its own
\end{verbatim}
%There is little to gain from rewriting a function which makes no tail calls, so these are not optimised and hence no other function can optimise tail calls to them. Due to the added code bloat of optimising a function for mutually recursive tail calls rather than just tail calls to itself, a function which could only optimise calls to itself is optimised separately and tail calls from other functions to it are also removed. This leaves us with all other functions benefiting from being made mutually tail recursive.\\

In the process of deciding on this method for choosing which functions to tail-call optimise on their own or as mutually recursive functions, I studied how the Grain complier implements this analysis. In doing so, I identified a bug where it would wrongly optimise certain tail calls, and submitted an issue to GitHub detailing this which has since been corrected. % Cite github issue?

% TODO: Better to just give pseudocode and leave out the details?
For a function \verb|f| taking argument \verb|x| which is to be made mutually recursive on its own, we create new variables \verb|next|, \verb|continue| and \verb|x'|. The argument of \verb|f| is changed to be \verb|x'| and the body of \verb|f| is then replaced with an assignment \verb|x = x'| and a while loop on the \verb|continue| variable. This loop sets \verb|continue| to false and \verb|result| to the result of evaluating the original body of \verb|f|. A tail call \verb|f(y)| is replaced with \verb|x = y; continue = true|. This will therefore execute the body of \verb|f| again with the new argument each time a tail call occurs. When the loop eventually exists, \verb|result| is the value the function returns. When \verb|f| is a curried function with multiple arguments, this is handled the same way but with a new variable \verb|x'| created for each argument \verb|x|.
% Mention that linearised tree may need to be rewritten

\begin{minipage}{0.3\textwidth}
\begin{verbatim}
let rec f x = 
  ...
  f(y)
\end{verbatim}
\end{minipage}
\hfill
\begin{minipage}{0.6\textwidth}
\begin{verbatim}
let rec f x' = 
  continue = true; result = 0; x = x';
  while (continue){
    continue = false;
    result = {
      ...
      x = y; continue = true
    }
  }
  result
\end{verbatim}
\end{minipage}

Above we were able to include the body of the function in the new function. For mutually tail recursive functions, the while loop must execute one of a number of mutually recursive functions so this is not possible. Instead, the body of \verb|f| is moved to a new function \verb|_f| taking just a unit argument and the new function \verb|f| just executes a while loop, calling a function selected by a variable \verb|next|.
% Give pseudocode for this too?

% Describe graphs


\subsection{Copying expressions, variables and constants}
Due to OCaml being a functional language, each local variable is assigned to once in the IR (with the exception of those variables introduced specially for tail call optimisation), making these optimisations relatively easy to implement. 

% TODO: Describe IR structure in more detail e.g. imm/compond/linast and labelled of nodes as Local|Export|Mutable
First, the tree is scanned and every \verb|let x = Constant c in ...| or \verb|let x = Ident y in ...| binding is added to a table (unless \verb|x| is one of the variables introduced by tail call optimisations and is therefore mutable). These are the ``immediate" values in the linearised IR and represent bindings that do no real work. Every variable use in the tree is looked up in the table, and wherever a corresponding entry is found that variable is replaced with its known value. 

% Could have done some sort of flow based analysis here but didn't, do I want to?
That alone is not enough to detect fixed values being propagated through memory. There are four sources of memory blocks in the OCaml Typedtree (ignoring floats, which are treated as constants until later stages, and closures, which are hidden in this representation): constructors, tuples, arrays and records. Of these, constructor and tuple fields are always immutable and record fields are unless specified as mutable. Therefore, during translation to the IR, although all of these are mapped to a single \verb|Block| structure, constructors, tuples and records are annotated with a list of booleans for which of their fields are immutable. We also annotate the nodes for these blocks with the constants or variables stored to each of those immutable fields. 
% Describe this process in more detail? Relates to constant propagation too.
% TODO: Could just describe by data-flow equations?
% Doing this across function calls would require CFA, which gets complex when not talking about function values
While traversing the IR tree, variable bindings copy these annotations to a table mapping identifiers to annotations, which are copied back onto nodes in the tree where these variables are used. Then, when a \verb|Field(x, i)| node is encountered, if such an annotation is present on \verb|x| then we check if element \verb|i| of the list is immutable. If so, the field access is replaced with the variable or constant known to be stored there. \\
Where possible, this information is also preserved across branches by taking the intersection of lists present on each branch if they exist. For example, consider the program \verb|let p = if cond then (x, y) else (x, z) in p[0]|. We would annotate the nodes forming each of the pairs, then the node corresponding to the \verb|if| statement would be annotated with the list \verb|[Some x, None]| and this would be used to identify \verb|p[0]| as \verb|x|.
% Annotations not carried across function arguments/results, would get a lot more complicated and need to make some approximations to ensure termination

As well constant and identifier assignments, common subexpressions are optimised too. When a binding is found of an expression which does not modify program state and has the same value whenever it is evaluated, that binding is stored in a table. Identifying such expressions is done by the analysis pass described below. Then, whenever such expressions are encountered in the IR tree and are present in the table, they are replaced by the identifier known to hold that value. So that we do not attempt to use variables that are out of scope, each entry is removed from the table after the subtree below its binding is searched.

These passes tend to result in unused variables due to them being replaced with their known values. Therefore, after each of these passes has been performed we identify dead assignments as assignments to variables that are never used elsewhere. Of these, those that have no side effects can then be removed from the tree.x

% Doing things in copy propagation/CSE doesn't make much difference overall

% TODO:
% CSE, evaluating simple expressions, optimising branches and dead assignments
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%

% TODO: Make side effect analysis iterative to solve functions?
\subsection{Effects analysis}
OCaml is an impure language, so some analysis of side effects is necessary to determine when the above optimisations are safe. This is done using the following annotations on nodes in the IR tree:

\verb"annotation ::= ... | Pure | Immutable | Latent of annotation list"

\verb|Pure| identifies expressions that do not modify program state, making them safe for dead code elimination to remove. Of the pure expressions, \verb|Immutable| identifies expressions that will return the same value each time they are evaluated, so common subexpression elimination can remove repeated occurrences of such expressions.

For the purpose of removing dead expressions, creating a block in memory is considered pure, as it would be immediately garbage collected if it is unused. It is also considered immutable if every field of the block is immutable so always has a fixed value. Accessing a field of a block is pure, and is immutable if that field of the block is immutable. \\
For branches in \verb|if| and \verb|switch| statements, the condition/switch argument is always an immutable integer so we can take the intersection of the annotations on each branch for the analysis of the overall statement, therefore safely underestimating these properties. 

%Due to the IR being linearised, the argument to an if or switch statement is always an immutable integer variable, so these expressions are immutable/pure if each possible branch is, which is easy to check. \\
% Describe in terms of annotations?
Functions are the most interesting case since they define expressions that may have latent side effects, as the function definition itself is pure and immutable, and any side effects only occur when the function is called. This is the purpose of the \verb|Latent| annotation. After analysing the body of the function, the annotations of the body are copied into a \verb|Latent| annotation and the function is marked with this as well as \verb|Pure| and \verb|Immutable|. This wrapping of annotations is repeated for each curried argument. \\
The reverse occurs at function application, unwrapping the \verb|Latent| annotation as the analysis of each argument application. If at any point in unwrapping these, the function is not also marked as \verb|Pure| or \verb|Immutable|, the missing annotation is removed from the final annotations of the application node. This accounts for any side effects in partially evaluating the function, such as for \verb|let f x = (z := 5; (fun y -> x + y))|. Here, \verb|f 1 2| is valid as \verb|f| returns a function, but the first application is impure despite the returned function being pure so the application is not marked as pure.

%The last case of interest is functions. Defining a function causes no immediate side effects so is both pure and immutable, but we still need to capture the latent effects incurred if that function executes. This is done by first analysing the function body, then attaching a third property to the function node in the tree to indicate the latent behaviour of the function body. This latent effect property can also be nested to describe the behaviour of curried function applications. \\
 %At a function application, if this annotation is present then it is unwrapped once for each argument to get the purity/immutability of the result of that application, assuming that the pure and immutable properties are present each time the latent effects are unwrapped. If there is a level of latent effects not containing the pure/immutable property, that means we have a function which performs some side effect then returns another function, so the results is not considered pure/immutable accordingly. Also, if the latent effects property is not present, then the function assigned to the variable being applied could not be determined and nothing is assumed about its behaviour. % This rules out inferring properties of recursive functions, which in general is a good idea to preserve termination behaviour. Also, most dead/common expressions are the result of compiler optimisations, and it is rare for these to be recursive function calls
% Handling of purity properties passed through memory is similar to long description below, only worth mentioning in one place. 
% TODO: We don't actually care about the immutability of linast terms, they aren't CSE optimised are they? Instead just rely on each compound being replaced?



\subsection{Uncurrying}
Functions which take many curried arguments are implemented as several functions, where each just constructs the closure to the next until the last evaluates the function. I looked at implementing this a different way. All but the last function just copy the closure then additionally write the argument of that call, one of the curried arguments, into the closure. Therefore, I included an integer of the number of arguments remaining, and made the original closure allocation large enough to store space for them. Then, each application just stores the argument in the closure and decrements the number of arguments remaining. When this value reaches 0, the last application is done as an actual function call. \\
The hope was that, by no longer producing a function for each argument, the output file size would be reduced. Also, a closure may be reused in several places so often needs copying anyway, but not when we have several applications to a curried function, in which case we know the intermediate partially applied functions are not being stored anywhere. Therefore, we avoid function applications which effectively just copy the closure every time so may also save on execution time and memory usage. \\
Unfortunately, the overhead of checking and modifying the number of arguments remaining on each function application, and the fact we still need to copy closures often enough, mean that this change did not improve any of the metrics and in some cases could increase both the filesize and memory usage by about 20\%, despite now having fewer functions defined.
% GO FIND THIS DATA!

An alternative approach I then looked at was to identify functions which were not exported and were always called with all of their curried arguments. This could also have been done where a function always had more than one but not all of its arguments applied, but the benefit would not be as significant. In this case, we can replace the curried function with one that takes all of its arguments as a tuple, and update the calling sites accordingly. Rather than actually creating a tuple in memory and passing that to the function (as in a function like \verb|f (x, y) = x + y|), the WebAssembly function takes multiple arguments directly, saving on memory usage. \\
Such functions and calls to them are tagged with an annotation in the IR tree, so that translation to WebAssembly can distinguish them from function definitions/calls using curried arguments. \\
This requires that every call to the function be identified, as a variable which could be one of two functions, each taking their arguments in different ways, would handle one case incorrectly and this optimisation would be unsafe. For this reason, the optimisation is only done when every occurrence of the function variable is an application. More complex analysis is possible to identify where the function is assigned to another variable and all of its uses can still be identified, such as in \verb|let g = f in g(1,2)|, but this was not implemented and such cases instead rely on copy propagation rewriting this to use \verb|f| directly and remove \verb|g|.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Garbage Collection}
One of the last extensions I made to my project was to implement a garbage collector as part of the runtime system. Due to the relative complexity compared to the other runtime functions and going through several iterations of its implementation, this was written in JavaScript and imported into the WebAssembly runtime program however the final version could have been mapped down to WebAssembly too.

% TODO: Talk about Wasm proposal for reference types/garbage collection?

% TODO: Move to preparation chapter!
\subsection{WebAssembly Stack Structure}
Function activation records are part of WebAssembly's implicit stack, which distinguishes stack entries as values, labels of control flow blocks, and activation records.% https://webassembly.github.io/spec/core/exec/runtime.html 
While most operations can only interact with the top of the stack and WebAssembly's type checking prevents a function from being able to access values on the stack below the last activation record or label, function arguments and local variables are accessed with the \verb|LocalGet|, \verb|LocalSet| and \verb|LocalTee| (equivalent to a set followed by a get) instructions. 

I chose to implement a mark and sweep algorithm since reference counting alone is insufficient to collect unused cyclic structures. Although these features are very convenient for producing WebAssembly code, they pose an issue for garbage collection as the garbage collector has no way to directly access the local variables of all function calls currently active, which along with the global variables determine the root set of reachable objects for a mark and seep algorithm. The solution to this is to implement a shadow stack in the linear memory, reserving a region at the start of memory to store copies of stack values as they are updated in functions. Besides a pointer to the top of the shadow stack, no other bookkeeping is needed since, besides the garbage collector, values are only every written there not read. The start function of a program therefore allocates space for the global variables, then each function just increases the stack pointer on entry and decreases it on exit. 

Although only reference values need to be written to the shadow stack, determining this usually requires testing the last bit of a value (since only pointer values have to last bit set) so instead all local variable assignments are written to the stack and this check is left for when garbage collection runs. These memory stores therefore introduces an overhead compared to compiling programs to not use garbage collection. % TODO: Optimise this for obvious cases as peephole optimisation e.g. i32.const n ; local.set i -- clearly don't need to update shadow stack


% TODO: Massively cut down details, don't need such low level information, especially as it is described in K&R
\subsection{Basic Structure}
The implementation can be divided into two parts, the \verb|malloc| and \verb|free| functions used for explicit memory management as in a language such as C, and the mark and sweep algorithm with determines when it is safe to call \verb|free|.

The initial implementation of \verb|malloc| and \verb|free| was taken from K\&R, although inefficiencies in this were resolved later. % https://kremlin.cc/k&r.pdf - The C Programming Language, Kernighan and Ritchie
The runtime maintains a free list where each cell is an unallocated block of memory, with its size and a pointer to the next block in an 8 byte header. All memory allocations are kept aligned to the header size so not all bits of the size and pointer fields are required, leaving space for an allocated and marked flag. The list is circular and initially contains just one large block for the whole of memory. \\
When a block is allocated, the free list is scanned until the first large enough block is found then either that whole block is allocated if it is a perfect fit, else the block is shrunk and just the tail end of it is allocated. If no large enough block can be found, the runtime attempts to grow the WebAssembly memory and treats that as a new free block.

When a block is freed, we again scan down the free list until the last unallocated block before the block being freed is found (if the block being freed is at the start of memory, this unallocated block will be the free block at the highest address). The block being freed is then inserted into the list and merged with the block on either side of it if those are also free. This helps to reduce fragmentation and ensures that the free list is always kept in memory order.\\


% Idea about using a list for the allocated blocks and then 'copy collecting' to a separate list in mark phase and not even needing a marked bit, just freeing the old list at the end?? Would even work with binning but requires checking sizes
The marking algorithm makes use of the pointer to the next free block in the header, which is still present on allocated blocks (since the size field is needed on allocated blocks) but otherwise unused as the block is no longer on the free list. Starting by marking all references from the shadow stack, it uses these pointers to maintain a stack of unexplored objects and does a depth first search of the reachable objects, setting the marked bit on all of them. \\
The sweep algorithm then walks over all blocks in memory using the size field of each block. Whenever a block is marked it clears the flag, and if a block is allocated but not marked then it is freed.


\subsection{Optimised malloc and free}
After implementing and testing the previously described approach, there were two inefficiencies to address. For both allocating and freeing a block of memory, the runtime first walks down the free list until it finds a block of the right size for allocating or the adjacent free block for freeing. With some additional information attached to each block, both of these traversals can be avoided to get \verb|malloc| and \verb|free| to run in constant time. 

Rather than just a header, an extra 8 bytes is allocated as a trailer for each block and the free list is changed to be a doubly linked list. The trailer contains the size and the pointer to the next block is moved to the trailer, with the header now instead holding a pointer back to the previous trailer. This means that, when the free list is in memory order, the forwards and backwards pointers just skip over regions of allocated memory. The size is also present in the trailers of allocated blocks, although the pointers in allocated blocks are still just used to keep a stack for marking, so the pointer in the trailer is meaningless. As in the original implementation, the last bit of the pointers in the header and trailer indicate if a block is allocated, and the last bit of the header size field is used to indicate a block is marked during garbage collection.

  \begin{tabular}{c|c|c|c|c|c|} \cline{2-6}
Allocated block: &   next header ptr  & size & $\dots$ & size & unused \\ \cline{2-6} \multicolumn{1}{c}{}  \\ \cline{2-6}
Unallocated block: &  previous trailer ptr & size & $\dots$ & size & next header ptr \\ \cline{2-6}
\end{tabular}

% Justifying why all of these new fields/flags are needed
Now, when a block is freed, from just the freed block's address we can determine its size and check the allocated flags of the blocks either side of it, which determines if either block should be merged with the one being freed. Also, since we now have a doubly linked list, each adjacent free block has a pointer to its successor and predecessor block so can be removed from the middle of the free list, still without needing to traverse the whole list. If neither block is merged, we just insert the newly freed block on the top of the free list and update the pointer kept to the free list. Because of this, the free list no longer keeps blocks in memory order.
% TODO: Include diagrams of the various (non-edge) cases


% Give reference for binning? And suggestion of sizes to use?
The expensive part of \verb|malloc| is needing to scan down the free list until a large enough block is found. 
This can be avoided by splitting the free list into several lists, each for a specific range of sizes, and then having an array of pointers to the start of each list. 
% TODO: Be specific about sizes chosen and justification e.g. http://gee.cs.oswego.edu/dl/html/malloc.html
Freed blocks are therefore put into a bin based on their size. The first several bins fit blocks of exact sizes, starting from the minimum possible block size. There are then a few bins which fit ranges that double in size with each bin, then one last bin for any larger sizes. 
As my compiler supports very limited array operations, the size of blocks allocated by a program is primarily determined by the complexity of datatypes defined and the number of free variables in functions. In many cases, these are relatively small so the fixed size bins make up most of the allocations. \\
With these bins, allocation scans from the earliest bin of large enough size until it finds a non-empty bin, and allocates the first large enough block in that list. In cases where the block found is significantly larger than required, an extra check is introduced that the remaining block has not shrunk to a size where it needs to move bin. \\
The only change to the \verb|free| function for this optimisation is the determine which bin a block needs to be placed in based on its size after possibly merging with free blocks adjacent to it.
% TODO: Include the 'list of lists' diagram!












