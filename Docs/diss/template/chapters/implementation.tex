\chapter{Implementation}

\section{Administrative Normal Form IR}
% Reference for ANF?
From OCaml's Typedtree data structure, the first pass of the compiler translates programs to administrative normal form (also called A-normal form or just ANF), which linearises the syntax tree so that the operands to operations are always constants or idnetifiers, with the exception of control flow construsts such as if/while statements. For example:

\begin{verbatim}
f(g(x), h(x+y))
\end{verbatim}
would be translated to
\begin{verbatim}
let v0 = g(x) in
  let t = x + y in 
    let v1 = h(t) in
      f(v0, v1)
\end{verbatim}

Temporary values are now explicit and the linear structure makes optimisations such as common subexpression elimination and dead assignment elimination easier to implement in this form. Code which may execute zero or many times cannot be completely linearised, for example the subexpressions for the body of a while loop must be evaluated within the while loop, since they are never evaluated if the condition is false. \\
Linearising is implemented using three mutually recursive functions which compile immediates (constants/identifiers), compound terms such as \verb|x + y|, and top level terms such as \verb|let x = e in e'| or \verb|e; e'|. The first two of these return a pair of the actual term needed, and a list of any setup operations extracted as part of linearising the expression being compiled. Compiling a top level term then converts this result and list of setup operations into a tree which performs all of the setup then evaluates the result.
% Give actual examples of code translation/algorithm cases?
% OCaml primitives handling?


\section{Pattern Matching}
The linearised intermediate representation also compiles patterns in OCaml's \verb|Typedtree| datatype to the code required to match them and bind variables within patterns. This includes combining the cases of a match expression or function into a single expression, making use of switch statements on integers where possible. 

I chose to implement the backtracking pattern matching compiler described by Fessant and Maranget \cite{ocamlpatternmatch}, which is also used in the OCaml compiler. A backtracking compiler minimises the size of the code generated, whereas a decision tree compiler may produce larger code but ensures that each pattern is examined at most once. The Grain compiler instead implements a decision tree compiler, however both approaches have similar performance once optimised.

% Description of algorithm in paper
The basic algorithm represents the patterns to compile by a vector of values to be matched and a matrix, where each row is a corresponding vector of patterns to match against (where rows are tried in top to bottom order) plus some code, called an action, to execute if the values match that row. The handling of guard statements is not described. Without guards, when the algorithm reaches a $m \times 0$ matrix (so each row has an empty vector of patterns left to match), the pattern has now been matched so the action of the top row is returned. To handle guards, each row of the matrix is augmented to also have an optional compiled guard expression. When an $m \times 0$ matrix is reached, this compiles to a series of if-then-else tests of the guards of each row until a row without a guard is reached. If all rows are guarded, then we eventually reach a $0 \times 0$ matrix which corresponds to pattern matching having failed, so we escape to the enclosing statement testing any other patterns or trap if there is nothing else to test. \\
As guard expressions can include variables bound by the pattern, bindings introduced by pattern matching also need to be kept separate from the action to perform when a match succeeds. This allows the guard expression to be inserted between these bindings and the body of the action, which must only execute if the guard succeeds. \\
For describing examples, $p$ represents a pattern, $a$ is an action, $g$ is an (optional) guard expression, and $b$ are binds introduced by pattern matching. $v$ is the first value being matched against.

Escaping out of failed matches is achieved by the intermediate representation having static exceptions in the form of a \verb|fail i| and \verb|try ... with i ...| expression. Therefore, the algorithm just remembers the closest enclosing handler at any point and escapes to that when pattern matching fails. Although regular exceptions are challenging to implement in WebAssembly, these static exceptions compile to simple blocks with branch instructions to escape out of depending on if the expression succeeds or fails.

% Mention that bindings and the actual action need to be kept separate so that the guard can be checked between them?
Another complication is the additional cases that need to be checked for. The described algorithm considers three types of patterns which are variables $\_$ or $x$ (where the latter generates a binding of the value to $x$ when it is matched), constructors $c(p_1, \dots, p_k)$ and OR patterns $(p_1 | p_2)$. Compilation can then be broken down into a handful of cases based on the patterns in the first column of the matrix. If every pattern is a variable pattern, then the first value and first column of the matrix can be discarded after binding the value to any non-wildcard variables for each row. 

$
\begin{pmatrix}
x & p^1_2 & \dots & p^1_n & \to & a^1 & g^1 & b^1 \\
y & p^2_2 & \dots & p^2_n & \to & a^2 & g^2 & b^2 \\
\_ & p^3_2 & \dots & p^3_n & \to & a^3 & g^3 & b^3
\end{pmatrix}
\to
\begin{pmatrix}
 p^1_2 & \dots & p^1_n  & \to & a^1 & g^1 & (b^1, x=v) \\
 p^2_2 & \dots & p^2_n  & \to & a^2 & g^2 & (b^2, y=v) \\
 p^3_2 & \dots & p^3_n  & \to & a^3 & g^3 & b^3
\end{pmatrix}
$

If instead every pattern is a constructor pattern, then the matrix is compiled into a switch statement on the variant tag of the first value. A case is generated for each constructor present in the matrix, where the body of the switch case is the result of compiling the matrix with all rows with incompatible constructors discarded, and any sub-patterns of the constructor pattern extracted out and appended to the front of each row. 

$
\begin{pmatrix}
c(q_1, \dots, q_k) & p^1_2 & \dots & p^1_n & \to & \dots \\
c'(\dots) & p^2_2 & \dots & p^2_n & \to & \dots \\
\end{pmatrix}
\xrightarrow{\text{specialise } c}
\begin{pmatrix}
 q_1 & \dots & q_k & p^1_2 & \dots & p^1_n  & \to & \dots 
\end{pmatrix}
$

Although this is described as one rule, there are many types of OCaml patterns which are all treated as constructors and need to be handled slightly differently. Tuples are treated as a constructor type with a single variant, so a switch statements is not required, just extracting each of the values contained from the tuple and corresponding patterns. Records are similarly treated as a constructor type with only one variant. As each pattern may match different fields of the record, the fields extracted are the union of the fields mentioned by each row. Where a field is extracted but a row does not match on that field, a wildcard pattern is introduced in the row to effectively ignore that extracted value.  Fields in a record which no pattern mentions are ignored entirely.

$
\begin{pmatrix}
\{f_1=q_1\} & p^1_2 & \dots & p^1_n & \to & \dots \\
\{f_2=q_2\} & p^2_2 & \dots & p^2_n & \to &\dots \\
\{f_1=q_3; f_2=q_4\} & p^3_2 & \dots & p^3_n & \to &\dots
\end{pmatrix}
\to
\begin{pmatrix}
q_1 & \_ & p^1_2 & \dots & p^1_n  & \to & \dots \\
\_ & q_2 & p^2_2 & \dots & p^2_n  & \to & \dots \\
 q_3 & q_4 & p^3_2 & \dots & p^3_n  & \to & \dots
\end{pmatrix}
$

Patterns for arrays in OCaml look like \verb"[|p1, p2, ..., pk|]" and match an array of a specific length. Arrays are tagged with their length so an array of a certain type can be viewed as a constructor with infinitely many variants, each variant having arity equal to the length of array it matches. This therefore becomes a switch statement on the length of the array, and must always introduce a default case since there are infinitely many variants so the match cannot be exhaustive. Integer constants are similarly handled as a constructor with infinitely many variants, except that these variants all have arity 0 as there are no sub-patterns to match. Floating point constants can be pattern matched in OCaml however these must be compiled to nested if-then-else statements, since my IR only allows switching on integers, either an integer constant or the tag of some constructor. This means switch statements can always be compiled as branch tables in WebAssembly later. \\
% Do I want to mention that there can be other rows after the OR pattern
If neither of the previous rules apply then either the matrix  starts with an OR pattern in its first row, which can be expanded into multiple rows, or the matrix is split into an upper or lower part where each can be checked recursively, with a failure in the upper matrix being handled by trying the lower matrix (referred to as the mixture rule).\\
The last feature of the OCaml language that needs supporting is aliases, for example \verb"match v with ((x::xs) as lst) -> ...". These are handled by always preprocessing the first column of the matrix to replace aliases in each row with a binding of the first value to the aliased variable. Preprocessing also simplifies any or patterns in the first column, replacing \verb"(_|p)" with just \verb|_| since the second part of the or pattern would never be considered.


$
\begin{pmatrix}
(p^1_1\ \mathrm{as}\ q) & p^1_2 & \dots & p^1_n & \to & a^1 & g^1 & b^1 \\
(\_ | p^2_1) & p^2_2 & \dots & p^2_n & \to & a^2 & g^2 & b^2 \\
p^3_1 & p^3_2 & \dots & p^3_n & \to & a^3 & g^3 & b^3
\end{pmatrix}
\to
\begin{pmatrix}
 p^1_1 & p^1_2 & \dots & p^1_n  & \to & a^1 & g^1 & (b^1, q=v) \\
\_ & p^2_2 & \dots & p^2_n  & \to & a^2 & g^2 & b^2 \\
p^3_1 & p^3_2 & \dots & p^3_n  & \to & a^3 & g^3 & b^3
\end{pmatrix}
$


\subsection{Optimisations}
This process was then improved by adding several auxiliary data structures, which are described here rather than waiting till the optimisations section.

% MENTION USE OF EXHAUSTIVENESS INFORMATION EARLY ON! EXPLAINS WHY WE DON'T NEED A TEST ON THE LAST ROW!

First, there are cases where it can be useful to swap the order of rows in the pattern matrix. When no rule can be directly applied, we split the matrix into two parts and recursively match each of them, backtracking out of the first case if necessary. A simple program matching two lists can be equivalently written as (the action in each case is unimportant):

\begin{minipage}{0.45\textwidth}
\begin{verbatim}
match l1, l2 with
  | [], _ -> 1
  | _, [] -> 2
  | x::xs, y::ys -> 3
\end{verbatim}
\end{minipage}\qquad
\begin{minipage}{0.45\textwidth}
\begin{verbatim}
match l1, l2 with
  | [], _ -> 1
  | x::xs, y::ys -> 3
  | _, [] -> 2
\end{verbatim}
\end{minipage}

These are equivalent as the last two rows are incompatible. Although patterns must be matched in order, any pattern matching row 3 could never match row 2 so they are safe to swap. The second case is more efficient as we can handle rows 1 and 3 together with the constructor rule, and only need to split the matrix once rather than twice. Of course, the first case could also avoid splitting the matrix twice if rows 2 and 3 were matched on the second pattern before the first, in which case applying the constructor rule would be possible. The problem of selecting the optimal column is thought to be NP-Complete and heuristics for this are more commonly used for decision tree based pattern matching, and the paper does not suggest how row and column based optimisations could be combined. % Cite the end of the OCaml paper for this (e.g. NP-Complete statement)
% Do I actually need to mention this?

% TODO: Not why we use contexts!!!
% Actually quite complex - interaction between jump summaries, reachable handlers, and how extraction from context is used to choose handler on each exit of failed constructor rule (i.e. simpler exhaustiveness because some case can't actually occur)

%Next, in addition to the matrix storing patterns an actions, we maintain and extended to structure called a context, which does not include actions but keeps an additional list of patterns on each row, called the prefix, for the structure already matched by pattern matching. The issue with using just the matrix described previously is that we effectively forget about what has already been seen once part of a match succeeds. Therefore, 

% Very long explanation, would go better in preparation???
If matching some pattern fails and we need to backtrack, we backtrack to the nearest enclosing handler. In some cases, we already know that this handler will fail too due to patterns matched before we had to backtrack. If so, then we can speed up the process by considering all enclosing handlers and jumping to the nearest one that may actually succeed. For this reason each handler has an integer identifier \verb|i|.

As an example, consider the program below (written out of order to show how the improved mixture rule previously described would process it): % Cite this as being the example given in the paper
\begin{verbatim}
type t = Nil | One of int | Cons of int * t
match l1, l2 with
  | Nil, _ -> 1
  | Cons(x, xs), Cons(y, ys) -> 5
  | _, Nil -> 2
  | One x, _ -> 3
  | _, One y -> 4
\end{verbatim} % TODO: Can cut all this down a little?

In pseudocode, this is what it will compile to:  % TODO: Cite that this comes from the paper!
\begin{verbatim}
try
  try
    try
      switch (l1):
        case Nil: 1
        case One: fail 1
        case Cons:
          switch (l2):
            case Cons: 5
            case One|Nil: fail 1
    with (1)
      switch (l2):
        case Nil: 2
        case Cons|One: fail 2
  with (2)
    switch (l1):
      case One: 3
      case Cons|Nil: fail 3
with (3) 4
\end{verbatim}

The OCaml compiler already warns about non-exhaustive patterns, so we can use exhaustiveness information to determine that any tests in the last handler would be redundant, as pattern matching cannot fail from there if the original patterns were exhaustive.

The mixture rule groups the first two rows to be matched by the constructor rule, and divides the remaining rows into separate 1-row matrices tried in turn. The goal is to jump past one handler to an enclosing one, so \verb|try ... with i ...| blocks are formed in bottom-up order so that matching of the first two rows is enclosed by 3 handlers. \\ % TODO: Add a diagram/pseudocode for this?
For a pair of values such as \verb|Cons(a, b), One(c)|, we first attempt to match against just the first two rows. We successfully match the \verb|Cons(x, xs)| constructor but fail on the second row. Naively, we would now try the first handler, which tests the second value against \verb|Nil| again and fails, then goes to the second handler which tests the first value against \verb|One x| and so also fails. Given the tests that had already been performed when we first failed, we knew the value had the form \verb|Cons(_, _), One(_)| and that this is incompatible with the matrices of both of the first two handlers, so we could have immediately jumped to testing the last row.

To track this information, we additionally maintain a list of reachable handlers, mapping their indices \verb|i| to the remaining matrix that handler tries to match. Similar to how rows of matrices are manipulated in the original algorithm, we can specialise the rows of these handler matrices as pattern matching progresses, removing incompatible rows. If one  of the cases when matching a constructor pattern then requires us to backtrack, we can now go to the first handler with a non-empty matrix in the reachable handlers, since all previous handlers will immediately fail. \\

% Include optimised version of code?
% TODO: Contexts and jump summaries

Having done this, it reveals another form of inefficiency. We can only reach a particular handler from exits at specific points in the code and, having successfully matched patterns up to those points, the set of possible values when that handler is reached is restricted. In the example above, we now jump straight to 
the last handler from line 10 when the second list is a \verb|One _| value, splitting the old \verb"case One|Nil" line into two different exits. This means that the first handler is only entered from one of two locations, when \verb|l1 = One v| or when \verb|l1 = Cons (v, vs)| and \verb|l2 = Nil|. In the latter case, we will return 2, but in the first case we exit to the second handler. This handler then checks if \verb|l1 = One v|, but we just established this is the only situation in which the second handler executes. Therefore, the other cases of the switch statement can be discarded and the whole second handler reduces to just the value 3.

To identify these case, we maintain an extended version of a pattern matrix called a context, which does not include actions but keeps an additional list of patterns on each row, called the prefix, for the structure already matched by pattern matching. As with the reachable handler list, the operations of pattern matrices are extended to apply to this, with additional operations to push or pop pattern between the prefix and the fringe of patterns yet to be matched. Now, as well as returning code to match the patterns in a matrix, the procedure recursively computes a jump summary, mapping each handler index to the union of the contexts at all points that exit to that handler. Recursive calls push patterns onto the prefix of the current context, as they match sub-patterns in a more specialised context, and returns pop patterns off the prefixes of context in the jump summary as we return to higher up points in the pattern matching process. 

Now, when we compile a handler, we take its context to be the context mapped to in the jump summary returned from the try block. When we construct a switch statement for the constructor rule, we can exclude any constructor patterns which are incompatible with all rows of the context, indicating that we will never see that constructor for values reaching this point in the pattern matching code. \\

% OR PATTERNS - DIFFERS FROM PAPER!
% Is this actually something I want to draw attention to???
% Should probably change this in my code to do it the other way, as it only every applies to OR patterns doesn't it? Can easily determine which variables need to be passed out.
%My implementation differs from the one described in the paper by not also specifying variables passed through to handlers in addition to a handler index. When an OR pattern \verb"(p1|...|pk) q1 q2 ..." is split into several rows, unless it is the only row of the matrix, we introduce an exit to a handler which matches the remaining patterns \verb|q1 q2 ...|. This is done to avoid duplicating the patterns \verb|q1 q2 ...| in the original matrix, which could result in duplicate code across switch cases. The issue here is that the patterns \verb"p1|...|pk" could each bind some variable \verb|x|, which is in scope when we enter the handler. The paper handles this by listing all such variables at the exit and at the start of the handler block. \\
%This is not necessary as parsing uniquely identifies reuses of the same variable name in different scopes, so a table can track all the variables seen so far without having to deal with renaming issues, at the relatively small overhead of needing to remember more variables simultaneously.
%%% ALSO PREVENTS OPTIMISING, BUT VERY RARE ANYWAY?



In summary, the compilation scheme now has three additional arguments, a boolean indicating if we know the current matrix of patterns to be exhaustive, a context giving additional information on the patterns already matched, and a list of the reachable trap handlers with the matrix each handler matches. It also returns both the code to match a given matrix, and a summary of the set of values which can cause the code to exit to each of the enclosing handlers. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Compiling Identifiers}
After any optimisations are performed, the linearised tree is then compiled to a lower intermediate form. The tree is replaced with a list of instructions, and identifiers are resolved into integer indexes for argument, local, global or closure bound variables. 
% Give more detail about blocks in WebAssembly
Conversion to a list of instructions is straightforward since WebAssembly contains nested blocks. \verb|Block| constructs in WebAssembly contain a list of instructions and introduce a label at the end of the block, which can be jumped to from within the block. \verb|If| blocks are similar but contain two lists of instructions, and \verb|Loop| blocks have the label at the start of the block, allowing code to be repeated. 
Rather than map to these straight away, programs are still described using \verb|if| and \verb|while| constructs, which are easily mapped to WebAssembly at the code generation stage. % Put examples when we get there, not here

This stage also lifts any function declarations to the top level of the program, since closures and environments are now explicit, so the result is a list of functions. The complexity here is in determining the free variables of a function and the number of local stack variables needed by the function body, both of which are done by simple recursive algorithms. The number of locals needed is the maximum needed at any point in the function body. This increases by one after each let binding is evaluated, and for loops also require two to remember the current and end values for the loop (for loops in OCaml have a very constrained \verb"for var = i [up|down]to j do ... done" syntax). 

% Currying removed at this point. Wasn't being used anyway so why do I allow it at all? Does it actually provide any benefit to me?

\section{Code Generation}
WebAssembly generation is then straightforward, as each of the constructs and primitive operators in my lower IR are all simple to implement. The variable bindings in the lower IR were separated into argument, local, global and closure bindings. Argument and global bindings translate directly to Local and Global operations respectively. The locals in a function are laid out as function arguments, then swap space locals used to implement some of the operations in WebAssembly, then locals allocated above for let bindings and loops. Therefore, local bindings are offset by the number of swap spaces and function arguments. The first argument to each function is its closure, so closure bindings first get local 0 then do a load from that address, with an offset selecting the correct variable from the closure. 

I also wrote a runtime in WebAssembly providing functions for memory allocation, polymorphic comparison, boxing of floats in memory, and list append and some of the integer primitives. These are declared as imports to the WebAssembly module and each function index in my IR must be offset by the number of runtime functions imported. Additionally, the memory for the module is also imported from the runtime since it is managed by the allocation function from the runtime imports. Lastly, each of the globals and functions from the module is exported. Every function is exported so that values representing closures can be returned from WebAssembly. As integers are encoded by shifting them left by one, and so that pointers to closures can be used to call functions in WebAssembly from JavaScript, I added a JavaScript wrapper which uses knowledge of how values are represented in memory to be able to pass values back into compiled functions, or return integer values back to a JavaScript caller.

Even before implementing a garbage collector, data still needed to be tagged to identify its type so that OCaml's \verb|compare| function, which is defined on all types, would work correctly. As every operation is on either 32-bit integers or 64-bit floats, pointers are always aligned on 4-byte boundaries so the last 2 bits are not needed. I therefore tag integers as 10 or 00 by shifting them left one, function as 11, and other blocks from tuples or constructors as 01. In the last case, data blocks are represented as a variant tag, their arity, then the elements they contain. As OCaml limits the number of different constructors supported% Reference required
, the variant tag is guaranteed to be a non-negative integer. Therefore, -1 was used to encode floats, which were separately represented as -1 followed by the 64-bit float value. This is necessary since the arguments and return values of WebAssembly functions are strongly typed. Therefore, all functions take a 32-bit integer which is decoded as an integer or pointer to data or a float as required by the body of the function.

\section{Optimisations}
\subsection{Tail calls}
Tail call optimisation is an important optimisation for functional languages, since loops in a pure functional language are written as recursive functions. Therefore, implemented naively a long iteration can easily exhaust the available stack space. This is avoided by wrapping the body of the function in a while loop and replacing the tail call with code that saves the arguments the function would have been called with and updates a variable to say the loop should not exit yet. This is extended to mutually tail recursive functions by additionally storing which function should be called next when a tail call is reached.

% Don't need so much detail?
The optimisation is done in two passes, the first of which just identifies which functions to tail call optimise. As the linearised tree for the program is traversed, we keep track of whether a function call at the current point in the tree would be a tail call (rather than being the body of a let binding, sequence of loop).
This allows us to mark every potential call site optimisation in the program and to construct a call graph consisting of just tail calls between procedures. Tail call optimisations only apply to fully applied curried functions, so we also discard any over or under-applied call sites.

As detailed below, tail call optimising a recursive function increases its size, and the optimisation is more expensive for mutually tail recursive functions as each function must be split in two. Therefore, this call graph is now iteratively pruned to identify functions worth optimising. Functions with no tail calls are removed from the graph, as are functions which only make tail calls to themselves so should be optimised in isolation. Once the graph becomes stable, we have the set of functions to make mutually tail recursive. Tail calls between these functions will then all be rewritten to avoid making additional function calls.

%
%% Body could be a tail call, not that we currently are a tail call
%\begin{itemize} % TODO: Find better way to describe this
%\item The body of a recursive binding to a variable \verb|f| is traversed as being a tail call position for the function \verb|f|.
%\item The body of a non-recursive binding or first part of a sequence is not a tail call position.
%\item The condition and body of a while loop and the body of a for loop or anonymous function are not tail call positions.
%\item If the current position is viewed as a tail call position for some function \verb|f|, then so is the body of each branch of an if statement or switch statement.
%\item If the current position is viewed as a tail call position for some function \verb|f|, then so is the remaining expression after a recursive or non-recursive binding or first part of a sequence.
%\end{itemize}
%These rules allow keeping track of whether an application is in a tail call position or not as the tree is traversed. Each variable declared by a recursive binding to a function is also added to an initially empty table \verb|tbl| and mapped to an empty set of variables and the function's arity. When an application of \verb|g| is found in a tail call position for some function \verb|f|, we check two things. First, \verb|g| must be known at compile time to be one of the recursive functions declared in the program and therefore present in \verb|tbl|. 
%% TODO: Is this obvious
%This is because we can only rewrite tail calls to functions optimised to make tail calls, the reason for which will be apparent when the changes made to functions are described below.
%Second, the number of arguments applied to \verb|g| must match the arity recorded for \verb|g| in the table. This allows curried functions to be tail call optimised. If the function is under-applied, then we do not have all of the arguments ready to evaluate it so it can not be tail called yet. If the function is over-applied, then the result of calling \verb|g| cannot be returned immediately so this is again not a tail call. if both conditions are satisfied, we add \verb|g| to the set of variables \verb|f| is mapped to in \verb|tbl|.
%
%At the end of this process, we have a table which stores the functions that are tail called within each recursive function. From this we determine which functions to tail call optimise on their own, and which to optimise as mutually tail recursive functions (which has a greater impact on code size since the function must be split into two functions). 

\begin{verbatim}
while graph changing:
  for each f in graph:
    if f makes no tail calls:
      remove f and all edges to it
    else if f only tail calls itself:
      remove f and all edges to it
      optimise f on its own
\end{verbatim}
%There is little to gain from rewriting a function which makes no tail calls, so these are not optimised and hence no other function can optimise tail calls to them. Due to the added code bloat of optimising a function for mutually recursive tail calls rather than just tail calls to itself, a function which could only optimise calls to itself is optimised separately and tail calls from other functions to it are also removed. This leaves us with all other functions benefiting from being made mutually tail recursive.\\

In the process of deciding on this method for choosing which functions to tail-call optimise on their own or as mutually recursive functions, I studied how the Grain complier implements this analysis. In doing so, I identified a bug where it would wrongly optimise certain tail calls, and submitted an issue to GitHub detailing this which has since been corrected. % Cite github issue?

% TODO: Better to just give pseudocode and leave out the details?
For a function \verb|f| taking argument \verb|x| which is to be made mutually recursive on its own, we create new variables \verb|next|, \verb|continue| and \verb|x'|. The argument of \verb|f| is changed to be \verb|x'| and the body of \verb|f| is then replaced with an assignment \verb|x = x'| and a while loop on the \verb|continue| variable. This loop sets \verb|continue| to false and \verb|result| to the result of evaluating the original body of \verb|f|. A tail call \verb|f(y)| is replaced with \verb|x = y; continue = true|. This will therefore execute the body of \verb|f| again with the new argument each time a tail call occurs. When the loop eventually exists, \verb|result| is the value the function returns. When \verb|f| is a curried function with multiple arguments, this is handled the same way but with a new variable \verb|x'| created for each argument \verb|x|.
% Mention that linearised tree may need to be rewritten

\begin{minipage}{0.3\textwidth}
\begin{verbatim}
let rec f x = 
  ...
  f(y)
\end{verbatim}
\end{minipage}
\hfill
\begin{minipage}{0.6\textwidth}
\begin{verbatim}
let rec f x' = 
  continue = true; result = 0; x = x';
  while (continue){
    continue = false;
    result = {
      ...
      x = y; continue = true
    }
  }
  result
\end{verbatim}
\end{minipage}

Above we were able to include the body of the function in the new function. For mutually tail recursive functions, the while loop must execute one of a number of mutually recursive functions so this is not possible. Instead, the body of \verb|f| is moved to a new function \verb|_f| taking just a unit argument and the new function \verb|f| just executes a while loop, calling a function selected by a variable \verb|next|.
% Give pseudocode for this too?

% Describe graphs

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Garbage Collection}
One of the last extensions I made to my project was to implement a garbage collector as part of the runtime system. Due to the relative complexity compared to the other runtime functions and going through several iterations of its implementation, this was written in JavaScript and imported into the WebAssembly runtime program however the final version could have been mapped down to WebAssembly too.

% TODO: Talk about Wasm proposal for reference types/garbage collection?

% TODO: Move to preparation chapter!
\subsection{WebAssembly Stack Structure}
Function activation records are part of WebAssembly's implicit stack, which distinguishes stack entries as values, labels of control flow blocks, and activation records.% https://webassembly.github.io/spec/core/exec/runtime.html 
While most operations can only interact with the top of the stack and WebAssembly's type checking prevents a function from being able to access values on the stack below the last activation record or label, function arguments and local variables are accessed with the \verb|LocalGet|, \verb|LocalSet| and \verb|LocalTee| (equivalent to a set followed by a get) instructions. 

I chose to implement a mark and sweep algorithm since reference counting alone is insufficient to collect unused cyclic structures. Although these features are very convenient for producing WebAssembly code, they pose an issue for garbage collection as the garbage collector has no way to directly access the local variables of all function calls currently active, which along with the global variables determine the root set of reachable objects for a mark and seep algorithm. The solution to this is to implement a shadow stack in the linear memory, reserving a region at the start of memory to store copies of stack values as they are updated in functions. Besides a pointer to the top of the shadow stack, no other bookkeeping is needed since, besides the garbage collector, values are only every written there not read. The start function of a program therefore allocates space for the global variables, then each function just increases the stack pointer on entry and decreases it on exit. 

Although only reference values need to be written to the shadow stack, determining this usually requires testing the last bit of a value (since only pointer values have to last bit set) so instead all local variable assignments are written to the stack and this check is left for when garbage collection runs. These memory stores therefore introduces an overhead compared to compiling programs to not use garbage collection. % TODO: Optimise this for obvious cases as peephole optimisation e.g. i32.const n ; local.set i -- clearly don't need to update shadow stack


% TODO: Massively cut down details, don't need such low level information, especially as it is described in K&R
\subsection{Basic Structure}
The implementation can be divided into two parts, the \verb|malloc| and \verb|free| functions used for explicit memory management as in a language such as C, and the mark and sweep algorithm with determines when it is safe to call \verb|free|.

The initial implementation of \verb|malloc| and \verb|free| was taken from K\&R, although inefficiencies in this were resolved later. % https://kremlin.cc/k&r.pdf - The C Programming Language, Kernighan and Ritchie
The runtime maintains a free list where each cell is an unallocated block of memory, with its size and a pointer to the next block in an 8 byte header. All memory allocations are kept aligned to the header size so not all bits of the size and pointer fields are required, leaving space for an allocated and marked flag. The list is circular and initially contains just one large block for the whole of memory. \\
When a block is allocated, the free list is scanned until the first large enough block is found then either that whole block is allocated if it is a perfect fit, else the block is shrunk and just the tail end of it is allocated. If no large enough block can be found, the runtime attempts to grow the WebAssembly memory and treats that as a new free block.

When a block is freed, we again scan down the free list until the last unallocated block before the block being freed is found (if the block being freed is at the start of memory, this unallocated block will be the free block at the highest address). The block being freed is then inserted into the list and merged with the block on either side of it if those are also free. This helps to reduce fragmentation and ensures that the free list is always kept in memory order.\\


% Idea about using a list for the allocated blocks and then 'copy collecting' to a separate list in mark phase and not even needing a marked bit, just freeing the old list at the end?? Would even work with binning but requires checking sizes
The marking algorithm makes use of the pointer to the next free block in the header, which is still present on allocated blocks (since the size field is needed on allocated blocks) but otherwise unused as the block is no longer on the free list. Starting by marking all references from the shadow stack, it uses these pointers to maintain a stack of unexplored objects and does a depth first search of the reachable objects, setting the marked bit on all of them. \\
The sweep algorithm then walks over all blocks in memory using the size field of each block. Whenever a block is marked it clears the flag, and if a block is allocated but not marked then it is freed.


\subsection{Optimised malloc and free}
After implementing and testing the previously described approach, there were two inefficiencies to address. For both allocating and freeing a block of memory, the runtime first walks down the free list until it finds a block of the right size for allocating or the adjacent free block for freeing. With some additional information attached to each block, both of these traversals can be avoided to get \verb|malloc| and \verb|free| to run in constant time. 

Rather than just a header, an extra 8 bytes is allocated as a trailer for each block and the free list is changed to be a doubly linked list. The trailer contains the size and the pointer to the next block is moved to the trailer, with the header now instead holding a pointer back to the previous trailer. This means that, when the free list is in memory order, the forwards and backwards pointers just skip over regions of allocated memory. The size is also present in the trailers of allocated blocks, although the pointers in allocated blocks are still just used to keep a stack for marking, so the pointer in the trailer is meaningless. As in the original implementation, the last bit of the pointers in the header and trailer indicate if a block is allocated, and the last bit of the header size field is used to indicate a block is marked during garbage collection.

  \begin{tabular}{c|c|c|c|c|c|} \cline{2-6}
Allocated block: &   next header ptr  & size & $\dots$ & size & unused \\ \cline{2-6} \multicolumn{1}{c}{}  \\ \cline{2-6}
Unallocated block: &  previous trailer ptr & size & $\dots$ & size & next header ptr \\ \cline{2-6}
\end{tabular}

% Justifying why all of these new fields/flags are needed
Now, when a block is freed, from just the freed block's address we can determine its size and check the allocated flags of the blocks either side of it, which determines if either block should be merged with the one being freed. Also, since we now have a doubly linked list, each adjacent free block has a pointer to its successor and predecessor block so can be removed from the middle of the free list, still without needing to traverse the whole list. If neither block is merged, we just insert the newly freed block on the top of the free list and update the pointer kept to the free list. Because of this, the free list no longer keeps blocks in memory order.
% TODO: Include diagrams of the various (non-edge) cases


% Give reference for binning? And suggestion of sizes to use?
The expensive part of \verb|malloc| is needing to scan down the free list until a large enough block is found. 
This can be avoided by splitting the free list into several lists, each for a specific range of sizes, and then having an array of pointers to the start of each list. 
% TODO: Be specific about sizes chosen and justification e.g. http://gee.cs.oswego.edu/dl/html/malloc.html
Freed blocks are therefore put into a bin based on their size. The first several bins fit blocks of exact sizes, starting from the minimum possible block size. There are then a few bins which fit ranges that double in size with each bin, then one last bin for any larger sizes. 
As my compiler supports very limited array operations, the size of blocks allocated by a program is primarily determined by the complexity of datatypes defined and the number of free variables in functions. In many cases, these are relatively small so the fixed size bins make up most of the allocations. \\
With these bins, allocation scans from the earliest bin of large enough size until it finds a non-empty bin, and allocates the first large enough block in that list. In cases where the block found is significantly larger than required, an extra check is introduced that the remaining block has not shrunk to a size where it needs to move bin. \\
The only change to the \verb|free| function for this optimisation is the determine which bin a block needs to be placed in based on its size after possibly merging with free blocks adjacent to it.
% TODO: Include the 'list of lists' diagram!












