\chapter{Implementation}

In this project, I implement a compiler from OCaml to WebAssembly, primarily supporting the integer operations and all of OCaml's control-flow structures, besides exceptions. The front-end of the OCaml compiler produces a representation called the Typedtree. I translate this first to an intermediate representation I designed, and then to WebAssembly instructions. I also build a runtime system to execute the output code, and extend my project with several optimisations to the compiler and a garbage collector.


\section{Linearised IR}
% Reference for ANF?
From OCaml's Typedtree data structure, the first pass of the compiler translates programs to be closer to administrative normal form (ANF), which linearises the syntax tree so that operands are always constants or identifiers. Expressions are broken down into immediate, compound and linast (linearised AST) terms to achieve this. The general structure is given below, with some cases omitted for brevity.

\begin{figure}[H]
\begin{verbatim}
linast = compound 
  | Sequence (compound, linast)
  | Let(Identifier, compound, linast) 
  | LetRec((Identifier * compound) list, linast)

compound = imm | unary_op imm | binary_op (imm, imm) 
  | Block (tag, imm list)
  | Field (imm, index) | SetField (imm, index, imm)
  | If (imm, linast, linast) | Switch (imm, (tag * linast) list)
  | Try (index, linast, linast) | Fail index
  | Function (Identifier list, linast) | App (imm, imm list)

imm = Constant | Identifier
tag = index = int
\end{verbatim}
\caption{Summary of IR structure}
\end{figure}

%For example:
%
%\begin{verbatim}
%f(g(x), h(x+y))
%\end{verbatim}
%would be translated to
%\begin{verbatim}
%let v0 = g(x) in
%  let t = x + y in 
%    let v1 = h(t) in
%      f(v0, v1)
%\end{verbatim}

Temporary values are now explicit and the linear structure makes optimisations easier to implement. Code that may execute zero or many times cannot be completely linearised, for example, the branches of an if statement, as only one branch will actually be evaluated. 

Given an expression in the Typedtree to translate to an immediate or compound term, the expression is recursively decomposed, returning a list of let bindings for any temporary values required in its computation and an immediate or compound term for its final value. In this way, an initial call to translate an expression to a linast term builds up a list of setup operations and a final compound term, then combines these into a tree by chaining the setup operations together, terminated by the final compound term.


%Linearising is implemented using three mutually recursive functions which compile immediates (constants/identifiers), compound terms such as \verb|x + y|, and top level terms such as \verb|let x = e in e'| or \verb|e; e'|. The first two of these return a pair of the actual term needed, and a list of any setup operations extracted as part of linearising the expression being compiled. Compiling a top level term then converts this result and list of setup operations into a tree which performs all of the setup then evaluates the result.
% Give actual examples of code translation/algorithm cases?
% OCaml primitives handling?

% TODO: Describe 3-level structure of immediate, compounds and linast terms

\section{Pattern matching}
Translation to the linearised IR also compiles patterns in the Typedtree to code that matches them. This includes combining the cases of a match expression or function into a single expression, making use of switch statements on integers where possible. 

I chose to implement the backtracking pattern-matching compiler described by Le Fessant and Maranget \cite{ocamlpatternmatch}, which is also implemented in the OCaml compiler for its intermediate representation. 
The idea is to have a vector of the values being matched, and a matrix representing the different cases and the action when each matches. An example of this is:

\begin{figure}[H]
\begin{minipage}{0.45\textwidth}
\begin{verbatim}
let f l1 l2 = match l1, l2 with
  | [], _ -> e1
  | _, [] -> e2
  | x::xs, y::ys -> e3
\end{verbatim}
\end{minipage}
\begin{minipage}{0.55\textwidth}
 values $\begin{pmatrix}
\verb|l1| \\
\verb|l2|
\end{pmatrix}$, cases
$\begin{pmatrix}
\verb|[]| & \_ & \to & \verb|e1| \\
\_ & \verb|[]| & \to & \verb|e2| \\
\verb|x::xs| & \verb|y::ys| & \to & \verb|e3| \\
\end{pmatrix}$
\end{minipage}
\caption{Representing a program as a pattern matrix and value vector}
\end{figure}

The algorithm given in the paper is made more complex by the range of syntax OCaml supports. %, and to a lesser extent by the fact the IR is linearised. 
First, OCaml patterns can have guard statements, which must be tested to see whether a pattern matches. Some of the compilation steps prefix additional variable bindings onto the actions, and the guard statement needs to be evaluated after these, but before any of the original action is evaluated. Therefore, each action $\to a$ is replaced with $\to a \ g \ b$ where $g$ is the optional guard expression and $b$ is the other bindings introduced.


The base case of this algorithm occurs when we reach a matrix with no rows. This indicates that we have run out of possibilities, so pattern matching fails and backtracks. If instead there are no columns, we have successfully matched the whole pattern. If the first row has no guard, we output the action of that row, otherwise the guard is tested and the action forms the \verb|then| case of an if statement, with the remaining rows of the matrix considered in the \verb|else| case.

%The base cases of this algorithm occur when we reach a matrix with either no rows or no columns of patterns. If there are no rows, we have run out of possibilities so pattern matching fails and backtracks. If there are no columns, we have successfully matched the whole pattern. We then output code testing the guard for each action from top to bottom of the matrix, executing the corresponding action if the guard succeeds. Either some row is not guarded, in which case it runs unconditionally if that is reached, or every row is guarded, in which case we similarly backtrack if all guards are false.

There are four broad cases for when the matrix is non-empty:
\begin{itemize}
\item \textbf{Variable rule}: If the first column has just variable patterns, a corresponding binding of the first value in the values vector is added to each row's bindings, then the first column of the matrix is discarded, as is the now-matched value of the value vector.
% INCLUDE A DIAGRAM??
\item \textbf{Constructor rule}: If the first column has just constructor patterns, several specialised matrices are produced, each removing incompatible rows and deconstructing the fields of the chosen constructor. The result of compiling each of these then forms the branches of a switch statement on the constructor tag. A default case captures any constructor with no corresponding rows, and fails so that matching backtracks. 
\begin{figure}[H]
\hfill
$
\begin{pmatrix}
c(q_1, \dots, q_k) & p^1_2 & \dots & p^1_n & \to & \dots \\
c'(\dots) & p^2_2 & \dots & p^2_n & \to & \dots \\
\end{pmatrix}
\xrightarrow{\text{specialise } c}
\begin{pmatrix}
 q_1 & \dots & q_k & p^1_2 & \dots & p^1_n  & \to & \dots 
\end{pmatrix}
$ \hfill
\caption{Constructor specialisation on a pattern matrix}
\end{figure}
% OR variables are an issue even at this stage, mention difference to paper's approach here about extended scope
\item \textbf{OR rule}: If there is just one row and it starts with an OR pattern \verb"(p1|...|pk)", a $k \times 1$ matrix is produced expanding out the OR pattern. %, with each row having an empty action. 
The output is the result of compiling this matrix, sequenced with the result of compiling the rest of the original matrix. Simply duplicating the original row once for each case of the OR pattern would duplicate all patterns in later columns. That could lead to exponential growth in code size, which backtracking compilation aims to avoid.
\item \textbf{Mixture rule}: If no other case is applicable, the matrix is split into an upper and lower matrix, where one of the other rules can be applied to the upper matrix. The matrices are compiled separately, with failure in the upper matrix resulting in a \verb|Fail i| instruction. This is handled by the \verb|try ... with (i) ...| construct, which backtracks to try the lower matrix instead.

\end{itemize}

The explanation above is how the paper describes the algorithm. In practice, OCaml has several different types that all fall under the ``constructor" rule and must be handled separately:
\begin{itemize}
\item \textbf{Constants}: Constants can be viewed as a datatype with infinitely many 0-arity constructors, so compile to a switch statement on the constant's value. A default case that backtracks is always included, for all of the constant values not handled.

% This always includes a default case that backtracks, as every constant value cannot be listed exhaustively. 
% Integers compile to a switch statement on the integer value, always including a default case that backtracks. 
%For floating-point constants, a switch statement cannot be used, so these instead compile to a sequence of if-then-else tests.
% and this switch statement will always have a default case that backtracks, since it can never be exhaustive. The decision to implement this switch statement as a branch table or nested if-then-else statements is made at the WebAssembly generation stage based on how sparse the set of values is. \\
%For floating point values, we must always use a chain of if-then-else tests so these are compiled as such at the IR level.
% TODO: Talk about branch tables or not?

\item \textbf{Arrays}: In OCaml, array patterns look like \verb"[|p1, p2, ..., pk|]", matching a specific length. 
The length is the tag of the object, so a switch statement on the tag is generated, extracting the corresponding number of fields in each case. 
A default case is always included for the array lengths not handled.

%As all possible lengths cannot be specified exhaustively, a default case is always included.

\item \textbf{Records}: A record type is just a constructor with a single variant, and each pattern specifies a subset of the fields to match.  To allow all rows of the matrix to be processed,  the union of the fields mentioned in each row is extracted from the record, inserting the wildcard pattern \verb|_| where a row does not match a field, so that the extracted value is ignored. In the example below, any unmentioned fields are ignored entirely.
\begin{figure}[H]
\hfill
$
\begin{pmatrix}
\{x=q_1\} & p^1_2 & \dots & p^1_n & \to & \dots \\
\{y=q_2\} & p^2_2 & \dots & p^2_n & \to &\dots \\
\{x=q_3; y=q_4\} & p^3_2 & \dots & p^3_n & \to &\dots
\end{pmatrix}
\to
\begin{pmatrix}
q_1 & \_ & p^1_2 & \dots & p^1_n  & \to & \dots \\
\_ & q_2 & p^2_2 & \dots & p^2_n  & \to & \dots \\
 q_3 & q_4 & p^3_2 & \dots & p^3_n  & \to & \dots
\end{pmatrix}
$
\hfill
\caption{Handling of record fields in pattern matrices}
\end{figure}

\item \textbf{Tuples}: Tuples are constructors with a single variant,  just like records, except that a tuple pattern always specifies patterns to match against each element of the tuple.
% Tuples are just constructors with a single variant of a particular size. Therefore, the matrix is specialised by extracting the nested patterns and no rows need to be discarded. There is also no need for a switch statement, since there is only one case.
\end{itemize}

%% ALIASES AREN'T THAT INTERESTING

%Finally, OCaml has alias patterns such as \verb|((x::xs) as lst)| which must be supported. These are handled by preprocessing the first column of the matrix at each step, replacing aliases in each row with a binding of the first value to the corresponding aliased variable. %Redundant OR patterns are also removed, replacing \verb"(_|p)" with just \verb|_|.
%The last feature of the OCaml language that needs supporting is aliases, for example \verb"match v with ((x::xs) as lst) -> ...". These are handled by always preprocessing the first column of the matrix to replace aliases in each row with a binding of the first value to the aliased variable. Preprocessing also simplifies any or patterns in the first column, replacing \verb"(_|p)" with just \verb|_| since the second part of the or pattern would never be considered.

%\begin{figure}[H]
%$
%\begin{pmatrix}
%(p^1_1\ \mathrm{as}\ q) & p^1_2 & \dots & p^1_n & \to & a^1 & g^1 & b^1 \\
%%(\_ | p^2_1) & p^2_2 & \dots & p^2_n & \to & a^2 & g^2 & b^2 \\
%p^2_1 & p^2_2 & \dots & p^2_n & \to & a^2 & g^2 & b^2
%\end{pmatrix}
%\to
%\begin{pmatrix}
% p^1_1 & p^1_2 & \dots & p^1_n  & \to & a^1 & g^1 & (b^1, q=v) \\
%%\_ & p^2_2 & \dots & p^2_n  & \to & a^2 & g^2 & b^2 \\
%p^2_1 & p^2_2 & \dots & p^2_n  & \to & a^2 & g^2 & b^2
%\end{pmatrix}
%$
%\caption{Handling of aliases in pattern matrices}
%\end{figure}

% MENTION ISSUE WITH OR PATTERNS?
%There is a complication with OR patterns which I handle differently to the paper. 


\subsection{Optimisations to pattern matching}
The above process is made more efficient by including auxiliary information. %, which is described here rather than waiting till the optimisations section.
% Mention OR pattern change or not?


First, the mixture rule can reduce the number of splits needed by maximising the size of the first matrix, which is then handled by a single use of another rule. This is achieved by swapping incompatible rows. Patterns must be matched in top-to-bottom order, but this does not matter for two rows that match disjoint sets of value, allowing them to be swapped. For example, the two programs in figure \ref{fig:rowswap} are equivalent, however the constructor rule can only be applied to the first row on the left, but to the first two rows on the right:

\begin{figure}[H]
\hfill
\begin{minipage}{0.45\textwidth}
\begin{verbatim}
match l1, l2 with
  | [], _ -> 1
  | _, [] -> 2
  | x::xs, y::ys -> 3
\end{verbatim}
\end{minipage}\qquad
\begin{minipage}{0.45\textwidth}
\begin{verbatim}
match l1, l2 with
  | [], _ -> 1
  | x::xs, y::ys -> 3
  | _, [] -> 2
\end{verbatim}
\end{minipage}\hfill
\caption{Example of reordering patterns not changing a program's semantics}
\label{fig:rowswap}
\end{figure}

%%% TO INCLUDE OR NOT? NOT THAT INTERESTING, NOR IS IT REALLY SOMETHING DONE BY ME, ANYTHING BETTER TO REMOVE?
%Another improvement is made by passing a boolean for exhaustiveness to the procedure, which is computed using existing utility functions in the OCaml front-end. %The OCaml front-end already has utility functions for estimating both when patterns are incompatible and when a set of patterns is exhaustive. % Exhaustiveness is unchanged by recursive calls, except for the mixture rule, where the top matrix is never considered exhaustive. 
%When a matrix is known to be exhaustive, switch statements can leave out default cases for missing constructors, as they must have been matched by an earlier stage of pattern matching. 



Next, several data structures are added to the procedure. They are all related so I describe each of them before explaining their benefit. 

The procedure is extended with a context, a pair of matrices of just patterns with no actions or guards. The first is called the prefix, remembering already matched structure, and the second is the fringe, describing the known structure of the values left to be matched. Together, these describe the possible values that may reach the point in the pattern-matching code currently being produced. Initially there is just one row, where the prefix is empty and the fringe is \verb|_|, as nothing is known about the value being matched. The specialisation operations on matrices extend to contexts in the obvious way, for example with constructors ($\bullet$ separates the prefix and fringe):

\begin{figure}[H]
$
\begin{pmatrix}
 p^1_1 \dots  p^1_k & \bullet & c(q_1, \dots, q_k) & \dots & q^1_n \\
 p^2_1 \dots  p^2_k & \bullet & \_ & \dots & q^2_n \\
 p^3_1 \dots  p^3_k &\bullet & c'(\dots)  & \dots & q^3_n  \\
\end{pmatrix}
\xrightarrow{\text{specialise } c}
\begin{pmatrix}
p^1_1 \dots  p^1_k  & c(\_, \dots, \_) & \bullet &  q_1 & \dots & q_k & \dots &  q^1_n \\
 p^2_1 \dots  p^2_k & c(\_, \dots, \_) & \bullet & \_ & \dots & \_ & \dots & q^2_n
\end{pmatrix}
$
\caption{Constructor specialisation on a context}
\end{figure}

The procedure is also passed a list of reachable handlers. When the mixture rule splits a matrix into upper and lower parts, $P$ and $Q$, and introduces handler $i$, then $P$ is complied with $(i, Q)$ added to its handler list, although only the patterns of $Q$ are stored in the handler list, not the actions or guards. Again, operations on matrices are naturally extended to each matrix in the list. 

Correspondingly, in addition to the code to match a matrix, the procedure returns a jump summary, a mapping from indices of enclosing handlers to the contexts representing sets of values that backtrack to those handlers.
 When the algorithm produces \verb|fail i| to backtrack, it returns a jump summary mapping \verb|i| to the current context, indicating that such values trigger backtracking to handler \verb|i|. At branches, the jump summaries of each branch are unioned by concatenating the rows of contexts for the same handler. %Patterns also move from the prefix of each context back to the fringe as nested calls return, 
%
% To compute this, where the algorithm inserts \verb|fail i| to backtrack, it also adds the current context to the jump summary for index \verb|i|, indicating that values of the form described by that context will trigger backtracking. For each index, the contexts of each branch are unioned at switch statements, amounting to concatenating the rows from each context. As jump summaries are returned out of nested calls, patterns are popped off the prefix of each context back to the fringe, to be consistent with the values matched up to that point in the process, but now making the fringe more precise. 
If compiling the upper matrix in the mixture rule returns jump summary $\rho$, then when compiling the lower matrix $Q$ for handler $i$, the context $\rho(i)$ is used.% from the jump summary.


The reachable handler list remembers which patterns each handler will try to match if exited to, and the contexts obtained from the jump summary provide some information on the structure of values that will trigger that handler. 

The first improvement is that, rather than always backtracking to the nearest handler, handlers guaranteed to fail can be skipped over. Since the matrices matched by reachable handlers are specialised as more specific patterns are matched recursively, at a point where backtracking occurs, some of those matrices may be empty, indicating that no values that reach this point in the code would be matched by those handers either.
% indicating that backtracking to that handler will just cause further backtracking, due to the known structure of values at this point. 
The procedure instead inserts a \verb|fail i| to the first handler \verb|i| with a non-empty matrix. %Note that, where the mixture rule is needed multiple times to split up a matrix, it now makes sense to nest the handlers so that they are all available to jump to, maximising the opportunities to skip handlers.  

Second, the lower matrix for a handler may not cover all constructor cases if some are covered by the upper matrix matched before backtracking. Naively, this would result in additional cases for the missing constructors, which we know will never occur. 
By taking the context from the jump summary, we know the set of values that can actually reach the handler. If some constructor is incompatible with every pattern in the first column of the fringe of the context, the value being matched in the handler will never be that constructor, hence such cases can be discarded to reduce code size. In instances where this reduces the switch statement to a single case, the test can be removed entirely.

%This is the purpose of taking the context from the jump summary, representing the set of values that can actually reach the handler. If some constructor is incompatible with every pattern in the first column of the fringe of the context, the value being matched will never be that constructor when the handler executes, so cases can be safely discarded to reduce code size. In some cases, switch statements may be reduced to a single case, then the test can be removed entirely.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Compiling identifiers}
After any optimisations on the IR, the linearised tree is lowered to WebAssembly in two stages. The first of these pulls function definitions out to the top level of the program, making closures and free variables explicit. It also converts the IR tree to lists of instructions (with nesting still present for \verb|if|, \verb|for| and \verb|while| statements as WebAssembly supports nested blocks) and replaces string identifiers.


Identifiers are replaced with a datatype indicating the kind of variable being accessed, with free variables being stored in the closure of a function:

\verb"binding = Global of int | Arg of int | Local of int | Closure of int"

%whether that variable is accessed as a global variable, a function argument, a local variable of the function, or a free variable in the function's closure, with an integer index into the corresponding set of variables. \\
Each let binding in a function's body generates a new local variable for that identifier. This results in a large number of local variables that is reduced after peephole optimisations on the generated WebAssembly are performed. 
The WebAssembly representation is stored with a flowgraph between instructions, a graph indicating which instructions may execute immediately before or after each instruction in a function. 
Live-variable analysis is performed on this, identifying the points in a function where each local variable's current value could be used later. These points indicate regions where a variable cannot be overwritten. From this, a clash graph is constructed with edges between every pair of variables that are ever simultaneously live, so must be stored separately. A graph-colouring heuristic then maps these local variables to a hopefully much smaller set of local variables, and unused local variables are removed from the function.

Using fewer local variables reduces code size, as a WebAssembly function must declare the type of each local variable.
%This register-allocation process is necessary for lower-level target languages where there is only a finite set of registers available. Although a WebAssembly function can be defined with arbitrarily many local variables, it is likely that a platform's WebAssembly implementation will use less memory by using fewer locals \cite{v8, searchfox}. 
Also, the number of local variables used by a function directly affects the space it occupies on the shadow stack, needed for garbage collection. Therefore, this is a useful optimisation over allocating local variables according to scoping, which will generally be much less efficient.


\section{Code generation}

Finally, the program is translated to WebAssembly code, with most operations mapping directly to fixed blocks of code that are concatenated together. 
%Variable accesses need to be mapped to \verb|LocalGet| and \verb|GlobalGet| operations, and equivalent \verb|Set| instructions.
%
% \verb|Global i| maps directly to \verb|GlobalGet i|, and \verb|Arg i| maps to \verb|LocalGet i|, as function arguments are the first local variables in WebAssembly. 
Two additional ``swap" local variables are introduced in each function, needed to compile some operations to WebAssembly. 
For example, if \verb|ar| evaluates to an array pointer and \verb|index| evaluates to the index being accessed, checking the array bounds and performing the load emits the code below. Some steps have been omitted for brevity, such as removing tags from values, or shifting the index to be a byte offset rather than a word offset.

% TODO: Check this is aligned nicely with pages
\begin{figure}[H]
\begin{verbatim}
ar; 
LocalTee 0;      // swap variable 0
Load;            // the size of the array is stored at its base address
index; 
LocalTee 1;      // swap variable 1
Compare GtU;     // size > index, unsigned comparison catches index < 0
if
  LocalGet 0;
  LocalGet 1;
  Add;
  Load offset=8; // adjust for the start of the array storing its size
else
  trap 
end
\end{verbatim}
\caption{Code generated for accessing an array element}
\end{figure}

The expressions \verb|ar| and \verb|index| could be free variables being loaded from a closure, in which case storing their values in local variables avoids repeated loads from memory. 
The first local variables of a WebAssembly function are its arguments, so a free variable binding \verb|Closure i| compiles to a  \verb|LocalGet 0| to get the closure pointer, then a \verb|Load| with an offset to that variable.
% For a function with \verb|n| arguments, we therefore compile \verb|Swap i| as \verb|LocalGet (n + i)| and \verb|Local i| as \verb|LocalGet (n + 2 + i)|. Lastly,  a free variable \verb|Closure i| compiles to a \verb|LocalGet 0| to get the closure pointer, then a \verb|Load| with an offset to that variable.



I wrote a runtime in WebAssembly providing functions for memory allocation, polymorphic comparison, boxing of floats in memory, and list append, which are declared as imports into compiled WebAssembly modules. %The module then exports each top-level global variable as its name from the OCaml program, and each of the user defined functions. 
I also wrote a short JavaScript wrapper to interpret integers and pointers to data or closures. This means that JavaScript code can interact with the WebAssembly values without needing to understand the runtime representation, and can call functions using returned closure pointers.
%As explained below, integers and pointers are tagged to distinguish them, so I wrote a short JavaScript wrapper that interprets values accordingly. This means that JavaScript code can interact with the WebAssembly values without needing to understand the runtime representation, and can call functions using returned closure pointers.

Data must be tagged to identify its type, so that OCaml's \verb|compare| function can be implemented correctly, which is defined on all types. As every operation is on either 32-bit integers or 64-bit floats, pointers are always aligned on 4-byte boundaries, so the last 2 bits are unused. I therefore distinguish the types of values by these last two bits. Closure pointers are tagged with 11, pointers to other objects in memory, such as constructors, are tagged with 01, and integers are tagged with 10 or 00 by shifting them left one, so odd values now end in 10 and even values end in 00. 

The layout of objects stored in memory is given by figure \ref{fig:memlayout}, where the ``arity" field specifies the number of 32-bit values in ``elements"/``free variables". Since arguments and return values in WebAssembly are strongly typed, and polymorphic OCaml functions need supporting, all values are 32-bit integers that are decoded as pointers to data or functions, as just described. In particular, floating-point values are ``boxed'' in memory and represented as pointers to data blocks storing their value. The variant tag on data blocks is non-negative, so -1 is used as a special value to distinguish blocks containing a float value.\\

%  \begin{tabular}{c|c|c|c|c|c|m{2cm}|} \cline{2-7}
%Constructor block &   variant tag  & arity &   \multicolumn{3}{c}{\hspace{2cm}{elements} } &  \\ \cline{2-7} \multicolumn{1}{c}{}  \\ \cline{2-7}
%Tuple/Record &   0  & arity &   \multicolumn{3}{c}{\hspace{2cm}{elements}} &  \\ \cline{2-7} \multicolumn{1}{c}{}  \\ \cline{2-4}
%Boxed float &  -1 & 0 & 64-bit float value      \\ \cline{2-4} \multicolumn{1}{c}{}  \\ \cline{2-7}
%Closure & function index  & arity &   \multicolumn{3}{c}{\hspace{2cm}{free variables}} &  \\ \cline{2-7} 
%\end{tabular}

\begin{figure}[H]
\hspace{-1cm}
\begin{bytefield}[bitformatting={\small\bfseries},bitwidth=1.6em, leftcurly=.]{22}
\bitheader{0,4,8,16} \\

\begin{leftwordgroup}{float}
\bitbox{4}{-1} & \bitbox{4}{0} & \bitbox{8}{64-bit float value} &	\bitbox[]{6}{} \end{leftwordgroup} \\
\text{}\\
\begin{leftwordgroup}{Constructor}
 \bitbox{4}{variant tag} & \bitbox{4}{arity} & \bitbox{14}{elements}  \end{leftwordgroup}  \\
\text{} \\
\begin{leftwordgroup}{Tuple/Record}
\bitbox{4}{0} & \bitbox{4}{arity} & \bitbox{14}{elements}  \end{leftwordgroup}  \\
\text{}\\
\begin{leftwordgroup}{Closure}
\bitbox{4}{function index} & \bitbox{4}{arity} & \bitbox{14}{free variables}  \end{leftwordgroup}  \\
    \end{bytefield}
\vspace{-1cm}
\caption{Memory layout of objects, column headers are byte offsets}
\label{fig:memlayout}
\end{figure}



The arity field in closures indicates the number of free variables. \verb|compare| is undefined on function values, but the arity is still needed for garbage collection so that pointers stored as free variables in closures can be searched. The field is unnecessary for floating-point values, but is included and set to 0 so that garbage collection can search each of these blocks for pointers in the same way, by first looking at the arity field then checking that many subsequent fields. %A float contains no pointers, hence the field is set to 0 so that the following 64 bits are not misinterpreted.

%As OCaml limits the number of different constructors supported% Reference required
%, the variant tag is guaranteed to be a non-negative integer. Therefore, as floats need to be``boxed'' and stored in memory as a data block, -1 was used to identify floats, which are stored similarly to . This is necessary since the arguments and return values of WebAssembly functions are strongly typed. Therefore, all functions take a 32-bit integer which is decoded as an integer or pointer to data or a float as required by the body of the function.

\section{Optimisations} \label{opts}

One of the extensions for my project was to implement optimisations once I had a working compiler. Three main optimisation passes were added: function inlining, uncurrying and tail-call optimisation. Several cleanup passes were also implemented, propagating values to reveal further optimisations, and removing inefficiencies at both the IR and WebAssembly level by detecting dead code.

\subsection{Tail calls}
Tail-call optimisation is important in functional languages, since loops in a functional language are written as recursive functions, so a naive implementation for a large number of iterations can easily exhaust the available stack space. This is avoided by wrapping the body of the function in an imperative while loop and replacing the tail call with code that saves the arguments the function would have been called with and updates a variable to say the loop should not exit yet. This is extended to mutually tail-recursive functions by additionally storing which function should be called next when a tail call is reached.

% Don't need so much detail?
There is first an analysis pass to construct a call graph of just the tail calls in recursive functions. Nodes in the graph correspond to recursive functions, with each node having edges to the functions it tail calls. This is performed conservatively, so if a function makes a tail call with a function variable that cannot be identified as a particular function, no edge is added.  Tail-call optimisations only apply to fully applied curried functions, so we also discard any over or under-applied call sites.


%The optimisation is done in two passes, the first of which just identifies which functions to tail call optimise. As the linearised tree for the program is traversed, we keep track of whether a function call at the current point in the tree would be a tail call (rather than being the body of a let binding, sequence of loop).
%This allows us to mark every potential call site optimisation in the program and to construct a call graph consisting of just tail calls between procedures.

As detailed below, tail-call optimising a recursive function increases its size, and the optimisation is more expensive for mutually tail-recursive functions, as each function must be split in two. Therefore, this call graph is iteratively pruned to identify functions worth optimising. Functions with no tail calls are removed from the graph, as are functions that only make tail calls to themselves, hence should be optimised in isolation. Once this process converges, we have the set of functions to make mutually tail recursive. Tail calls between these functions will then be rewritten to avoid making additional function calls.



\begin{figure}[H]
\centering
\begin{BVerbatim}
while graph changing:
    for each f in graph:
        if f makes no tail calls:
            remove f and all edges to it
        else if f only tail calls itself:
            remove f and all edges to it
            optimise f on its own
\end{BVerbatim}
\caption{Iterative algorithm to determine mutually tail-recursive functions}
\end{figure}


When deciding on this method for choosing which functions to tail-call optimise on their own or as mutually recursive functions, I studied how the Grain complier implements this analysis. In doing so, I identified a bug where it would wrongly optimise certain tail calls, and submitted an issue on GitHub\footnote{https://github.com/grain-lang/grain/issues/506} detailing this, which has since been corrected. % Cite github issue?

% TODO: Better to just give pseudocode and leave out the details?
For a function \verb|f| taking argument \verb|x| that is to be made mutually recursive on its own, we create new variables \verb|result|, \verb|continue| and \verb|x'|. The argument of \verb|f| is changed to be \verb|x'| and the body of \verb|f| is replaced with an assignment \verb|x = x'| and a while loop on the \verb|continue| variable. This loop sets \verb|continue| to false and \verb|result| to the result of evaluating the original body of \verb|f|. A tail call \verb|f(y)| is replaced with \verb|x = y; continue = true|. This will therefore execute the body of \verb|f| again with the new argument each time a tail call occurs. When the loop eventually exits, \verb|result| is the value the function returns. When \verb|f| is a curried function with multiple arguments, a new variable is created for each argument.
% Mention that linearised tree may need to be rewritten

\begin{figure}[H]
\begin{minipage}{0.24\textwidth}
\begin{verbatim}
let rec f x = 
  ...
  f(y)
\end{verbatim}
\end{minipage}
 $\Longrightarrow$ \hfill
\begin{minipage}{0.6\textwidth}  % SLIGHTLY INACCURATE, REALLY AN EXTRA SET OF MUTABLE VARIABLES TO KEEP MUTABLE VALUES DISTINCT (but this doesn't make sense if the function only has 1 argument, as in this example)
\begin{verbatim}
let rec f x' = 
  continue = true; result = 0; x = x';
  while (continue){
    continue = false;
    result = {
      ...
      x = y; continue = true
    }
  }
  result
\end{verbatim}
\end{minipage}
\caption{Example of how a single function is tail-call optimised}
\end{figure}


Above we were able to include the body of the function in the new function. For mutually tail-recursive functions, the while loop must execute one of a number of mutually recursive functions so this is not possible. Instead, the body of \verb|f| is moved to a new function \verb|_f| taking a unit argument and the new function \verb|f| just executes a while loop, calling a function selected by a variable \verb|next|, set at each tail call. \verb|next|, \verb|continue| and \verb|result| are all shared global variables in this case, so they can be set by different functions. A shared set of global variables is also used to pass arguments at tail calls, and their values are bound to the original function variables when \verb|_f| executes.
% Give pseudocode for this too?

\begin{figure}[H]
\begin{minipage}{0.24\textwidth}
\begin{verbatim}
let rec f x = 
  ...
  g(y)

and g x = 
  ...
  f(z)
\end{verbatim}
\end{minipage}
 $\Longrightarrow$ \hfill
\begin{minipage}{0.65\textwidth}
\begin{verbatim}
let arg = 0; continue = false; result = 0; next = 0;

let rec f x' = 
  continue = true; next = _f; arg = x';
  while (continue){
    continue = false;
    result = next();
  }
  result

and _f () = 
  let x = arg;
  ...
  arg = y; continue = true; next = _g;

... similarly for g and _g ...
\end{verbatim}
\end{minipage}
\caption{Example of how mutually recursive functions are tail-call optimised}
\end{figure}




\subsection{Uncurrying}
% IS THIS WORTH MENTIONING? ALREADY HAVE ENOUGH DIFFERENT THINGS TO TALK ABOUT WITHOUT DISCUSSING FAILURES

%Functions which take many curried arguments are implemented as several functions, where each just constructs the closure to the next until the last evaluates the function. I looked at implementing this a different way. All but the last function just copy the closure then additionally write the argument of that call, one of the curried arguments, into the closure. Therefore, I included an integer of the number of arguments remaining, and made the original closure allocation large enough to store space for them. Then, each application just stores the argument in the closure and decrements the number of arguments remaining. When this value reaches 0, the last application is done as an actual function call. \\
%The hope was that, by no longer producing a function for each argument, the output file size would be reduced. Also, a closure may be reused in several places so often needs copying anyway, but not when we have several applications to a curried function, in which case we know the intermediate partially applied functions are not being stored anywhere. Therefore, we avoid function applications which effectively just copy the closure every time so may also save on execution time and memory usage. \\
%Unfortunately, the overhead of checking and modifying the number of arguments remaining on each function application, and the fact we still need to copy closures often enough, mean that this change did not improve any of the metrics and in some cases could increase both the filesize and memory usage by about 20\%, despite now having fewer functions defined.
% GO FIND THIS DATA!

A curried function of many arguments is implemented in WebAssembly as several functions, where all but the last function just constructs the closure to the next function, copying across free variables in the closure and the curried argument. 

This is made more efficient for functions that are not exported and are always called with all of their curried arguments.  In this case, we can replace the set of curried functions with one function that takes all of its arguments as a tuple, and update the calling sites accordingly. Rather than actually creating a tuple in memory and passing that to the function (as in a function like \verb|f (x, y) = x + y|), the WebAssembly function takes multiple arguments directly, saving on memory usage. Functions and applications that are optimised in this way are annotated in the IR tree, so that translation to WebAssembly distinguishes them from functions called using curried arguments.

%Such functions and calls to them are tagged with an annotation in the IR tree, so that translation to WebAssembly can distinguish them from function definitions/calls using curried arguments. 

This optimisation requires that every call to the function can be identified precisely, since some other call that is not annotated would be compiled to a function call passing one argument at a time, which would be incorrect. For this reason, the optimisation is only done when every occurrence of the function variable is an application, so it is never bound to another variable or passed between functions. The compiler then relies on copy propagation, described below, to transform cases such as  \verb|let g = f in g(1,2)| to direct uses of \verb|f| and removing \verb|g|, allowing \verb|f| to be optimised. 

This could also be performed where a function always has more than one but not all of its arguments applied, but the benefit would not be as significant.


\subsection{Inlining}
Only non-recursive function applications are inlined, allowing values to be propagated inside the function body and some expressions to be evaluated by the compiler. This is achieved using some simple heuristics based on those described by Bonachea for the Titanium compiler \cite{titanium}. 

A function is always inlined if its body contains at most 5 nodes in the IR tree, as the increase in code size is negligible and this avoids making function calls for simple operations. Also, if a function is only used in one place and not exported from the program, it is inlined, since the function definition can then be removed so code size does not increase.
Larger functions are only inlined if doing so does not exceed the budget for code size, in terms of nodes in the IR tree. The resulting program cannot be more than 50\% larger than the original program size.
%First, functions with bodies containing at most 5 nodes in the IR tree are always inlined, as there is little cost in doing so and this handles very simple functions, such as accessing a field of a record or loading a mutable value. Also, if a function is not exported from the program and its only use is in a single application, this is also always inlined as the function definition can then be removed, so there is no increase in code size.
% Lastly, larger functions are considered for inlining if doing so does not exceed the budget for code size. The original size of the program is stored in terms of the number of nodes in the IR tree, and larger functions may only be inlined if the resulting program is less than 50\% larger than its initial size. 
This limit was found to be enough that useful inlining occurred, without the output code size increasing significantly once further optimisations were performed.

% TODO: Better justify these heuristics
% Also use smarter heuristics like number of function calls the function being inlined makes, or if it is being called from within a loop or not (more calls, bigger impact)


\subsection{Copying expressions, variables and constants}
As OCaml is a mostly functional language, local variables in the IR are only assigned to once (excluding variables introduced specially for tail-call optimisation, which are marked as mutable and ignored), making these optimisations relatively straightforward. 

% TODO: Describe IR structure in more detail e.g. imm/compond/linast and labelled of nodes as Local|Export|Mutable
First, the program is scanned and each \verb|let x = Constant c| or \verb|let x = Ident y| binding is added to a table. % (unless \verb|x| is one of the variables introduced by tail call optimisations and is therefore mutable). 
These are ``immediate" values in the linearised IR and indicate bindings that do no real work. Whenever a variable \verb|x| present in the table is used, it is replaced with its known value. 

This is also extended to values passed through immutable memory, such as tuples, constructor fields or some record fields. During translation to the IR, these expressions in OCaml's \verb|Typedtree| are translated to the \verb|Block| expression in the IR, and tagged with an \verb|ImmutableBlock l| annotation, where \verb|l| is a boolean list indicating which of the block's fields are immutable. 


When the IR tree is analysed, \verb|Block| nodes are annotated with the variables or constants stored in the block's immutable fields.
 A table mapping variables to annotations records this when the block is bound to a variable, so this propagates to any uses of the variable too. 
This information is also approximated at branches, by taking the intersection of the known fields when each branch returns a block. For example, \verb|if cond then (x, y) else (x, z)| would annotate the \verb|if| node with that fact that its first field is \verb|x|. 
When an immutable field of a block is accessed as \verb|Field(b, i)|, if the known value of that field is annotated on \verb|b|, the node is replaced with the known constant/variable, avoiding accessing it indirectly.

% this annotation is present on \verb|b| and contains its element at index \verb|i|, the node is replaced with the known constant/variable to propagate this value through the program.

Common subexpressions are optimised similarly to constant and identifier assignments. First, side-effect-free expressions are identified by the effects analysis described below. When such an expression is bound to a variable \verb|x|, subsequent occurrences of that expression are replaced with \verb|x|, as it holds the same value and avoids recomputing the expression. 
%As well as constant and identifier assignments, common subexpressions are optimised too. When a binding is found of an expression that does not modify program state and has the same value whenever it is evaluated, that binding is stored in a table of common expressions. Identifying such expressions is done by the effects analysis described below. 
%Subsequent occurrences of that expression are then replaced by the variable known to hold its value. So that variables are only reused when in scope, each entry is removed from the table after the subtree below its binding is searched.


These passes enable further optimisations, by removing redundancy or indirect accesses in the program. They also tend to result in unused variables, due to expressions being replaced with their known values.  Therefore, after these passes have been performed,  dead assignments are identified as variable assignments that are never used, and those that have no side effects are removed.

% SOMETHING SIMILAR ALREADY MENTIONED FOR UNCURRYING
%These optimisations also increase the number of cases found for tail-call optimisation. Where a function \verb|f| is assigned to another variable \verb|g| and that is used instead, tail call optimisation will not identify which function is being called by \verb|g|. These optimisations replace some such cases with using \verb|f| directly, in which case the function can now be identified and the call will be considered for tail-call optimisation.


% TODO: Make side effect analysis iterative to solve functions?
\subsection{Effects analysis}
OCaml is an impure language, so analysis of side effects is necessary to determine when expressions can be safely removed as above. This is achieved using the following annotations on nodes in the IR tree:

\verb"annotation = ... | Pure | Immutable | Latent of annotation list"

\verb|Pure| identifies expressions that do not modify program state, making them safe for dead-assignment elimination to remove. This includes creating a block in memory, as it could be immediately garbage collected if unused.

Of the pure expressions, \verb|Immutable| identifies expressions that will return the same value each time they are evaluated, so common-subexpression elimination can remove repeated occurrences of such expressions. Creating blocks containing only immutable fields is treated as immutable, as is accessing an immutable field of a block, since it can never be overwritten so always has the same value.

The \verb|Latent| annotation is used for functions, since a function definition itself is pure and immutable, but the body of the function may cause latent side effects when the function is called. Therefore, if a function body has annotations $F$, the function definition is annotated with $\{$\verb|Pure|, \verb|Immutable|,  \verb|Latent|$(F) \}$, and $F$ is unwrapped at function applications.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The algorithm is summarised by the inference rules and functions in figure \ref{fig:inferenceRules}, abbreviating the three annotations as $P$, $I$ and $L$. $\Gamma$ is a mapping of both variables and handler indices to analyses. 
The algorithm safely over-approximates possible side-effects, by under-approximating the annotated properties in multiple places. 
Nothing is assumed about the latent effects of variables that have not been analysed, which occurs for function parameters or recursive function variables,
 and the intersection of properties is taken at branches.

The function \verb|seq| encodes that the latent effects of \verb|e; e'| are those of \verb|e'|, but the expression is only pure/immutable if both \verb|e| and \verb|e'| are. The function \verb|app| encodes the same for unwrapping the analyses of latent effects as a function is applied. This is necessary, rather than just returning the latent effects, since a partial application may perform some impure computation then return another function, in which case the overall application is impure.

% Cases left out: Handler jumps, unary/binary operations
% For simplicity, ignores the propagation of 'Fields' analysis happening at the same time, as this generally isn't useful
% Only case it is useful is if we have 'if p then (f1, f2) else (f3, f4)', can't propagate the Idents themselves but can propagate effect analysis

\begin{figure}[H]
\begin{prooftree}
\hypo{ $\lstinline{x}$\   : F \in \Gamma}
\infer1{\Gamma \vdash \  $\lstinline{x}$\   : F \cup \{P, I\}}
\end{prooftree} \hfill
\begin{prooftree}
\hypo{ $\lstinline{x}$ \ \not \in \Gamma}
\infer1{\Gamma \vdash \  $\lstinline{x}$\   : \{P, I\}}
\end{prooftree} \hfill
\begin{prooftree}
\hypo{ \Gamma \vdash\  $\lstinline{e}$ \ : F}
\hypo{ \Gamma, x:F \vdash \ $\lstinline{e'}$\  : F'}
\infer2{\Gamma \vdash\   $\lstinline{let x = e in e'}$ \ :\ $\lstinline{seq}$(F, F')}
\end{prooftree}\hfill \\

\vspace{0.2cm}
\begin{prooftree}
\hypo{ \Gamma \vdash \  $\lstinline{e}$\  : F}
\infer1{\Gamma \vdash\   $\lstinline{fun x -> e}$\   : \{P, I, L(F)\}}
\end{prooftree}\hfill
\begin{prooftree}
\hypo{ \Gamma \vdash \  $\lstinline{e}$\  : F}
\hypo{ \Gamma, f:\{P, I, L(F)\} \vdash \ $\lstinline{e'}$\  : F'}
\infer2{\Gamma \vdash\   $\lstinline{let rec f x = e in e'}$\   :  F'}
\end{prooftree} \\

\vspace{0.2cm}
 \begin{prooftree}
\hypo{ \Gamma \vdash\  $\lstinline{f}$\ : F}
\hypo{ $\lstinline{app}$\  F\ $\lstinline{args}$\ = F'}
\infer2{\Gamma \vdash \ $\lstinline{f(args)}$\  : F'}
\end{prooftree} \hfill
\begin{prooftree}
\hypo{F = \{P, I\}$ if each field is immutable else $\{P\}}
\infer1{\Gamma \vdash\    $\lstinline{Block (tag, args)}$\  :  F}
\end{prooftree} 
 \\

\vspace{0.2cm}
\begin{prooftree}
\infer0{\Gamma \vdash   \ $\lstinline{SetField (e, i, e')}$ \ :  \{\}}
\end{prooftree} \hfill
\begin{prooftree}
\hypo{F = \{P, I\}$ if field \lstinline{i} of \lstinline{e} is immutable else $\{P\}}
\infer1{\Gamma \vdash\    $\lstinline{Field (e, i)}$\  :  F}
\end{prooftree}\hfill \\

\vspace{0.2cm}
 \begin{prooftree}
\hypo{ \Gamma \vdash\  $\lstinline{e}$ \ : F}
\hypo{ \Gamma \vdash\  $\lstinline{e'}$\  : F'}
\infer2{\Gamma \vdash\   $\lstinline{if p then e else e'}$ \ :  F \cap F'}
\end{prooftree} 
\hfill\begin{prooftree}
\hypo{ \Gamma \vdash  \ $\lstinline{e}$\ : F}
\hypo{ \Gamma \vdash\  $\lstinline{e'}$\ : F'}
\infer2{\Gamma \vdash \  $\lstinline{e; e'}$\ : \ $\lstinline{seq}$(F, F')}
\end{prooftree}\\

\vspace{0.2cm}
\begin{prooftree}
\hypo{ \Gamma \vdash\  $\lstinline{e'}$\ : F'}
\hypo{ \Gamma, i:F' \vdash  \ $\lstinline{e}$\ : F}
\infer2{\Gamma \vdash \  $\lstinline{try e with (i) -> e'}$\ :  F}
\end{prooftree}\hfill
\begin{prooftree}
\hypo{i : F \in \Gamma}
\infer1{\Gamma \vdash\   $\lstinline{fail i}$ \ :  F }
\end{prooftree}\hfill 
\begin{prooftree}
\infer0{\Gamma \vdash \  $\lstinline{Const c}$\   : \{P, I\}}
\end{prooftree} \\

\begin{lstlisting}
let seq (F1, F2) =  F2 $\setminus$ ($\{P, I\}\ \setminus$ F1)

let rec app F = function
  | [] -> F
  | arg::args -> 
    if $L$(F') $\in$ F:
      seq (F, app F' args)
    else: $\{\}$
\end{lstlisting}

\caption{Summary of the inference rules for effects analysis}
\label{fig:inferenceRules}
\end{figure}

%For the purpose of removing dead assignments, creating a block in memory is considered pure, as it could be immediately garbage collected if unused. If every field is immutable, then the block itself is also marked as immutable, since its fields can never be overwritten. Accessing a field of a block is pure, and is immutable if that field of the block is immutable. This therefore relies on the propagation of \verb|ImmutableBlock| annotations described previously, left out here to avoid further complicating the inference rules.

Using a linearised IR has resulted in simpler rules in some cases. For example, the condition of an \verb|if| statement is always a variable or constant so has no side effects, hence it is ignored.

%If the analysis of latent effects of a function being applied are unknown, for example, because the function applied is an argument to the current function (\verb|f| in \verb|map f lst|), then nothing is assumed about the result. 
%
%If one of the partial applications of a function is not marked as \verb|Pure| or \verb|Immutable|, then neither is the application node. This accounts for any side effects in partially evaluating the function, such as for \verb|let f x = (z := 5; (fun y -> x + y))|. 
%\verb|f| returns a function which is pure, but  the first application in \verb|f 1 2| evaluates the body of \verb|f|, which has side effects, so \verb|f 1 2| would not be marked as a pure expression.


\subsection{WebAssembly peephole optimisations}
%Lastly, several simple optimisations are made at the IR and WebAssembly level. Integer or floating-point operations in the IR tree involving constants are evaluated during compilation. Similarly, a \verb|switch| or \verb|if| statement switching on a constant is replaced with the branch that would be taken. Both of these help simplify function applications after they are inlined, reducing code size and execution time. 

At the WebAssembly level, in addition to being used for register allocation, live-variable analysis identifies assignments to variables that are unused so can be removed. Peephole optimisations also simplify several common cases resulting from compiling each operation independently. For example, \verb|LocalSet i; LocalGet i| is replaced with \verb|LocalTee i|, and \verb|i32.const n; drop| is removed (\verb|i32.const 0; drop| often occurs where a unit-valued expression is the first part of a sequence \verb|e1; e2|). These changes enable a more precise analysis of which variables are actually used by a program, allowing further dead assignments to be removed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Garbage collection}
One of the last extensions to my project was to implement a garbage collector as part of the runtime system. Due to its complexity compared to the other runtime functions and going through several improvements, this was written in JavaScript and imported into the WebAssembly runtime rather than writing it in WebAssembly. %I initially implemented a reference-counting garbage collector, but changed to a tracing implementation to be able to collect cyclic data structures.

% TODO: Talk about Wasm proposal for reference types/garbage collection?

%\subsection{Shadow stack}
A tracing collector marks all objects reachable from pointers on the stack and global variables, then collects all allocated unmarked objects.
Since the stack in WebAssembly is implicit, local variables are copied to the shadow stack in linear memory, allowing them to be explored during garbage collection. This additional store on every local-variable update is one of the main overheads of implementing garbage collection. The only bookkeeping required is a pointer to the top of the shadow stack, which moves up and down as functions are called and return.
% -- CUT OUT TO REDUCE WORD COUNT
%Although only pointer values need to be stored on the shadow stack, identifying pointers requires propagating type information through all stages of the compiler and would still be imprecise for polymorphic functions. Instead, all local variable assignments in functions first copy the new value to the corresponding slot on the shadow stack, and checking which values are pointers is left until garbage collection runs. This additional store on every local-variable update is one of the main overheads of implementing garbage collection.



\subsection{Simple memory allocator}
The garbage collector can be split into two components, one that provides \verb|malloc| and \verb|free| functions for manual memory allocation, and the other that implements the mark and sweep algorithm on top of this to decide which blocks to free. 

I initially implemented the first part following the algorithm described in K\&R \cite{k_and_r}. A cyclic linked list, called the free list, is maintained, chaining together all of the free blocks in memory, and the runtime keeps a pointer to some block in the list. Each block has an 8-byte header containing the size of the block and a pointer to the start of the next block. % Dont bother mentioning alignment?
As all values in compiled programs are 32-bit integers or 64-bit floats, blocks are always aligned to at least 4 bytes, so the least significant bits of each field are unused, leaving space for an allocated and marked flag. 

\verb|malloc| scans the free list until the first large-enough block is found. Either the whole block is allocated if it is a perfect fit, else the size of the block is reduced and just the tail end of it is allocated. If there are no large-enough blocks, the garbage collector is called.
If that frees a large-enough block then the list is scanned again, otherwise the WebAssembly memory is grown and a block allocated from the new space.
%When a block is requested, the algorithm is first-fit so the list is scanned for the first block large enough for the allocation. If less than one header size larger than the size requested, the block is removed from the list, marked as allocated, and returned. Otherwise, the size of the block is reduced and just the tail end of the block is returned. 


\verb|free| also scans the free list, until the last unallocated block before the block being freed is found. The freed block is then inserted here in the list, keeping blocks in the list in memory order. Existing free blocks adjacent to it in memory are also merged into one larger block to reduce fragmentation.

The mark and sweep algorithm then determines which blocks to free. When invoked, the shadow stack is scanned and each block pointed to is marked and put onto a stack. This stack is maintained using the pointer fields in the headers of blocks, which are otherwise meaningless for allocated blocks not in the free list. A depth-first search is then performed to mark all reachable blocks, using this stack to track which objects to explore the fields of next.
Finally, the sweep part of the algorithm traverses all blocks in memory, % using the size field in each block, 
clearing marked flags and freeing allocated blocks that were not marked and are therefore unreachable.

% TODO: Massively cut down details, don't need such low level information, especially as it is described in K&R
%\subsection{Basic Structure}
%The implementation can be divided into two parts, the \verb|malloc| and \verb|free| functions used for explicit memory management as in a language such as C, and the mark and sweep algorithm with determines when it is safe to call \verb|free|.
%
%The initial implementation of \verb|malloc| and \verb|free| was taken from K\&R, although inefficiencies in this were resolved later. \cite{k_and_r} % https://kremlin.cc/k&r.pdf - The C Programming Language, Kernighan and Ritchie
%The runtime maintains a free list where each cell is an unallocated block of memory, with its size and a pointer to the next block in an 8 byte header. All memory allocations are kept aligned to the header size so not all bits of the size and pointer fields are required, leaving space for an allocated and marked flag. The list is circular and initially contains just one large block for the whole of memory. \\
%When a block is allocated, the free list is scanned until the first large-enough block is found then either that whole block is allocated if it is a perfect fit, else the block is shrunk and just the tail end of it is allocated. If no large-enough block can be found, the runtime attempts to grow the WebAssembly memory and treats that as a new free block.
%
%When a block is freed, we again scan down the free list until the last unallocated block before the block being freed is found (if the block being freed is at the start of memory, this unallocated block will be the free block at the highest address). The block being freed is then inserted into the list and merged with the block on either side of it if those are also free. This helps to reduce fragmentation and ensures that the free list is always kept in memory order.\\
%
%
%% Idea about using a list for the allocated blocks and then 'copy collecting' to a separate list in mark phase and not even needing a marked bit, just freeing the old list at the end?? Would even work with binning but requires checking sizes
%The marking algorithm makes use of the pointer to the next free block in the header, which is still present on allocated blocks (since the size field is needed on allocated blocks) but otherwise unused as the block is no longer on the free list. Starting by marking all references from the shadow stack, it uses these pointers to maintain a stack of unexplored objects and does a depth first search of the reachable objects, setting the marked bit on all of them. \\
%The sweep algorithm then walks over all blocks in memory using the size field of each block. Whenever a block is marked it clears the flag, and if a block is allocated but not marked then it is freed.


\subsection{Optimised malloc and free}
The implementation above requires \verb|malloc| and \verb|free| to both walk along the free list, either to find a large-enough block or to find where to insert a freed block. \verb|free| can be made to always run in constant time, as can \verb|malloc| for the majority of calls.

First, an additional 8 byte trailer is added to each block, used as shown below, with the flags A = allocated, M = marked. A block's size, and whether it is allocated or not, is now accessible from either end of the block. The free list is doubly linked but is no longer made circular. 

\begin{figure}[H]
\hspace{-0.8cm}
\begin{bytefield}[bitformatting={\small\bfseries},bitwidth=1.1em, leftcurly=.]{34}
\begin{leftwordgroup}{Free block}
\bitbox{7}{last tralier ptr} & \bitbox{1}{A} \bitbox{7}{size} & \bitbox{1}{0} & \bitbox{2}{\dots} & \bitbox{7}{size} & \bitbox{1}{0}  &	\bitbox{7}{next header ptr} & \bitbox{1}{A} \end{leftwordgroup} \\
\text{}\\
\begin{leftwordgroup}{Allocated block}
\bitbox{7}{marked stack ptr} & \bitbox{1}{A}  & \bitbox{7}{size} & \bitbox{1}{M}  & \bitbox{2}{\dots} & \bitbox{7}{size} & \bitbox{1}{0}  &	\bitbox{7}{unused} & \bitbox{1}{A} \end{leftwordgroup} 
    \end{bytefield}
\vspace{-0.5cm}
\caption{Memory layout of the header and trailer for optimised memory allocation}
\end{figure}



%  \begin{tabular}{c|c|c|c|c|c|} \cline{2-6}
%Free block: &  previous trailer ptr & size & $\dots$ & size & next header ptr \\ \cline{2-6} \multicolumn{1}{c}{}  \\ \cline{2-6}
%Allocated block: &  marked stack ptr  & size & $\dots$ & size & unused \\ \cline{2-6}
%\end{tabular}


This is enough to be able to free a block without scanning the free list. From just the freed block's address, the procedure gets its size and checks the allocated flags of the blocks either side of it. %, which is possible since the block before has its size and allocated flag in its trailer. 
This determines if either block should be merged with the one being freed. Also, since the list is doubly linked, each adjacent free block has a pointer to its successor and predecessor block so can be removed from the middle of the free list without traversing it. If neither block is merged, the newly freed block is inserted at the front of the free list, so the free list no longer keeps blocks in address order.

The cost of \verb|malloc| comes from scanning the free list for a large-enough block. The solution to this is binning blocks into separate free lists based on their size, so scanning for a free block can skip searching any lists of blocks that will all be too small. All but the last bin fit exact sizes starting from the smallest possible allocation, then the last bin holds all larger blocks. 
The compiler supports limited array operations, so the size of most allocations is determined by user-defined datatypes and the number of free variables in functions. 
Often both are relatively small, hence a large proportion of freed blocks are placed in the fixed size bins.  
%There is now an array of free list pointers, and \verb|malloc| scans each list in turn searching for a block, starting from the earliest bin that could contain a large-enough block. 
%The rest of the algorithm is mostly unchanged, except that 
\verb|free| must now insert freed blocks into the correct free list, and shrinking blocks when allocating, or merging blocks when freeing, means that free blocks sometimes move between lists.

%%%%%%%%%%%%%%%%%%%%%%%%%%

A characteristic observed from tracing the behaviour of memory-intensive test programs is that, until garbage collection fails to free a suitable block and memory has to be grown, the garbage collector typically has diminishing returns each invocation as the set of long-lived objects grows. This results in there being less memory available over time and the garbage collector being called increasingly frequently.
Memory grows in multiples of WebAssembly pages, which are 64KiB. 
Once the memory being freed drops below about 1KiB, a future garbage collection almost always grows memory eventually. Therefore, another optimisation was to grow memory as soon as this threshold is crossed, rather than waiting for several inefficient collections before the available memory is fully exhausted. This reduces the overhead of garbage collection by avoiding many inefficient collections, without growing memory unnecessarily.
%Across several test programs, once the memory being freed drops below about 1KiB, the garbage collector start being called very frequently and almost always results in memory growing eventually. Therefore, another optimisation was to grow memory whenever this threshold is crossed, even if a block large enough for the requested allocation is found. This doesn't affect the amount of memory used, as memory almost always grows anyway after this point, but all of the objects freed by several frequent calls to the garbage collector are now collected in a single pass when this large block of memory is next used up, reducing the overhead of garbage collection.


\newpage
\section{Summary}
This chapter has described the components of my compiler and the decisions made while writing it. A wide range of OCaml programs that do not use the module language can be compiled to WebAssembly, and executed using JavaScript. I have also described the extensions made to the project, adding a garbage collector and several optimisation passes, which will be evaluated in the next chapter. The original goal was to support the comparison, boolean, integer and list operations from the standard library, which has been achieved and extended with the reference and basic floating-point operations.


\section{Repository overview}
\begin{figure}[H]
\dirtree{%
.1 . 
.2 src.
.3 ocaml/\DTcomment{OCaml compiler's frontend for parsing/typing}.
.3 middle-end/\DTcomment{Translation to IR}. 
.3 optimisations/\DTcomment{IR optimisations}. 
.3 codegen/\DTcomment{Translation from IR to WebAssembly flowgraph}. 
.3 graph/\DTcomment{WebAssembly flowgraph representation and optimisations}. 
.3 runtime. 
.4 memory.js. 
.4 runtime.wast. 
.2 samples\DTcomment{Test programs for compiler input}. 
.3 results.txt\DTcomment{Expected output of compiled WebAssembly}. 
.3 test.js\DTcomment{Instantiate complied tests and compare outputs}. 
.2 benchmarks.
.3 OCaml/.
.3 C/. 
.3 Grain/.  
.2 unit-tests/\DTcomment{Tests for isolated elements of translation/optimisation}.
}
\caption{Overview of the structure of the repository. Most of the contents of directories are omitted for brevity, such as the example programs in the \texttt{samples} directory.}
\end{figure}



