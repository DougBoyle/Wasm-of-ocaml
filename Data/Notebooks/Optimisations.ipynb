{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mpld3\n",
    "#mpld3.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(means, errors, baseline, keep=False):\n",
    "    new_means = {}\n",
    "    new_errors = {}\n",
    "    num_tests = len(means[baseline])\n",
    "    for version in means:\n",
    "        if version == baseline and not keep:\n",
    "            continue\n",
    "        new_means[version] = [means[version][i]/means[baseline][i] for i in range(num_tests)]\n",
    "        if errors is not None:\n",
    "            new_errors[version] =[errors[version][i]/means[baseline][i] for i in range(num_tests)]\n",
    "    return new_means, new_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot(ax, data, errors=None, colors=None, total_width=0.8, single_width=1, legend=False, capsize=3):\n",
    "    \"\"\"Draws a bar plot with multiple bars per data point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib.pyplot.axis\n",
    "        The axis we want to draw our plot on.\n",
    "\n",
    "    data: dictionary\n",
    "        A dictionary containing the data we want to plot. Keys are the names of the\n",
    "        data, the items is a list of the values.\n",
    "\n",
    "        Example:\n",
    "        data = {\n",
    "            \"x\":[1,2,3],\n",
    "            \"y\":[1,2,3],\n",
    "            \"z\":[1,2,3],\n",
    "        }\n",
    "\n",
    "    errors: dictionary, optional\n",
    "        Dictionary of standard deviations, corresponding structure to data\n",
    "\n",
    "    colors : array-like, optional\n",
    "        A list of colors which are used for the bars. If None, the colors\n",
    "        will be the standard matplotlib color cyle. (default: None)\n",
    "\n",
    "    total_width : float, optional, default: 0.8\n",
    "        The width of a bar group. 0.8 means that 80% of the x-axis is covered\n",
    "        by bars and 20% will be spaces between the bars.\n",
    "\n",
    "    single_width: float, optional, default: 1\n",
    "        The relative width of a single bar within a group. 1 means the bars\n",
    "        will touch eachother within a group, values less than 1 will make\n",
    "        these bars thinner.\n",
    "\n",
    "    legend: bool, optional, default: True\n",
    "        If this is set to true, a legend will be added to the axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if colors where provided, otherwhise use the default color cycle\n",
    "    if colors is None:\n",
    "        colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "    # Number of bars per group\n",
    "    n_bars = len(data)\n",
    "\n",
    "    # The width of a single bar\n",
    "    bar_width = total_width / n_bars\n",
    "\n",
    "    # List containing handles for the drawn bars, used for the legend\n",
    "    bars = []\n",
    "\n",
    "    # Iterate over all data\n",
    "    for i, (name, values) in enumerate(data.items()):\n",
    "        # The offset in x direction of that bar\n",
    "        x_offset = (i - n_bars / 2) * bar_width + bar_width / 2\n",
    "\n",
    "        # Draw a bar for every value of that type\n",
    "        for x, y in enumerate(values):\n",
    "            if errors is None:\n",
    "                bar = ax.bar(x + x_offset, y, width=bar_width * single_width, color=colors[i % len(colors)])\n",
    "            else:\n",
    "                err = errors[name][x]\n",
    "                bar = ax.bar(x + x_offset, y, yerr=err, error_kw=dict(capsize=capsize),\n",
    "                             width=bar_width * single_width, color=colors[i % len(colors)])\n",
    "\n",
    "        # Add a handle to the last drawn bar, which we'll need for the legend\n",
    "        bars.append(bar[0])\n",
    "\n",
    "#    # Draw legend if we need\n",
    "#    if legend:\n",
    "#        ax.legend(bars, data.keys())\n",
    "    # return the handlers/labels for a legend\n",
    "    if legend:\n",
    "        return bars, data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-yield",
   "metadata": {},
   "source": [
    "## Comparison of All optimisations, just IR, just Wasm, None\n",
    "All done with optimised pattern matching, no GC, and 3 iterations where optimisation passes are being applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "versions = [\"None\", \"Wasm\", \"IR\",  \"All\"]\n",
    "# Check file to see formatting\n",
    "with open(\"../../wasm-of-ocaml/benchmarks/evaluation/optimisations.txt\") as f:\n",
    "    f.readline()\n",
    "    for version in versions:\n",
    "        f.readline()\n",
    "        line = f.readline().strip().split()\n",
    "        while line != []:\n",
    "            if line[0] not in data:\n",
    "                data[line[0]] = {}\n",
    "            data[line[0]][version] = \\\n",
    "              {\"time\" : float(line[1]), \"error\" : float(line[2]), \"heap\": float(line[3]), \"filesize\" : float(line[4])}\n",
    "            line = f.readline().strip().split()\n",
    "tests = data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(10,10))\n",
    "ax = axs[0]\n",
    "times = {version: [data[test][version][\"time\"] for test in tests] for version in versions}\n",
    "errors = {version: [data[test][version][\"error\"] for test in tests] for version in versions}\n",
    "times, errors = normalise(times, errors, \"None\", True)\n",
    "#times = {lang : [means[lang][b] for b in benchmarks if b != \"arith\"] for lang in means if lang != \"Grain\"}\n",
    "bar_plot(ax, times, errors=errors)\n",
    "ax.set_title(\"Execution time\")\n",
    "ax.set_xticks([])\n",
    "ax.set_ylabel(\"relative time\")\n",
    "\n",
    "ax = axs[1]\n",
    "heap = {version: [data[test][version][\"heap\"] for test in tests] for version in versions}\n",
    "heap, _ = normalise(heap, None, \"None\", True)\n",
    "handlers, labels = bar_plot(ax, heap, legend=True)\n",
    "ax.set_title(\"Heap usage\")\n",
    "ax.set_xticks([])\n",
    "\n",
    "ax = axs[2]\n",
    "sizes = {version: [data[test][version][\"filesize\"] for test in tests] for version in versions}\n",
    "sizes, _ = normalise(sizes, None, \"None\", True)\n",
    "handlers, labels = bar_plot(ax, sizes, legend=True)\n",
    "ax.set_title(\"Filesize\")\n",
    "ax.set_xticks(range(len(tests)))\n",
    "ax.set_xticklabels(tests, rotation=90)\n",
    "ax.set_ylabel(\"size (bytes)\");\n",
    "\n",
    "fig.legend(handlers, labels, loc='center left');\n",
    "plt.subplots_adjust(left=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-medicare",
   "metadata": {},
   "source": [
    "Optimisations at both the WebAssembly and IR level reduce filesize across the board. TODO: Average percentage/range?  \n",
    "In many cases, execution time is significantly reduced by the IR level optimisations, although the WebAssembly optimisations have little impact on performance, possibly making it slightly worse in some cases but within 1 standard deviation of the original execution time. The are a couple of tests where execution time is not improved, `nbody` is an imperative style program performing floating point calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-webmaster",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratio = [1-sizes[\"All\"][i] / sizes[\"None\"][i] for i in range(len(tests))]\n",
    "print(min(ratio), max(ratio))\n",
    "print(np.mean(ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = [1-times[\"All\"][i] / times[\"None\"][i] for i in range(len(tests))]\n",
    "print(min(ratio), max(ratio))\n",
    "print(np.mean(ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-worse",
   "metadata": {},
   "source": [
    "Comparing all optimisations vs none:  \n",
    "The difference in filesize varies from -14% to -39%, with an average of -23%.  \n",
    "Execution time at best decreases by 90%, but in the worst example only increases by 2.2%, and it decreases in the majority of cases, particularly the more functional style programs.  \n",
    "TODO: Are such figures meaningful for hand picked examples? Particularly the average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-lottery",
   "metadata": {},
   "source": [
    "## Specific IR optimisations (inlining, tail calls, uncurrying)\n",
    "I now look at the effect of removing one of inlining, tail calls, and uncurrying on the resulting program to assess their relative impact. Copy propagation/folding and dead code elimination is kept in all cases.  \n",
    "\n",
    "One additional factor not shown is that tail call optimisation can prevent a program from exceeding the maximum call stack size by avoiding nested function calls. Here is a program that illustrates this behaviour:\n",
    "```\n",
    "let rec f x =\n",
    "  if x = 0 then 0 \n",
    "  else f (x-1)\n",
    "\n",
    "let a = f 100000\n",
    "```\n",
    "Without tail call optimisations, each recursive call adds another stack frame, eventually exhausting the available stack space and causing an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-antibody",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "labels = [\"All\", \"-inline\", \"-uncurry\", \"-tail-call\"]\n",
    "# Check file to see formatting\n",
    "with open(\"../../wasm-of-ocaml/benchmarks/evaluation/specific_opts.txt\") as f:\n",
    "    for i in labels:\n",
    "        f.readline()\n",
    "        line = f.readline().strip().split()\n",
    "        while line != []:\n",
    "            if line[0] not in data:\n",
    "                data[line[0]] = {}\n",
    "            data[line[0]][i] = \\\n",
    "              {\"time\" : float(line[1]), \"error\" : float(line[2]), \"heap\": float(line[3]), \"filesize\" : float(line[4])}\n",
    "            line = f.readline().strip().split()\n",
    "tests = data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-trunk",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(12,10))\n",
    "ax = axs[0]\n",
    "times = {i: [data[test][i][\"time\"] for test in tests] for i in labels}\n",
    "errors = {i: [data[test][i][\"error\"] for test in tests] for i in labels}\n",
    "times, errors = normalise(times, errors, \"All\", True)\n",
    "#times = {lang : [means[lang][b] for b in benchmarks if b != \"arith\"] for lang in means if lang != \"Grain\"}\n",
    "bar_plot(ax, times, errors=errors)\n",
    "ax.set_title(\"Execution time\")\n",
    "ax.set_xticks([])\n",
    "ax.set_ylabel(\"relative time\")\n",
    "\n",
    "ax = axs[1]\n",
    "mem = {i: [data[test][i][\"heap\"] for test in tests] for i in labels}\n",
    "mem, _ = normalise(mem, None, \"All\", True)\n",
    "handlers, labels = bar_plot(ax, mem, legend=True)\n",
    "ax.set_title(\"Heap usage\")\n",
    "ax.set_xticks([])\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylabel(\"Memory (bytes)\")\n",
    "\n",
    "ax = axs[2]\n",
    "sizes = {i: [data[test][i][\"filesize\"] for test in tests] for i in labels}\n",
    "sizes, _ = normalise(sizes, None, \"All\", True)\n",
    "handlers, labels = bar_plot(ax, sizes, legend=True)\n",
    "ax.set_title(\"Filesize\")\n",
    "ax.set_xticks(range(len(tests)))\n",
    "ax.set_xticklabels(tests, rotation=90)\n",
    "ax.set_ylabel(\"size (bytes)\")\n",
    "ax.set_ylim((0.8,1.3))\n",
    "\n",
    "fig.legend(handlers, labels, loc='center left')\n",
    "plt.subplots_adjust(left=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-deviation",
   "metadata": {},
   "source": [
    "Is this a slightly confusing way to display this information? i.e. worse score when removed => better performance when present?\n",
    "Would be better to just show effect of each optimisation in isolation?\n",
    "\n",
    "From the timing graph we see two expected results: In functions where currying can be removed, not creating additional closures can significantly improve performance, running up to twice as fast.  Also, tail call optimisation generally reduces performance due to the overhead of implementing it, the benefit being that programs will not give an error due to using all available stack space in tail recursive calls.  \n",
    "In terms of memory consumption, as expected there is a significant improvement by replacing curried calls with passing the arguments as a tuple where possible. In the case of the arithmetic programs, these closures make up almost all of the memory allocations performed by the program, so removing them can reduce memory usage almost entirely.  \n",
    "Lastly, we see that removing currying also reduces the size of programs, as extra functions to create closures are left out of the code. By limiting the amount of extra code that can be introduced by inlining, the code bloat it causes is limited, and in some cases programs even become smaller due to a function being completely inlined and removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-humidity",
   "metadata": {},
   "source": [
    "## Phase order and number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "# Check file to see formatting\n",
    "with open(\"../../wasm-of-ocaml/benchmarks/evaluation/iterations.txt\") as f:\n",
    "    for i in labels:\n",
    "        f.readline()\n",
    "        line = f.readline().strip().split()\n",
    "        while line != []:\n",
    "            if line[0] not in data:\n",
    "                data[line[0]] = {}\n",
    "            data[line[0]][i] = \\\n",
    "              {\"time\" : float(line[1]), \"error\" : float(line[2]), \"heap\": float(line[3]), \"filesize\" : float(line[4])}\n",
    "            line = f.readline().strip().split()\n",
    "tests = data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(12,10))\n",
    "ax = axs[0]\n",
    "times = {i: [data[test][i][\"time\"] for test in tests] for i in labels}\n",
    "errors = {i: [data[test][i][\"error\"] for test in tests] for i in labels}\n",
    "times, errors = normalise(times, errors, \"0\", True)\n",
    "#times = {lang : [means[lang][b] for b in benchmarks if b != \"arith\"] for lang in means if lang != \"Grain\"}\n",
    "bar_plot(ax, times, errors=errors)\n",
    "ax.set_title(\"Execution time\")\n",
    "ax.set_xticks([])\n",
    "ax.set_ylabel(\"relative time\")\n",
    "\n",
    "ax = axs[1]\n",
    "mem = {i: [data[test][i][\"heap\"] for test in tests] for i in labels}\n",
    "mem, _ = normalise(mem, None, \"0\", True)\n",
    "handlers, labels = bar_plot(ax, mem, legend=True)\n",
    "ax.set_title(\"Heap usage\")\n",
    "ax.set_xticks([])\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylabel(\"Memory (bytes)\")\n",
    "\n",
    "ax = axs[2]\n",
    "sizes = {i: [data[test][i][\"filesize\"] for test in tests] for i in labels}\n",
    "sizes, _ = normalise(sizes, None, \"0\", True)\n",
    "handlers, labels = bar_plot(ax, sizes, legend=True)\n",
    "ax.set_title(\"Filesize\")\n",
    "ax.set_xticks(range(len(tests)))\n",
    "ax.set_xticklabels(tests, rotation=90)\n",
    "ax.set_ylabel(\"size (bytes)\");\n",
    "\n",
    "fig.legend(handlers, labels, loc='center left');\n",
    "plt.subplots_adjust(left=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-christmas",
   "metadata": {},
   "source": [
    "From these plots we see that performance rarely gets worse with more iterations, and that further improvement after three iterations only happens in a couple instances. Where there are improvements from performing more than three iterations, these tend to be much smaller than the improvements already achieved, hence 3 iterations is chosen as the number of times to run the set of optimisation passes for all other analysis.  \n",
    "\n",
    "Performing multiple passes of the iterations reduces the significance of the ordering of passes. Still, the more complex optimisations such as inlining or uncurrying functions tend to rely on some values being propagated through the code initially produced. As such, these information propagation passes such as CSE and constant porpagation are run first.  \n",
    "Functions are then inlined where beneficial, and those functions which are fully applied but are not selected to be inlined are then uncurried. Lastly, dead assignment elimination removes any now useless definitions and tail call optimisation attempts to optimise suitable recursive functions that have not been removed.  \n",
    "\n",
    "Inlining in particular reveals new opportunities to propagate values, hence the benefit of running iterations in multiple passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "labels = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "# Check file to see formatting\n",
    "with open(\"../../wasm-of-ocaml/benchmarks/evaluation/phaseorder.txt\") as f:\n",
    "    for i in labels:\n",
    "        f.readline()\n",
    "        line = f.readline().strip().split()\n",
    "        while line != []:\n",
    "            if line[0] not in data:\n",
    "                data[line[0]] = {}\n",
    "            data[line[0]][i] = \\\n",
    "              {\"time\" : float(line[1]), \"error\" : float(line[2]), \"heap\": float(line[3]), \"filesize\" : float(line[4])}\n",
    "            line = f.readline().strip().split()\n",
    "tests = data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(12,10))\n",
    "ax = axs[0]\n",
    "times = {i: [data[test][i][\"time\"] for test in tests] for i in labels}\n",
    "errors = {i: [data[test][i][\"error\"] for test in tests] for i in labels}\n",
    "times, errors = normalise(times, errors, \"1\", True)\n",
    "#times = {lang : [means[lang][b] for b in benchmarks if b != \"arith\"] for lang in means if lang != \"Grain\"}\n",
    "bar_plot(ax, times, errors=errors)\n",
    "ax.set_title(\"Execution time\")\n",
    "ax.set_xticks([])\n",
    "ax.set_ylabel(\"relative time\")\n",
    "\n",
    "ax = axs[1]\n",
    "mem = {i: [data[test][i][\"heap\"] for test in tests] for i in labels}\n",
    "mem, _ = normalise(mem, None, \"1\", True)\n",
    "handlers, labels = bar_plot(ax, mem, legend=True)\n",
    "ax.set_title(\"Heap usage\")\n",
    "ax.set_xticks([])\n",
    "ax.set_ylabel(\"Memory (bytes)\")\n",
    "\n",
    "ax = axs[2]\n",
    "sizes = {i: [data[test][i][\"filesize\"] for test in tests] for i in labels}\n",
    "sizes, _ = normalise(sizes, None, \"1\", True)\n",
    "handlers, labels = bar_plot(ax, sizes, legend=True)\n",
    "ax.set_title(\"Filesize\")\n",
    "ax.set_xticks(range(len(tests)))\n",
    "ax.set_xticklabels(tests, rotation=90)\n",
    "ax.set_ylabel(\"size (bytes)\");\n",
    "\n",
    "fig.legend(handlers, labels, loc='center left');\n",
    "plt.subplots_adjust(left=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-logic",
   "metadata": {},
   "source": [
    "Have swapped best choice i.e. now doing uncurrying before inlining and dead code last. (but only a relative graph so doesn't matter if its consistent or not)\n",
    "Various orderings of the IR optimisations were considered, treating all of the optimisations which just propagate values as one group i.e. copy/constant propagation and CSE. 5 orderings of propagation, uncurrying, inlining, tail calls, and dead code elimination are shown above. In most instances, the choice of ordering had no impact on heap usage or filesize so this is not shown.  \n",
    "From the timing plots, options 4 and 5 are significantly worse in most cases. Between the other three, there is no clearly better choice as it appears to depend on the program compiled. Therefore, I use ordering 1 in all other tests:  \n",
    "propagate-uncurry-inline-tailcall-dead code  \n",
    "Intuitively, propagating known values and evaluating constant expressions makes sense to do first as this exposes opportunities to perform other optimisation. Uncurrying is done next as it potentially removes several intermediate functions, followed by inlining and tailcall optimisation. The order of these two makes no difference, as we do not inline recursive functions whereas we only tailcall optimise recursive functions. Finally, after all of these changes have been made, dead code elimination removes all values which are no longer required.  \n",
    "The fact that this is repeated 3 times reduces how sensitive the output is to the ordering chosen.\n",
    "\n",
    "\n",
    "The 5 examples are shown just for an idea of the variation caused by the ordering, as even with inlining and tailcall optimisation being interchangeable there are 96 distinct orderings, were they to be considered exhaustively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-netscape",
   "metadata": {},
   "source": [
    "## Outputting data for MATLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "cases = [0, 3, 5, 6, 7, 9]\n",
    "print([list(tests)[i] for i in cases])\n",
    "print(\"[\" +  \"; \".join([\" \".join([str(times[label][i]) for label in labels]) for i in cases]) + \"]\")\n",
    "print(\"[\" +  \"; \".join([\" \".join([str(errors[label][i]) for label in labels]) for i in cases]) + \"]\")\n",
    "print()\n",
    "print(\"[\" +  \"; \".join([\" \".join([str(mem[label][i]) for label in labels]) for i in cases]) + \"]\")\n",
    "print()\n",
    "print(\"[\" +  \"; \".join([\" \".join([str(sizes[label][i]) for label in labels]) for i in cases]) + \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "times[\"-inline\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
