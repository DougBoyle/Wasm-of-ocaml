{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mpld3\n",
    "#mpld3.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(means, errors, baseline, keep=False):\n",
    "    new_means = {}\n",
    "    new_errors = {}\n",
    "    num_tests = len(means[baseline])\n",
    "    for version in means:\n",
    "        if version == baseline and not keep:\n",
    "            continue\n",
    "        new_means[version] = [means[version][i]/means[baseline][i] for i in range(num_tests)]\n",
    "        new_errors[version] =[errors[version][i]/means[baseline][i] for i in range(num_tests)]\n",
    "    return new_means, new_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot(ax, data, errors=None, colors=None, total_width=0.8, single_width=1, legend=False, capsize=3):\n",
    "    \"\"\"Draws a bar plot with multiple bars per data point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib.pyplot.axis\n",
    "        The axis we want to draw our plot on.\n",
    "\n",
    "    data: dictionary\n",
    "        A dictionary containing the data we want to plot. Keys are the names of the\n",
    "        data, the items is a list of the values.\n",
    "\n",
    "        Example:\n",
    "        data = {\n",
    "            \"x\":[1,2,3],\n",
    "            \"y\":[1,2,3],\n",
    "            \"z\":[1,2,3],\n",
    "        }\n",
    "\n",
    "    errors: dictionary, optional\n",
    "        Dictionary of standard deviations, corresponding structure to data\n",
    "\n",
    "    colors : array-like, optional\n",
    "        A list of colors which are used for the bars. If None, the colors\n",
    "        will be the standard matplotlib color cyle. (default: None)\n",
    "\n",
    "    total_width : float, optional, default: 0.8\n",
    "        The width of a bar group. 0.8 means that 80% of the x-axis is covered\n",
    "        by bars and 20% will be spaces between the bars.\n",
    "\n",
    "    single_width: float, optional, default: 1\n",
    "        The relative width of a single bar within a group. 1 means the bars\n",
    "        will touch eachother within a group, values less than 1 will make\n",
    "        these bars thinner.\n",
    "\n",
    "    legend: bool, optional, default: True\n",
    "        If this is set to true, a legend will be added to the axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if colors where provided, otherwhise use the default color cycle\n",
    "    if colors is None:\n",
    "        colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "    # Number of bars per group\n",
    "    n_bars = len(data)\n",
    "\n",
    "    # The width of a single bar\n",
    "    bar_width = total_width / n_bars\n",
    "\n",
    "    # List containing handles for the drawn bars, used for the legend\n",
    "    bars = []\n",
    "\n",
    "    # Iterate over all data\n",
    "    for i, (name, values) in enumerate(data.items()):\n",
    "        # The offset in x direction of that bar\n",
    "        x_offset = (i - n_bars / 2) * bar_width + bar_width / 2\n",
    "\n",
    "        # Draw a bar for every value of that type\n",
    "        for x, y in enumerate(values):\n",
    "            if errors is None:\n",
    "                bar = ax.bar(x + x_offset, y, width=bar_width * single_width, color=colors[i % len(colors)])\n",
    "            else:\n",
    "                err = errors[name][x]\n",
    "                bar = ax.bar(x + x_offset, y, yerr=err, error_kw=dict(capsize=capsize),\n",
    "                             width=bar_width * single_width, color=colors[i % len(colors)])\n",
    "\n",
    "        # Add a handle to the last drawn bar, which we'll need for the legend\n",
    "        bars.append(bar[0])\n",
    "\n",
    "#    # Draw legend if we need\n",
    "#    if legend:\n",
    "#        ax.legend(bars, data.keys())\n",
    "    # return the handlers/labels for a legend\n",
    "    if legend:\n",
    "        return bars, data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-yield",
   "metadata": {},
   "source": [
    "## Comparison of All optimisations, just IR, just Wasm, None\n",
    "All done with optimised pattern matching, no GC, and 3 iterations where optimisation passes are being applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "versions = [\"None\", \"Wasm\", \"IR\",  \"All\"]\n",
    "# Check file to see formatting\n",
    "with open(\"../../wasm-of-ocaml/benchmarks/evaluation/optimisations.txt\") as f:\n",
    "    f.readline()\n",
    "    for version in versions:\n",
    "        f.readline()\n",
    "        line = f.readline().strip().split()\n",
    "        while line != []:\n",
    "            if line[0] not in data:\n",
    "                data[line[0]] = {}\n",
    "            data[line[0]][version] = \\\n",
    "              {\"time\" : float(line[1]), \"error\" : float(line[2]), \"heap\": float(line[3]), \"filesize\" : float(line[4])}\n",
    "            line = f.readline().strip().split()\n",
    "tests = data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-target",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(10,8))\n",
    "ax = axs[0]\n",
    "times = {version: [data[test][version][\"time\"] for test in tests] for version in versions}\n",
    "errors = {version: [data[test][version][\"error\"] for test in tests] for version in versions}\n",
    "times, errors = normalise(times, errors, \"None\", True)\n",
    "#times = {lang : [means[lang][b] for b in benchmarks if b != \"arith\"] for lang in means if lang != \"Grain\"}\n",
    "bar_plot(ax, times, errors=errors)\n",
    "ax.set_title(\"Execution time\")\n",
    "ax.set_xticks([])\n",
    "ax.set_ylabel(\"relative time\")\n",
    "\n",
    "ax = axs[1]\n",
    "sizes = {version: [data[test][version][\"filesize\"] for test in tests] for version in versions}\n",
    "handlers, labels = bar_plot(ax, sizes, legend=True)\n",
    "ax.set_title(\"Filesize\")\n",
    "ax.set_xticks(range(len(tests)))\n",
    "ax.set_xticklabels(tests, rotation=90)\n",
    "ax.set_ylabel(\"size (bytes)\");\n",
    "\n",
    "fig.legend(handlers, labels, loc='center left');\n",
    "plt.subplots_adjust(left=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-medicare",
   "metadata": {},
   "source": [
    "Optimisations at both the WebAssembly and IR level reduce filesize across the board. TODO: Average percentage/range?  \n",
    "In many cases, execution time is significantly reduced by the IR level optimisations, although the WebAssembly optimisations have little impact on performance, possibly making it slightly worse in some cases but within 1 standard deviation of the original execution time. The are a couple of tests where execution time is not improved, `nbody` is an imperative style program performing floating point calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-webmaster",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratio = [1-sizes[\"All\"][i] / sizes[\"None\"][i] for i in range(len(tests))]\n",
    "print(min(ratio), max(ratio))\n",
    "print(np.mean(ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = [1-times[\"All\"][i] / times[\"None\"][i] for i in range(len(tests))]\n",
    "print(min(ratio), max(ratio))\n",
    "print(np.mean(ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-worse",
   "metadata": {},
   "source": [
    "Comparing all optimisations vs none:  \n",
    "The difference in filesize varies from -14% to -39%, with an average of -23%.  \n",
    "Execution time at best decreases by 90%, but in the worst example only increases by 2.2%, and it decreases in the majority of cases, particularly the more functional style programs.  \n",
    "TODO: Are such figures meaningful for hand picked examples? Particularly the average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-lottery",
   "metadata": {},
   "source": [
    "## Specific IR optimisations (inlining, tail calls, uncurrying)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-humidity",
   "metadata": {},
   "source": [
    "## Phase order and number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "# Check file to see formatting\n",
    "with open(\"../../wasm-of-ocaml/benchmarks/evaluation/iterations.txt\") as f:\n",
    "    for i in labels:\n",
    "        f.readline()\n",
    "        line = f.readline().strip().split()\n",
    "        while line != []:\n",
    "            if line[0] not in data:\n",
    "                data[line[0]] = {}\n",
    "            data[line[0]][i] = \\\n",
    "              {\"time\" : float(line[1]), \"error\" : float(line[2]), \"heap\": float(line[3]), \"filesize\" : float(line[4])}\n",
    "            line = f.readline().strip().split()\n",
    "tests = data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(12,10))\n",
    "ax = axs[0]\n",
    "times = {i: [data[test][i][\"time\"] for test in tests] for i in labels}\n",
    "errors = {i: [data[test][i][\"error\"] for test in tests] for i in labels}\n",
    "times, errors = normalise(times, errors, \"0\", True)\n",
    "#times = {lang : [means[lang][b] for b in benchmarks if b != \"arith\"] for lang in means if lang != \"Grain\"}\n",
    "bar_plot(ax, times, errors=errors)\n",
    "ax.set_title(\"Execution time\")\n",
    "ax.set_xticks([])\n",
    "ax.set_ylabel(\"relative time\")\n",
    "\n",
    "ax = axs[1]\n",
    "mem = {i: [data[test][i][\"heap\"] for test in tests] for i in labels}\n",
    "handlers, labels = bar_plot(ax, mem, legend=True)\n",
    "ax.set_title(\"Heap usage\")\n",
    "ax.set_xticks([])\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylabel(\"Memory (bytes)\")\n",
    "\n",
    "ax = axs[2]\n",
    "sizes = {i: [data[test][i][\"filesize\"] for test in tests] for i in labels}\n",
    "handlers, labels = bar_plot(ax, sizes, legend=True)\n",
    "ax.set_title(\"Filesize\")\n",
    "ax.set_xticks(range(len(tests)))\n",
    "ax.set_xticklabels(tests, rotation=90)\n",
    "ax.set_ylabel(\"size (bytes)\");\n",
    "\n",
    "fig.legend(handlers, labels, loc='center left');\n",
    "plt.subplots_adjust(left=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-christmas",
   "metadata": {},
   "source": [
    "From these plots we see that performance rarely gets worse with more iterations, and that further improvement after three iterations only happens in a couple instances. Where there are improvements from performing more than three iterations, these tend to be much smaller than the improvements already achieved, hence 3 iterations is chosen as the number of times to run the set of optimisation passes for all other analysis.  \n",
    "\n",
    "Performing multiple passes of the iterations reduces the significance of the ordering of passes. Still, the more complex optimisations such as inlining or uncurrying functions tend to rely on some values being propagated through the code initially produced. As such, these information propagation passes such as CSE and constant porpagation are run first.  \n",
    "Functions are then inlined where beneficial, and those functions which are fully applied but are not selected to be inlined are then uncurried. Lastly, dead assignment elimination removes any now useless definitions and tail call optimisation attempts to optimise suitable recursive functions that have not been removed.  \n",
    "\n",
    "Inlining in particular reveals new opportunities to propagate values, hence the benefit of running iterations in multiple passes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
